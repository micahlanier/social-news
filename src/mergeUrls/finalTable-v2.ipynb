{
 "metadata": {
  "name": "",
  "signature": "sha256:da07617388cccbf229623ff76e52b6bd7f188e7167570cfbdbe1735893c41c8e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Libraries.\n",
      "import json\n",
      "import os\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from urlparse import urlparse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Imports.\n",
      "import re\n",
      "\n",
      "# Define pattern.\n",
      "urlPattern = '(?i)\\\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\\\s()<>{}\\\\[\\\\]]+|\\\\([^\\\\s()]*?\\\\([^\\\\s()]+\\\\)[^\\\\s()]*?\\\\)|\\\\([^\\\\s]+?\\\\))+(?:\\\\([^\\\\s()]*?\\\\([^\\\\s()]+\\\\)[^\\\\s()]*?\\\\)|\\\\([^\\\\s]+?\\\\)|[^\\\\s`!()\\\\[\\\\]{};:\\'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019])|(?:(?<!@)[a-z0-9]+(?:[.\\\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\\\b/?(?!@)))'\n",
      "\n",
      "\"\"\"\n",
      "Detect if a URL is a link to a Facebook photo.\n",
      "\n",
      "Examples:\n",
      "\turlIsFacebookPhoto('https://www.facebook.com/bbcworldnews/photos/pcb.10153822033402588/10153822032512588/?type=1') == True\n",
      "\turlIsFacebookPhoto('http://bbc.in/fgh543q') == False\t\n",
      "\"\"\"\n",
      "def urlIsFacebookPhoto(url):\n",
      "\treturn bool(re.match('^https?://(www\\\\.)?facebook.com/[a-zA-Z0-9]+/photos/.+$', url))\n",
      "\n",
      "\"\"\"\n",
      "Detect if a URL is a link to a Facebook video.\n",
      "\n",
      "Examples:\n",
      "\turlIsFacebookPhoto('https://www.facebook.com/video.php?v=10150471411394999') == True\n",
      "\turlIsFacebookPhoto('http://bbc.in/fgh543q') == False\t\n",
      "\"\"\"\n",
      "def urlIsFacebookVideo(url):\n",
      "\treturn bool(re.match('^https?://(www\\\\.)?facebook.com/video.php\\\\?.+$', url))\n",
      "\n",
      "\"\"\"\n",
      "TODO\n",
      "\"\"\"\n",
      "def urlsInText(t):\n",
      "\t# Perform search.\n",
      "\turlMatches = re.search(urlPattern, t)\n",
      "\tif urlMatches:\n",
      "\t\treturn [g for g in urlMatches.groups()]\n",
      "\treturn []\n",
      "\n",
      "def urlIsFacebookLink(url):\n",
      "    return bool(re.match('^https?://(www\\\\.)?facebook.com.+$', url))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sources = ['twitter','facebook']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "address = ['1. twitter', '2. facebook']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# File paths\n",
      "pathConsolidated = os.getcwd()+'/Social News/data/%s/consolidated/'\n",
      "pathMergedUrls = os.getcwd()+'/Social News/data/5. mergedUrls/'\n",
      "pathSocialMediaMetrics = os.getcwd()+'/Social News/data/6. socialMetrics/'\n",
      "pathBitly = os.getcwd()+'/Social News/data/7. bitly/'\n",
      "pathFinalTable = os.getcwd()+'/Social News/data/8. finalTable/'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get all files to traverse. Assume that we can get everything in consolidated folder.\n",
      "postFiles = []\n",
      "for source in address:\n",
      "    sourcePath = pathConsolidated % source\n",
      "    postFiles += [(source,p[:p.find('-')],sourcePath + p) for p in os.listdir(sourcePath) if p[-5:] == '.json']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "postFiles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Organization Names\n",
      "organizationNames = [x[1].split(\".\")[0] for x in postFiles]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "organizationNames = set(organizationNames)\n",
      "organizationNames = list(organizationNames)\n",
      "organizationNames.sort()\n",
      "len(organizationNames)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Extract Social Metrics Data#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extractSocialMetrics(filenames, destFolder):\n",
      "    tw_columns = ['tw_expandedUrl','tw_favorites', 'tw_retweet', 'tw_text', 'tw_postTime']\n",
      "    fb_columns = ['fb_expandedUrl','fb_likes', 'fb_share', 'fb_text', 'fb_postTime']\n",
      "    #print columnNames\n",
      "    rowCount = 0\n",
      "    for organization in filenames:\n",
      "        socialMedia = organization[0].split(' ')[-1]\n",
      "        print organization[1]+\" \"+socialMedia\n",
      "        if socialMedia == 'twitter':\n",
      "            columnNames = tw_columns\n",
      "        else:\n",
      "            columnNames = fb_columns\n",
      "        \n",
      "        resultDF = pd.DataFrame(columns=columnNames)\n",
      "        \n",
      "        # Load fb/tw json objects\n",
      "        data = json.load(open(organization[2]))\n",
      "        for article in data:\n",
      "            #Twitter data\n",
      "            if socialMedia == 'twitter':\n",
      "                #Account for tweets from account holder or retweeting another tweet\n",
      "                if 'retweeted_status' in article and 'favorite_count' in article['retweeted_status']:\n",
      "                    favorites = article['retweeted_status']['favorite_count']\n",
      "                elif 'favorite_count' in article:\n",
      "                    favorites = article['favorite_count']\n",
      "                else:\n",
      "                    favorites = np.nan\n",
      "                retweet = article['retweet_count']\n",
      "                postTime = article['created_at']\n",
      "                text = article['text']\n",
      "                if len(article['entities']['urls']) == 0:\n",
      "                    expandedUrl = np.nan\n",
      "                else:\n",
      "                    expandedUrl = article['entities']['urls'][0]['expanded_url']\n",
      "            #Facebook data\n",
      "            else:\n",
      "                if 'link' in article and (urlIsFacebookPhoto(article['link']) or urlIsFacebookVideo(article['link'])):\n",
      "                    #expandedUrl = urlsInText(article['message'])\n",
      "                    if 'message' in article:\n",
      "                        if len(urlsInText(article['message'])) > 0:\n",
      "                            expandedUrl = urlsInText(article['message'])[0]\n",
      "                        else:\n",
      "                            expandedUrl = np.nan\n",
      "                    else:\n",
      "                        expandedUrl = np.nan\n",
      "                elif 'link' in article:\n",
      "                    expandedUrl = article['link']\n",
      "                else:\n",
      "                    expandedUrl = np.nan\n",
      "                if 'likes' in article and 'summary' in article['likes'] and 'total_count' in article['likes']['summary']:\n",
      "                    favorites = article['likes']['summary']['total_count']\n",
      "                else:\n",
      "                    favorites = np.nan\n",
      "                if 'shares' in article and 'count' in article['shares']:\n",
      "                    retweet = article['shares']['count']\n",
      "                else:\n",
      "                    retweet = np.nan\n",
      "                postTime = article['created_time']\n",
      "                if 'message' in article:\n",
      "                    text = article['message']\n",
      "                elif 'description' in article:\n",
      "                    text = article['description']\n",
      "                else:\n",
      "                    text = np.nan\n",
      "            resultDF.loc[rowCount] = [expandedUrl, favorites, retweet, text, postTime]\n",
      "            rowCount += 1\n",
      "        #Save DF\n",
      "        filename = organization[1].split('.')[0]\n",
      "        resultDF.to_csv(destFolder+filename+'_'+socialMedia+'.csv', encoding='utf-8')\n",
      "        #print resultDF\n",
      "        #print data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# NYTimes\n",
      "#extractSocialMetrics([postFiles[15], postFiles[40]], tw_columns, pathSocialMediaMetrics)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# BBC\n",
      "#extractSocialMetrics([postFiles[3], postFiles[28]], pathSocialMediaMetrics)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "extractSocialMetrics(postFiles, pathSocialMediaMetrics)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testpath = pathBitly+'twitter/bbc.json'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(testpath)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "postFiles[0][2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(postFiles[0][2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Load in Bitly Data#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extractBitly(bitlyInputFolder, destFolder, orgNames, socialMedia):\n",
      "    \n",
      "    for orgName in orgNames:\n",
      "        print orgName\n",
      "        rowCount = 0\n",
      "        if socialMedia == 'twitter':\n",
      "            bitlyDF = pd.DataFrame(columns=['tw_url', 'tw_link_clicks', 'tw_timeRetrieved'])\n",
      "        else:\n",
      "            bitlyDF = pd.DataFrame(columns=['fb_url', 'fb_link_clicks', 'fb_timeRetrieved'])\n",
      "        \n",
      "        bitlyJson = json.load(open(bitlyInputFolder+orgName+'.json'))\n",
      "        \n",
      "        for key, value in bitlyJson.items():\n",
      "            #print key\n",
      "            #print value\n",
      "            if ('data' in value) and (value['status_txt'] != 'NOT_FOUND' and value['status_txt'] != 'UNKNOWN_ERROR'):\n",
      "                links_clicks = value['data']['link_clicks']\n",
      "            else:\n",
      "                links_clicks = np.nan\n",
      "            value['retrieved']\n",
      "            #Save results\n",
      "            bitlyDF.loc[rowCount] = [key, links_clicks, value['retrieved']]\n",
      "            rowCount += 1\n",
      "        print bitlyDF.shape\n",
      "        bitlyDF.to_csv(destFolder+orgName+'_'+socialMedia+'.csv', encoding='utf-8')\n",
      "    \n",
      "    return bitlyDF    \n",
      "    \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Example\n",
      "#extractBitly(pathBitly+'twitter/', pathBitly+'1. extractedBitly/', ['nbcnews'], 'twitter').head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Extract bitly links from twitter\n",
      "extractBitly(pathBitly+'twitter/', pathBitly+'1. extractedBitly/', organizationNames, 'twitter').head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Extract bitly links from facebook\n",
      "extractBitly(pathBitly+'facebook/', pathBitly+'1. extractedBitly/', organizationNames, 'facebook').head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Merge the Facebook, Twitter, Bitly Social Media Data with Url Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sumSocialMetrics(socialMedia, socialMedia_DF, socialMedia_mergedUrls_DF, destFolder, orgName):\n",
      "    if socialMedia == 'twitter':\n",
      "        social_w_Urls = socialMedia_DF.merge(socialMedia_mergedUrls_DF, left_on='tw_expandedUrl', right_on='TW_ShortenedUrl', how=\"inner\")\n",
      "        social_w_Urls = social_w_Urls.drop_duplicates(cols=['tw_expandedUrl', 'tw_postTime'] , take_last=True)\n",
      "        social_w_Urls.to_csv(destFolder+'1. Intermediates/PreSum/'+orgName+'_PreSumSocialwUrls_twitter.csv', encoding='utf-8')\n",
      "        social_w_Urls = social_w_Urls.groupby(['tw_expandedUrl']).agg([np.sum, len])\n",
      "        social_w_Urls.to_csv(destFolder+'1. Intermediates/'+orgName+'_SocialwUrls_twitter.csv', encoding='utf-8')\n",
      "\n",
      "        DF_summed = socialMedia_DF.merge(social_w_Urls[['tw_favorites', 'tw_retweet', 'tw_text', 'tw_postTime']], \n",
      "                                   left_on='tw_expandedUrl', right_index=True , how=\"inner\")\n",
      "        DF_summed.columns = ['tw_expandedUrl', 'tw_favorites', 'tw_retweet', 'tw_text', 'tw_postTime',\n",
      "                                 'tw_favorites_Sum', 'tw_favorities_Count', 'tw_retweet_Sum', 'tw_retweet_Count',\n",
      "                                 'tw_text_Sum', 'tw_text_Count', 'tw_text_Sum', 'tw_article_appearance_Count']\n",
      "        DF_summed = DF_summed.drop(['tw_favorities_Count', 'tw_retweet_Count', 'tw_text_Count'], axis=1)\n",
      "        DF_summed = DF_summed.drop_duplicates(cols='tw_expandedUrl', take_last=True)\n",
      "        DF_summed.to_csv(destFolder+'1. Intermediates/'+orgName+'_SocialwUrls_twitter_merged.csv', encoding='utf-8')\n",
      "    else:\n",
      "        social_w_Urls = socialMedia_DF.merge(socialMedia_mergedUrls_DF, left_on='fb_expandedUrl', right_on='FB_ShortenedUrl', how=\"inner\")\n",
      "        social_w_Urls = social_w_Urls.drop_duplicates(cols=['fb_expandedUrl', 'fb_postTime'] , take_last=True)\n",
      "        social_w_Urls.to_csv(destFolder+'1. Intermediates/PreSum/'+orgName+'_PreSumSocialwUrls_facebook.csv', encoding='utf-8')\n",
      "        social_w_Urls = social_w_Urls.groupby(['fb_expandedUrl']).agg([np.sum, len])\n",
      "        social_w_Urls.to_csv(destFolder+'1. Intermediates/'+orgName+'_SocialwUrls_facebook.csv', encoding='utf-8')\n",
      "\n",
      "        DF_summed = socialMedia_DF.merge(social_w_Urls[['fb_likes', 'fb_share', 'fb_text', 'fb_postTime']], \n",
      "                                   left_on='fb_expandedUrl', right_index=True , how=\"inner\")\n",
      "        DF_summed.columns = ['fb_expandedUrl', 'fb_likes', 'fb_share', 'fb_text', 'fb_postTime',\n",
      "                                 'fb_likes_Sum', 'fb_likes_Count', 'fb_share_Sum', 'fb_share_Count',\n",
      "                                 'fb_text_Sum', 'fb_text_Count', 'fb_text_Sum', 'fb_article_appearance_Count']\n",
      "        DF_summed = DF_summed.drop(['fb_likes_Count', 'fb_share_Count', 'fb_text_Count'], axis=1)\n",
      "        DF_summed = DF_summed.drop_duplicates(cols='fb_expandedUrl', take_last=True)\n",
      "        DF_summed.to_csv(destFolder+'1. Intermediates/'+orgName+'_SocialwUrls_facebook_merged.csv', encoding='utf-8')\n",
      "    return DF_summed"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Function used to extract urls from the json objects into a dataframe\n",
      "def mergeSocialMetricsData(mergedUrlsInputFolder, socialMetricsInputFolder, bitlyInputFolder, orgNames, destFolder):\n",
      "    rowCount = 0\n",
      "    facebookLinks = 0 # fb photo + video links\n",
      "    facebookCount = 0 # all fb links\n",
      "    facebookDict = {}\n",
      "    tw_repostCounts = 0\n",
      "    fb_repostCounts = 0\n",
      "    \n",
      "    #print columnNames\n",
      "    \n",
      "    for orgName in orgNames:\n",
      "        print orgName\n",
      "        fb_DF = pd.read_csv( socialMetricsInputFolder+orgName+'_facebook.csv', index_col=0)\n",
      "        tw_DF = pd.read_csv( socialMetricsInputFolder+orgName+'_twitter.csv', index_col=0)\n",
      "        bitly_tw = pd.read_csv( bitlyInputFolder+'1. extractedBitly/'+orgName+'_twitter.csv', index_col=0)\n",
      "        bitly_fb = pd.read_csv( bitlyInputFolder+'1. extractedBitly/'+orgName+'_facebook.csv', index_col=0)\n",
      "        mergedUrls_DF = pd.read_csv( mergedUrlsInputFolder+orgName+'_merged.csv', index_col=0)\n",
      "        mergedUrls_DF = mergedUrls_DF.drop_duplicates(cols='TW_matchUrl', take_last=False)\n",
      "        \n",
      "        #Merging fb and tw data\n",
      "        tw_DF_summed = sumSocialMetrics('twitter', tw_DF, mergedUrls_DF, destFolder, orgName)\n",
      "        fb_DF_summed = sumSocialMetrics('facebook', fb_DF, mergedUrls_DF, destFolder, orgName)\n",
      "        finalmerged_DF = tw_DF_summed.merge(mergedUrls_DF, left_on='tw_expandedUrl', right_on='TW_ShortenedUrl', how=\"inner\")\n",
      "        finalmerged_DF = finalmerged_DF.merge(fb_DF_summed, left_on='FB_ShortenedUrl', right_on='fb_expandedUrl', how=\"inner\")\n",
      "        \n",
      "        # Bitly\n",
      "        finalmerged_DF = finalmerged_DF.merge(bitly_tw, left_on='TW_ShortenedUrl', right_on='tw_url', how=\"left\", left_index=True)\n",
      "        finalmerged_DF = finalmerged_DF.merge(bitly_fb, left_on='FB_ShortenedUrl', right_on='fb_url', how=\"left\", left_index=True)\n",
      "        \n",
      "        # Rename column names\n",
      "        finalmerged_DF.rename(columns={'tw_article_appearance_Count_x':'tw_article_appearance_Count', \n",
      "                                       'FB_HistoryUrls_x':'FB_HistoryUrls',\n",
      "                                       'fb_article_appearance_Count_x':'fb_article_appearance_Count',\n",
      "                                       'tw_timeRetrieved_x':'tw_timeRetrieved'}, inplace=True)\n",
      "        # To csv\n",
      "        finalmerged_DF.to_csv(destFolder+orgName+'_finaltable.csv', encoding='utf-8')\n",
      "        rowCount += finalmerged_DF.shape[0]\n",
      "        print finalmerged_DF.shape\n",
      "        tw_repostCounts += finalmerged_DF[finalmerged_DF['tw_article_appearance_Count'] > 1].count()['tw_expandedUrl']\n",
      "        fb_repostCounts += finalmerged_DF[finalmerged_DF.fb_article_appearance_Count > 1].count()['tw_expandedUrl']\n",
      "        \n",
      "        \n",
      "    print \"Number of articles: %d\" % rowCount\n",
      "    print \"Number of tw reposts: %d\" % tw_repostCounts\n",
      "    print \"Number of fb reposts: %d\" % fb_repostCounts\n",
      "    return fb_DF, tw_DF, mergedUrls_DF, finalmerged_DF\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "organizationNames[3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_fb_DF, test_tw_DF, test_mergedUrls_DF, test_finalmerged_DF = mergeSocialMetricsData(pathMergedUrls, pathSocialMediaMetrics, \n",
      "                                                                                         pathBitly, [organizationNames[3], organizationNames[15]], pathFinalTable)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_fb_DF.head(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_tw_DF.head(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_mergedUrls_DF.head(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(test_finalmerged_DF)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_finalmerged_DF.head(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_mergedUrls_DF.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_finalmerged_DF.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Full Run\n",
      "fb_DF, tw_DF, mergedUrls_DF, finalmerged_DF = mergeSocialMetricsData(pathMergedUrls, pathSocialMediaMetrics, \n",
      "                                                                                         pathBitly, organizationNames, pathFinalTable)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}