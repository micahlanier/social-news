{
 "metadata": {
  "name": "",
  "signature": "sha256:0237cf2c4776bf5515607b3140debed838babe61456d85056a89d69e3e83b761"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Libraries.\n",
      "import json\n",
      "import os\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from urlparse import urlparse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Imports.\n",
      "import re\n",
      "\n",
      "# Define pattern.\n",
      "urlPattern = '(?i)\\\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\\\s()<>{}\\\\[\\\\]]+|\\\\([^\\\\s()]*?\\\\([^\\\\s()]+\\\\)[^\\\\s()]*?\\\\)|\\\\([^\\\\s]+?\\\\))+(?:\\\\([^\\\\s()]*?\\\\([^\\\\s()]+\\\\)[^\\\\s()]*?\\\\)|\\\\([^\\\\s]+?\\\\)|[^\\\\s`!()\\\\[\\\\]{};:\\'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019])|(?:(?<!@)[a-z0-9]+(?:[.\\\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\\\b/?(?!@)))'\n",
      "\n",
      "\"\"\"\n",
      "Detect if a URL is a link to a Facebook photo.\n",
      "\n",
      "Examples:\n",
      "\turlIsFacebookPhoto('https://www.facebook.com/bbcworldnews/photos/pcb.10153822033402588/10153822032512588/?type=1') == True\n",
      "\turlIsFacebookPhoto('http://bbc.in/fgh543q') == False\t\n",
      "\"\"\"\n",
      "def urlIsFacebookPhoto(url):\n",
      "\treturn bool(re.match('^https?://(www\\\\.)?facebook.com/[a-zA-Z0-9]+/photos/.+$', url))\n",
      "\n",
      "\"\"\"\n",
      "Detect if a URL is a link to a Facebook video.\n",
      "\n",
      "Examples:\n",
      "\turlIsFacebookPhoto('https://www.facebook.com/video.php?v=10150471411394999') == True\n",
      "\turlIsFacebookPhoto('http://bbc.in/fgh543q') == False\t\n",
      "\"\"\"\n",
      "def urlIsFacebookVideo(url):\n",
      "\treturn bool(re.match('^https?://(www\\\\.)?facebook.com/video.php\\\\?.+$', url))\n",
      "\n",
      "\"\"\"\n",
      "TODO\n",
      "\"\"\"\n",
      "def urlsInText(t):\n",
      "\t# Perform search.\n",
      "\turlMatches = re.search(urlPattern, t)\n",
      "\tif urlMatches:\n",
      "\t\treturn [g for g in urlMatches.groups()]\n",
      "\treturn []\n",
      "\n",
      "def urlIsFacebookLink(url):\n",
      "    return bool(re.match('^https?://(www\\\\.)?facebook.com.+$', url))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Setup File Paths#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sources = ['twitter','facebook']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"The current working directory is\", os.getcwd()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\\Social News\\data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pathConsolidated = os.getcwd()+'/Social News/data/3. urls/%s/'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pathExtractedUrl = os.getcwd()+'/Social News/data/4. extractedUrl/'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pathMergedUrl = os.getcwd()+'/Social News/data/5. mergedUrls/'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get all files to traverse. Assume that we can get everything in consolidated folder.\n",
      "postFiles = []\n",
      "for source in sources:\n",
      "    sourcePath = pathConsolidated % source\n",
      "    postFiles += [(source,p[:p.find('-')],sourcePath + p) for p in os.listdir(sourcePath) if p[-5:] == '.json']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "postFiles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "organizationNames = [x[1].split(\".\")[0] for x in postFiles]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "organizationNames = set(organizationNames)\n",
      "organizationNames = list(organizationNames)\n",
      "organizationNames.sort()\n",
      "len(organizationNames)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Data Wrangling#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "columns = ['NewsOrganization', 'PathUrl','matchUrl', 'FullUrl', 'ShortenedUrl', 'HistoryUrls']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "columns_tw = [\"TW_\" + name for name in columns]\n",
      "columns_fb = [\"FB_\" + name for name in columns]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "columns_tw"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "columns_fb"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Function used to extract urls from the json objects into a dataframe\n",
      "def extractUrltoDF(filenames, columnNames, socialMedia, destFolder):\n",
      "    rowCount = 0\n",
      "    facebookLinks = 0 # fb photo + video links\n",
      "    facebookCount = 0 # all fb links\n",
      "    facebookDict = {}\n",
      "    \n",
      "    print columnNames\n",
      "    for organization in filenames:\n",
      "        resultDF = pd.DataFrame(columns=columnNames) \n",
      "        if organization[0] == socialMedia:\n",
      "            print organization[1]\n",
      "            #print organization[2]\n",
      "            data = json.load(open(organization[2]))\n",
      "\n",
      "            #Iterate over dictionary to construct Dataframe\n",
      "            for key, value in data.items():\n",
      "                #print key\n",
      "                if urlIsFacebookLink(key) and not (urlIsFacebookPhoto(key) or urlIsFacebookVideo(key)):\n",
      "                    facebookCount += 1\n",
      "                    #facebookArray.append(key)\n",
      "                    facebookDict[key] = organization[1]\n",
      "                if urlIsFacebookPhoto(key) or urlIsFacebookVideo(key):\n",
      "                    facebookLinks += 1\n",
      "                else:\n",
      "                    newOrganization = value['endUrl']['netloc']\n",
      "                    pathUrl = value['endUrl']['path']\n",
      "                    matchUrl = newOrganization+pathUrl\n",
      "                    fullUrl = value['endUrl']['url']\n",
      "                    shortenedUrl = key\n",
      "                    historyUrl = \"\"\n",
      "                    for hUrl in value['history']:\n",
      "                        historyUrl = historyUrl+\"???\"+hUrl\n",
      "                    #Save results\n",
      "                    resultDF.loc[rowCount] = [newOrganization, pathUrl, matchUrl, fullUrl, shortenedUrl, historyUrl]\n",
      "                    rowCount += 1\n",
      "            filename = organization[1].split('.')[0]\n",
      "            resultDF.to_csv(destFolder+filename+'_'+socialMedia+'.csv', encoding='utf-8')\n",
      "            del resultDF\n",
      "    print \"Number of rows: %d\" % rowCount\n",
      "    print \"Number of fb photo + video: %d\" % facebookLinks\n",
      "    print \"Number of fb links: %d\" % facebookCount\n",
      "    return rowCount, facebookDict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fbDF, fbArray = extractUrltoDF([postFiles[28]], columns_fb, 'facebook', pathExtractedUrl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fbArray"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#data['https://www.facebook.com/bbcworldnews/photos/pcb.10153737351557588/10153737348007588/?type=1&relevant_count=2']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#data.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#data.values()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#len(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "postFiles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "postFiles[28]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Twitter Data#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "postFiles[15]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#NYTimes example\n",
      "twitterDF = extractUrltoDF([postFiles[15]], columns_tw, 'twitter', pathExtractedUrl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Run all\n",
      "twitterDF = extractUrltoDF(postFiles, columns_tw, 'twitter', pathExtractedUrl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Facebook Data#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#NYTimes example\n",
      "fbDF = extractUrltoDF([postFiles[40]], columns_fb, 'facebook', pathExtractedUrl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pathExtractedUrl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "postFiles[28]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Run all\n",
      "fbDF, notCapturedfbLinks = extractUrltoDF(postFiles, columns_fb, 'facebook', pathExtractedUrl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "notCapturedfbLinks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Load in Processed Data from above#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mergeUrlFiles(inputFolder, outputFolder, orgNames):\n",
      "    \n",
      "    summaryDF = pd.DataFrame(columns=['newsOrg', 'tw_total', 'tw_unique', 'tw_repeat', \n",
      "                                      'fb_total', 'fb_unique', 'fb_repeat', 'merged_unique']) \n",
      "    rowCount = 0\n",
      "    for orgName in orgNames:\n",
      "        print orgName\n",
      "        fb_DF = pd.read_csv( inputFolder+orgName+'_facebook.csv', index_col=0)\n",
      "        fb_DF_unique = fb_DF.drop_duplicates(cols='FB_matchUrl', take_last=False)\n",
      "        tw_DF = pd.read_csv( inputFolder+orgName+'_twitter.csv', index_col=0)\n",
      "        tw_DF_unique = tw_DF.drop_duplicates(cols='TW_matchUrl', take_last=False)\n",
      "        \n",
      "        #\n",
      "        fb_series = fb_DF['FB_matchUrl'].value_counts()\n",
      "        fb_url_count = fb_series.to_frame('Counts')\n",
      "        fb_url_count.to_csv(outputFolder+'DupStat/dup_count_'+orgName+'_fb.csv', encoding='utf-8')\n",
      "        tw_series = tw_DF['TW_matchUrl'].value_counts()\n",
      "        tw_url_count = tw_series.to_frame('Counts')\n",
      "        tw_url_count.to_csv(outputFolder+'DupStat/dup_count_'+orgName+'_tw.csv', encoding='utf-8')\n",
      "        \n",
      "        #tw.groupby('TW_matchUrl').count()\n",
      "        #print fb_DF\n",
      "        #print tw_DF\n",
      "        merged = pd.merge(tw_DF, fb_DF, left_on='TW_matchUrl', right_on='FB_matchUrl', how=\"inner\")\n",
      "        merged.to_csv(outputFolder+orgName+'_merged.csv', encoding='utf-8')\n",
      "        merged_unique = merged.drop_duplicates(cols='TW_matchUrl', take_last=False)\n",
      "        \n",
      "        #Save results\n",
      "        summaryDF.loc[rowCount] = [orgName, tw_DF.shape[0],tw_DF_unique.shape[0],tw_DF.shape[0]-tw_DF_unique.shape[0],\n",
      "                                   fb_DF.shape[0],fb_DF_unique.shape[0],fb_DF.shape[0]-fb_DF_unique.shape[0],\n",
      "                                   merged_unique.shape[0]]\n",
      "        rowCount += 1\n",
      "    return summaryDF\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "summaryStat = mergeUrlFiles(pathExtractedUrl, pathMergedUrl, ['nytimes'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "summaryStat.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "summaryStat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "summaryStat = mergeUrlFiles(pathExtractedUrl, pathMergedUrl, organizationNames)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "minUnique = pd.DataFrame([summaryStat.tw_unique, summaryStat.fb_unique]).min()\n",
      "minUnique.values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "summaryStat['MergePercent'] = summaryStat.merged_unique/minUnique.values*100\n",
      "#TODO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "summaryStat.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "summaryStat = summaryStat.sort('MergePercent')\n",
      "summaryStat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "summaryStat.merged_unique.sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "summaryStat.to_csv(\"summaryStat.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dup_DF = pd.read_csv( pathMergedUrl+'DupStat/dup_count_'+'nytimes'+'_fb.csv', index_col=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dup_DF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}