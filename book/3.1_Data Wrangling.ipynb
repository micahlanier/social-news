{
 "metadata": {
  "name": "",
  "signature": "sha256:b6475a0aab91629bf3e18348c596847b6687d559c497a6e6922e0fd24482704d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Summary:\n",
      "\n",
      "###The following code extracts URL data for twitter and facebook posts. Then, it finds the same articles that are posted both on twitter and facebook###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Libraries.\n",
      "import json\n",
      "import os\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from urlparse import urlparse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Imports.\n",
      "import re\n",
      "\n",
      "# Define pattern.\n",
      "urlPattern = '(?i)\\\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\\\s()<>{}\\\\[\\\\]]+|\\\\([^\\\\s()]*?\\\\([^\\\\s()]+\\\\)[^\\\\s()]*?\\\\)|\\\\([^\\\\s]+?\\\\))+(?:\\\\([^\\\\s()]*?\\\\([^\\\\s()]+\\\\)[^\\\\s()]*?\\\\)|\\\\([^\\\\s]+?\\\\)|[^\\\\s`!()\\\\[\\\\]{};:\\'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019])|(?:(?<!@)[a-z0-9]+(?:[.\\\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\\\b/?(?!@)))'\n",
      "\n",
      "\"\"\"\n",
      "Detect if a URL is a link to a Facebook photo.\n",
      "\n",
      "Examples:\n",
      "\turlIsFacebookPhoto('https://www.facebook.com/bbcworldnews/photos/pcb.10153822033402588/10153822032512588/?type=1') == True\n",
      "\turlIsFacebookPhoto('http://bbc.in/fgh543q') == False\t\n",
      "\"\"\"\n",
      "def urlIsFacebookPhoto(url):\n",
      "\treturn bool(re.match('^https?://(www\\\\.)?facebook.com/[a-zA-Z0-9]+/photos/.+$', url))\n",
      "\n",
      "\"\"\"\n",
      "Detect if a URL is a link to a Facebook video.\n",
      "\n",
      "Examples:\n",
      "\turlIsFacebookPhoto('https://www.facebook.com/video.php?v=10150471411394999') == True\n",
      "\turlIsFacebookPhoto('http://bbc.in/fgh543q') == False\t\n",
      "\"\"\"\n",
      "def urlIsFacebookVideo(url):\n",
      "\treturn bool(re.match('^https?://(www\\\\.)?facebook.com/video.php\\\\?.+$', url))\n",
      "\n",
      "\"\"\"\n",
      "TODO\n",
      "\"\"\"\n",
      "def urlsInText(t):\n",
      "\t# Perform search.\n",
      "\turlMatches = re.search(urlPattern, t)\n",
      "\tif urlMatches:\n",
      "\t\treturn [g for g in urlMatches.groups()]\n",
      "\treturn []\n",
      "\n",
      "def urlIsFacebookLink(url):\n",
      "    return bool(re.match('^https?://(www\\\\.)?facebook.com.+$', url))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Setup File Paths#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sources = ['twitter','facebook']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"The current working directory is\", os.getcwd()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The current working directory is C:\\Users\\JS\\OneDrive\\Documents\\Harvard\\1. AM 209 (Data Science)\\Final Project\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pathConsolidated = os.getcwd()+'/Social News/data/3. urls/%s/'\n",
      "pathExtractedUrl = os.getcwd()+'/Social News/data/4. extractedUrl/'\n",
      "pathMergedUrl = os.getcwd()+'/Social News/data/5. mergedUrls/'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get all files to traverse. Assume that we can get everything in consolidated folder.\n",
      "postFiles = []\n",
      "for source in sources:\n",
      "    sourcePath = pathConsolidated % source\n",
      "    postFiles += [(source,p[:p.find('-')],sourcePath + p) for p in os.listdir(sourcePath) if p[-5:] == '.json']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Uncomment to see all files read in\n",
      "#postFiles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Extract Organization Names\n",
      "organizationNames = [x[1].split(\".\")[0] for x in postFiles]\n",
      "organizationNames = set(organizationNames)\n",
      "organizationNames = list(organizationNames)\n",
      "organizationNames.sort()\n",
      "len(organizationNames)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "25"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Data Wrangling#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "columns = ['NewsOrganization', 'PathUrl','matchUrl', 'FullUrl', 'ShortenedUrl', 'HistoryUrls']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "columns_tw = [\"TW_\" + name for name in columns]\n",
      "columns_fb = [\"FB_\" + name for name in columns]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Function used to extract urls from the json objects into a dataframe\n",
      "param[in] filenames: folder+filenames of the json objects\n",
      "param[in] columnNames: column names for the resulting dataframe\n",
      "param[in] socialMedia: 'twitter' or 'facebook'\n",
      "param[in] destFolder: destination folder to save output csvs\n",
      "\"\"\"\n",
      "def extractUrltoDF(filenames, columnNames, socialMedia, destFolder):\n",
      "    rowCount = 0\n",
      "    facebookLinks = 0 # fb photo + video links\n",
      "    facebookCount = 0 # all fb links\n",
      "    facebookDict = {}\n",
      "    \n",
      "    print columnNames\n",
      "    # Loop through each organization\n",
      "    for organization in filenames:\n",
      "        resultDF = pd.DataFrame(columns=columnNames) \n",
      "        if organization[0] == socialMedia:\n",
      "            print organization[1]\n",
      "            #print organization[2]\n",
      "            data = json.load(open(organization[2]))\n",
      "\n",
      "            #Iterate over dictionary to construct Dataframe\n",
      "            for key, value in data.items():\n",
      "                #print key\n",
      "                if urlIsFacebookLink(key) and not (urlIsFacebookPhoto(key) or urlIsFacebookVideo(key)):\n",
      "                    facebookCount += 1\n",
      "                    facebookDict[key] = organization[1]\n",
      "                if urlIsFacebookPhoto(key) or urlIsFacebookVideo(key):\n",
      "                    facebookLinks += 1\n",
      "                else:\n",
      "                    newOrganization = value['endUrl']['netloc']\n",
      "                    pathUrl = value['endUrl']['path']\n",
      "                    matchUrl = newOrganization+pathUrl\n",
      "                    fullUrl = value['endUrl']['url']\n",
      "                    shortenedUrl = key\n",
      "                    historyUrl = \"\"\n",
      "                    for hUrl in value['history']:\n",
      "                        historyUrl = historyUrl+\"???\"+hUrl\n",
      "                    #Save results\n",
      "                    resultDF.loc[rowCount] = [newOrganization, pathUrl, matchUrl, fullUrl, shortenedUrl, historyUrl]\n",
      "                    rowCount += 1\n",
      "            filename = organization[1].split('.')[0]\n",
      "            resultDF.to_csv(destFolder+filename+'_'+socialMedia+'.csv', encoding='utf-8')\n",
      "            del resultDF\n",
      "    print \"Number of rows: %d\" % rowCount\n",
      "    print \"Number of fb photo + video: %d\" % facebookLinks\n",
      "    print \"Number of fb links: %d\" % facebookCount\n",
      "    return rowCount, facebookDict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fbDF, fbArray = extractUrltoDF([postFiles[28]], columns_fb, 'facebook', pathExtractedUrl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['FB_NewsOrganization', 'FB_PathUrl', 'FB_matchUrl', 'FB_FullUrl', 'FB_ShortenedUrl', 'FB_HistoryUrls']\n",
        "bbc.jso\n",
        "Number of rows: 1620"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of fb photo + video: 1458\n",
        "Number of fb links: 13\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Twitter Data#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#NYTimes example\n",
      "#twitterDF = extractUrltoDF([postFiles[15]], columns_tw, 'twitter', pathExtractedUrl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Run all\n",
      "twitterDF = extractUrltoDF(postFiles, columns_tw, 'twitter', pathExtractedUrl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['TW_NewsOrganization', 'TW_PathUrl', 'TW_matchUrl', 'TW_FullUrl', 'TW_ShortenedUrl', 'TW_HistoryUrls']\n",
        "abcnews.jso\n",
        "afp.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ap.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "bbc.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "bostonglobe.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cbsnews.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cnn.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dailymail.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "foxnews.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "huffingtonpost.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "latimes.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "msnbc.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "nbcnews.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "newsweek.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "npr.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "nytimes.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "reuters.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "slate.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "thedailybeast.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "theguardian.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "time.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "usatoday.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "washingtonpost.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "wsj.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "yahoonews.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of rows: 128673"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of fb photo + video: 6\n",
        "Number of fb links: 21\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Facebook Data#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#NYTimes example\n",
      "#fbDF = extractUrltoDF([postFiles[40]], columns_fb, 'facebook', pathExtractedUrl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Run all\n",
      "fbDF, notCapturedfbLinks = extractUrltoDF(postFiles, columns_fb, 'facebook', pathExtractedUrl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['FB_NewsOrganization', 'FB_PathUrl', 'FB_matchUrl', 'FB_FullUrl', 'FB_ShortenedUrl', 'FB_HistoryUrls']\n",
        "abcnews.jso\n",
        "afp.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ap.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "bbc.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "bostonglobe.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cbsnews.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cnn.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dailymail.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "foxnews.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "huffingtonpost.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "latimes.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "msnbc.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "nbcnews.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "newsweek.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "npr.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "nytimes.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "reuters.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "slate.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "thedailybeast.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "theguardian.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "time.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "usatoday.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "washingtonpost.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "wsj.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "yahoonews.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of rows: 60283"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of fb photo + video: 7410\n",
        "Number of fb links: 1842\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Merge Twitter and Facebook URLs#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Function merges twitter and facebook article URLs\n",
      "param[in] inputFolder: folder+filenames of the extracted url csv files\n",
      "param[in] outputFolder: destination to save the output csv files\n",
      "param[in] orgNames: names of the news organizations used to read and save csv files\n",
      "\"\"\"\n",
      "def mergeUrlFiles(inputFolder, outputFolder, orgNames):\n",
      "    \n",
      "    summaryDF = pd.DataFrame(columns=['newsOrg', 'tw_total', 'tw_unique', 'tw_repeat', \n",
      "                                      'fb_total', 'fb_unique', 'fb_repeat', 'merged_unique']) \n",
      "    rowCount = 0\n",
      "    for orgName in orgNames:\n",
      "        print orgName\n",
      "        # Read in fb and tw url files and drop duplicates\n",
      "        fb_DF = pd.read_csv( inputFolder+orgName+'_facebook.csv', index_col=0)\n",
      "        fb_DF_unique = fb_DF.drop_duplicates(cols='FB_matchUrl', take_last=False)\n",
      "        tw_DF = pd.read_csv( inputFolder+orgName+'_twitter.csv', index_col=0)\n",
      "        tw_DF_unique = tw_DF.drop_duplicates(cols='TW_matchUrl', take_last=False)\n",
      "        \n",
      "        # Calculate number of duplicates\n",
      "        fb_series = fb_DF['FB_matchUrl'].value_counts()\n",
      "        fb_url_count = fb_series.to_frame('Counts')\n",
      "        fb_url_count.to_csv(outputFolder+'DupStat/dup_count_'+orgName+'_fb.csv', encoding='utf-8')\n",
      "        tw_series = tw_DF['TW_matchUrl'].value_counts()\n",
      "        tw_url_count = tw_series.to_frame('Counts')\n",
      "        tw_url_count.to_csv(outputFolder+'DupStat/dup_count_'+orgName+'_tw.csv', encoding='utf-8')\n",
      "        \n",
      "        # Merge both twitter and facebook data\n",
      "        merged = pd.merge(tw_DF, fb_DF, left_on='TW_matchUrl', right_on='FB_matchUrl', how=\"inner\")\n",
      "        merged.to_csv(outputFolder+orgName+'_merged.csv', encoding='utf-8')\n",
      "        merged_unique = merged.drop_duplicates(cols='TW_matchUrl', take_last=False)\n",
      "        \n",
      "        #Save results\n",
      "        summaryDF.loc[rowCount] = [orgName, tw_DF.shape[0],tw_DF_unique.shape[0],tw_DF.shape[0]-tw_DF_unique.shape[0],\n",
      "                                   fb_DF.shape[0],fb_DF_unique.shape[0],fb_DF.shape[0]-fb_DF_unique.shape[0],\n",
      "                                   merged_unique.shape[0]]\n",
      "        rowCount += 1\n",
      "    return summaryDF\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "summaryStat = mergeUrlFiles(pathExtractedUrl, pathMergedUrl, organizationNames)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "abcnews\n",
        "afp"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ap"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "bbc"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "bostonglobe"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cbsnews"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cnn"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dailymail"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "foxnews"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "huffingtonpost"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "latimes"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "msnbc"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "nbcnews"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "newsweek"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "npr"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "nytimes"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "reuters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "slate"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "thedailybeast"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "theguardian"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "time"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "usatoday"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "washingtonpost"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "wsj"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "yahoonews"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Users\\JS\\Anaconda\\lib\\site-packages\\pandas\\util\\decorators.py:53: FutureWarning: cols is deprecated, use subset instead\n",
        "  warnings.warn(msg, FutureWarning)\n"
       ]
      }
     ],
     "prompt_number": 18
    }
   ],
   "metadata": {}
  }
 ]
}