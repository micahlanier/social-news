{
 "metadata": {
  "name": "",
  "signature": "sha256:8e27e8e915b49e3dc9da1401734e035f60e22ffbfea59e3c13242232067603c4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<a id=\"s8\"></a>Predictive Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The richness of the data that we collected ought to be ideal for predictive analysis problems surrouning social media outcomes. We hoped that building such models would enable us to predict social media success (in terms of link clicks, and network-specific engagement like retweets and likes) and explain it in terms of strategic adjustments that news organizations might enact. This section describes the modeling process itself; [the next section](#9) delves into the strategic implications of our analysis.\n",
      "\n",
      "To accomplish a balance of interpretability and predictive power, we attempted to fit both Ridge Regression and Random Forest Regression models to various combinations of features and outcomes. We attempted to fit Support Vector Regression models as well, but found that our understanding of the model and the difficulties of tuning it compromised our ability to generate useful results. Also, after an early forray into predicting \"aggregate\" outcomes between *both* social networks, it quickly became apparent that generating separate models\u2014one for Facebook and one for Twitter\u2014would yield better predictions and better-accommodate network-specific features (e.g., followers, photo/video classification, etc.).\n",
      "\n",
      "Finally, we employed standard cross-validation techniques discussed in class to tune parameters: the $\\alpha$ regularization parameter in the case of Ridge Regression and the number of estimators in the Random Forest model. Linear regression models were tuned using `RidgeCV`, while the Random Forest model relied on out-of-bag validation."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scripts in this section utilize the following libraries for processing and prediction."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from dateutil.parser import parse as dp\n",
      "import json\n",
      "from nltk import word_tokenize as tokenize\n",
      "import os\n",
      "import pandas as pd\n",
      "import pytz"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Data for Prediction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We encountered difficulty repurposing the \"merged\" dataset from past analysis for use in prediction, as it did not retain all of the features that we expected to be useful for predictive purposes. The following code aims to summarize all Twitter, Facebook, URL, Bitly, and sentiment data for prediction. When calculating click outcomes, it makes several adjustments for multi-posted links, aiming to properly attribute link traffic to posts. It also performs additional cleaning, such as removing clicks to retweeted articles entirely."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "\n",
      "This script combines Twitter and Facebook data with Bitly, URL, and sentiment information specifically for purposes of prediction.\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "##### Main Setup\n",
      "\n",
      "# Data directories.\n",
      "dirPath_consolidatedTw = '../../../data/twitter/consolidated/'\n",
      "dirPath_consolidatedFb = '../../../data/facebook/consolidated/'\n",
      "dirPath_bitly = '../../../data/bitly/%s/'\n",
      "dirPath_sentiment = '../../../data/sentiment/%s/'\n",
      "dirPath_urls = '../../../data/urls/%s/'\n",
      "filePath_dates = '../../conf/dates.json'\n",
      "filePath_output = '../../../data/unmergedPostSummaries/%s.csv'\n",
      "filePath_users = '../../data/users/%s.json'\n",
      "\n",
      "# Get organizational information.\n",
      "orgs = json.load(open('../../conf/organizations.json'))\n",
      "\n",
      "# Get account information.\n",
      "twAccounts = json.load(open(filePath_users % 'twitter'))\n",
      "fbAccounts = json.load(open(filePath_users % 'facebook'))\n",
      "\n",
      "# Tokens to skip for word count.\n",
      "# This is an inherently difficult task, but this should help us get to where we need to go.\n",
      "skipTokens = {'\\'s','.','?',',','(',')','#','\\\\',':',';','@','-','\\''}\n",
      "\n",
      "# Facebook status type mapping.\n",
      "fbStatusTypeToMedium = {'added_photos': 'photo', 'added_video': 'video'}\n",
      "\n",
      "# Information about proper dates.\n",
      "dates = json.load(open(filePath_dates))\n",
      "twStartDate = dp(dates['twitter']['start'])\n",
      "twEndDate   = dp(dates['twitter']['end'])\n",
      "fbStartDate  = dp(dates['facebook']['start'])\n",
      "fbEndDate    = dp(dates['facebook']['end'])\n",
      "\n",
      "# Get local timezone.\n",
      "localTz = pytz.timezone('America/New_York')\n",
      "\n",
      "# URL counts. Allows us to adjust URL traffic.\n",
      "urlCounts = dict()        # Click counts by URL.\n",
      "urlNetworks = dict()    # Distinct networks by URL.\n",
      "retweetedUrls = set()    # Set of all retweets.\n",
      "\n",
      "# Containers for statistics that will become DFs.\n",
      "tweetInfo = []\n",
      "postInfo  = []\n",
      "\n",
      "##### Helper Functions\n",
      "\n",
      "\"\"\"\n",
      "Remove URLs, clean syntax.\n",
      "\"\"\"\n",
      "def cleanMessage(message):\n",
      "    # Remove Twitter-specific syntax.\n",
      "    message = message.replace('@','')\n",
      "    message = message.replace('#','')\n",
      "    # Remove links.\n",
      "    links = socialUrlUtils.urlsInText(message)\n",
      "    # links.reverse()\n",
      "    for link in links:\n",
      "        message = message.replace(link,'')\n",
      "    # Return.\n",
      "    return message.strip()\n",
      "\n",
      "##### Main Execution\n",
      "\n",
      "# Traverse all organizations.\n",
      "for org, orgData in orgs.iteritems():\n",
      "    # Org setup.\n",
      "    orgName = orgData['name']\n",
      "\n",
      "    # Get all relevant Twitter information.\n",
      "    service   = 'twitter'\n",
      "    tweets    = json.load(open(dirPath_consolidatedTw + [f for f in os.listdir(dirPath_consolidatedTw) if f.startswith(org)][-1]))\n",
      "    bitly     = json.load(open((dirPath_bitly % service) + org + '.json'))\n",
      "    sentiment = json.load(open((dirPath_sentiment % service) + org + '.json'))\n",
      "    urls      = json.load(open((dirPath_urls % service) + org + '.json'))\n",
      "\n",
      "    # Traverse tweets.\n",
      "    for tweet in tweets:\n",
      "        # Get some initial information.\n",
      "        expandedUrls = [u['expanded_url'] for u in tweet['entities']['urls']]\n",
      "        # Ensure date in proper range. We want to exclude everything outside of it.\n",
      "        created_utc = dp(tweet['created_at'])\n",
      "        if created_utc < twStartDate or created_utc >= twEndDate:\n",
      "            continue\n",
      "\n",
      "        # Container to store stats that will eventually become a data frame.\n",
      "        thisTweet = { 'service': service, 'id': tweet['id']}\n",
      "\n",
      "        # Tweet features.\n",
      "        thisTweet['text'] = tweet['text']\n",
      "        thisTweet['hashtags'] = len(tweet['entities']['hashtags'])\n",
      "        thisTweet['retweeted'] = ('retweeted_status' in tweet or tweet['text'].startswith('RT '))\n",
      "        media = tweet['entities'].get('media')\n",
      "        thisTweet['medium'] = media[0]['type'] if media else 'text'\n",
      "        # Tweet datetime features.\n",
      "        thisTweet['date_utc'] = str(created_utc.date())\n",
      "        thisTweet['day_of_week_utc'] = created_utc.weekday()\n",
      "        thisTweet['weekend_utc'] = thisTweet['day_of_week_utc'] >= 5\n",
      "        thisTweet['time_utc'] = float(created_utc.hour) + float(created_utc.minute)/60\n",
      "        thisTweet['minute_utc'] = float(created_utc.minute)\n",
      "        thisTweet['day_time_utc'] = thisTweet['day_of_week_utc'] + float(created_utc.hour) / 24.0\n",
      "        created_est = created_utc.replace(tzinfo=pytz.utc).astimezone(localTz)\n",
      "        thisTweet['date_est'] = str(created_est.date())\n",
      "        thisTweet['day_of_week_est'] = created_est.weekday()\n",
      "        thisTweet['weekend_est'] = thisTweet['day_of_week_est'] >= 5\n",
      "        thisTweet['time_est'] = float(created_est.hour) + float(created_est.minute)/60\n",
      "        thisTweet['minute_est'] = float(created_est.minute)\n",
      "        thisTweet['day_time_est'] = thisTweet['day_of_week_est'] + float(created_est.hour) / 24.0\n",
      "        # Org features.\n",
      "        thisTweet['org'] = org\n",
      "        thisTweet['org_category'] = orgData['category'].lower()\n",
      "        thisTweet['social_flow_user'] = orgData['socialFlow']\n",
      "        thisTweet['followers_count'] = twAccounts[org]['followers_count']\n",
      "        # Sentiment features.\n",
      "        thisTweet['word_count'] = len([t for t in tokenize(cleanMessage(tweet['text'])) if t not in skipTokens])\n",
      "        sent = sentiment.get(str(tweet['id']))\n",
      "        if sent:\n",
      "            thisTweet['sentiment_class'] = sent['class']\n",
      "            thisTweet['sentiment_score_positive'] = sent['meanScorePosSig']\n",
      "            thisTweet['sentiment_score_negative'] = sent['meanScoreNegSig']\n",
      "        else:\n",
      "            thisTweet['sentiment_class'] = None\n",
      "            thisTweet['sentiment_score_positive'] = None\n",
      "            thisTweet['sentiment_score_negative'] = None\n",
      "\n",
      "        # Tweet outcomes.\n",
      "        thisTweet['favorites'] = tweet['favorite_count']\n",
      "        thisTweet['retweets']  = tweet['retweet_count']\n",
      "\n",
      "        # Bitly features. This is going to be tougher.\n",
      "        # Traverse all URLs and store those that we've traversed.\n",
      "        # We'll make a second pass later and clean up.\n",
      "        thisTweet['raw_clicks'] = dict()\n",
      "        for u in set(expandedUrls):\n",
      "            # Increment count in urlCounts.\n",
      "            urlCounts[u] = (urlCounts.get(u) or 0) + 1\n",
      "            # Identify retweeted URLs.\n",
      "            if (thisTweet['retweeted']):\n",
      "                retweetedUrls.add(u)\n",
      "            # Identify URL as posted on Twitter.\n",
      "            if (u not in urlNetworks): urlNetworks[u] = set()\n",
      "            urlNetworks[u].add(service)\n",
      "            # Update raw clicks.\n",
      "            thisTweet['raw_clicks'][u] = bitly[u]['data']['link_clicks'] if u in bitly and bitly[u]['data'] else None\n",
      "\n",
      "        # URL records. These are not really features, just JSON arrays.\n",
      "        tweetEndUrls = []\n",
      "        # Traverse URLs.\n",
      "        for u in set(expandedUrls):\n",
      "            # Find any for which we have end destinations.\n",
      "            if u in urls:\n",
      "                # If we do, find end URL information and append to urls field.\n",
      "                tweetEndUrls.append(urls[u]['endUrl']['netloc']+urls[u]['endUrl']['path'])\n",
      "        # Make a string.\n",
      "        thisTweet['urls'] = json.dumps(tweetEndUrls)\n",
      "\n",
      "        # Whew. Finally. Now append to the list.\n",
      "        tweetInfo.append(thisTweet)\n",
      "\n",
      "    # Get all relevant Facebook information.\n",
      "    service   = 'facebook'\n",
      "    posts     = json.load(open(dirPath_consolidatedFb + [f for f in os.listdir(dirPath_consolidatedFb) if f.startswith(org)][-1]))\n",
      "    bitly     = json.load(open((dirPath_bitly % service) + org + '.json'))\n",
      "    sentiment = json.load(open((dirPath_sentiment % service) + org + '.json'))\n",
      "    urls      = json.load(open((dirPath_urls % service) + org + '.json'))\n",
      "\n",
      "    # Traverse posts.\n",
      "    for post in posts:\n",
      "        # Get some initial information.\n",
      "        expandedUrls = []\n",
      "        if 'link' in post:\n",
      "            expandedUrls.append(post['link'])\n",
      "        if 'message' in post:\n",
      "            expandedUrls += socialUrlUtils.urlsInText(post['message'])\n",
      "        created_utc = dp(post['created_time'])\n",
      "        # Ensure date in proper range. We want to exclude everything outside of it.\n",
      "        if created_utc < fbStartDate or created_utc >= fbEndDate:\n",
      "            continue\n",
      "\n",
      "        # Container to store stats that will eventually become a data frame.\n",
      "        thisPost = { 'service': service, 'id': post['id']}\n",
      "\n",
      "        # Post features.\n",
      "        thisPost['text'] = post.get('message')\n",
      "        thisPost['medium'] = fbStatusTypeToMedium.get(post.get('status_type')) or 'other'\n",
      "        # Post datetime features.\n",
      "        thisPost['date_utc'] = str(created_utc.date())\n",
      "        thisPost['day_of_week_utc'] = created_utc.weekday()\n",
      "        thisPost['weekend_utc'] = thisPost['day_of_week_utc'] >= 5\n",
      "        thisPost['time_utc'] = float(created_utc.hour) + float(created_utc.minute)/60\n",
      "        thisPost['minute_utc'] = float(created_utc.minute)\n",
      "        thisPost['day_time_utc'] = thisPost['day_of_week_utc'] + float(created_utc.hour) / 24.0\n",
      "        created_est = created_utc.replace(tzinfo=pytz.utc).astimezone(localTz)\n",
      "        thisPost['date_est'] = str(created_est.date())\n",
      "        thisPost['day_of_week_est'] = created_est.weekday()\n",
      "        thisPost['weekend_est'] = thisPost['day_of_week_est'] >= 5\n",
      "        thisPost['time_est'] = float(created_est.hour) + float(created_est.minute)/60\n",
      "        thisPost['minute_est'] = float(created_est.minute)\n",
      "        thisPost['day_time_est'] = thisPost['day_of_week_est'] + float(created_est.hour) / 24.0\n",
      "        # Org features.\n",
      "        thisPost['org'] = org\n",
      "        thisPost['org_category'] = orgData['category'].lower()\n",
      "        thisPost['social_flow_user'] = orgData['socialFlow']\n",
      "        thisPost['account_likes'] = fbAccounts[org]['likes']\n",
      "        # Sentiment features.\n",
      "        thisPost['word_count'] = len([t for t in tokenize(cleanMessage(post['message'])) if t not in skipTokens]) if 'message' in post else 0\n",
      "        sent = sentiment.get(str(post['id']))\n",
      "        if sent:\n",
      "            thisPost['sentiment_class'] = sent['message']['class']\n",
      "            thisPost['sentiment_score_positive'] = sent['message']['meanScorePosSig']\n",
      "            thisPost['sentiment_score_negative'] = sent['message']['meanScoreNegSig']\n",
      "            thisPost['agg_sentiment_class'] = sent['agg']['class']\n",
      "            thisPost['agg_sentiment_score_positive'] = sent['agg']['meanScorePosSig']\n",
      "            thisPost['agg_sentiment_score_negative'] = sent['agg']['meanScoreNegSig']\n",
      "        else:\n",
      "            thisPost['message_sentiment_class'] = None\n",
      "            thisPost['message_sentiment_score_positive'] = None\n",
      "            thisPost['message_sentiment_score_negative'] = None\n",
      "            thisPost['agg_sentiment_class'] = None\n",
      "            thisPost['agg_sentiment_score_positive'] = None\n",
      "            thisPost['agg_sentiment_score_negative'] = None\n",
      "\n",
      "        # Post outcomes.\n",
      "        thisPost['comments'] = post['comments']['summary']['total_count'] if 'comments' in post else 0\n",
      "        thisPost['likes']    = post['likes']['summary']['total_count'] if 'likes' in post else 0\n",
      "        thisPost['shares']   = post['shares']['count'] if 'shares' in post else 0\n",
      "\n",
      "        # Bitly features. This is going to be tougher.\n",
      "        # Traverse all URLs and store those that we've traversed.\n",
      "        # We'll make a second pass later and clean up.\n",
      "        thisPost['raw_clicks'] = dict()\n",
      "        for u in set(expandedUrls):\n",
      "            # Increment count in urlCounts.\n",
      "            urlCounts[u] = (urlCounts.get(u) or 0) + 1\n",
      "            # Identify URL as posted on Twitter.\n",
      "            if (u not in urlNetworks): urlNetworks[u] = set()\n",
      "            urlNetworks[u].add(service)\n",
      "            # Update raw clicks.\n",
      "            thisPost['raw_clicks'][u] = bitly[u]['data']['link_clicks'] if u in bitly and bitly[u]['data'] else None\n",
      "\n",
      "        # URL records. These are not really features, just JSON arrays.\n",
      "        postEndUrls = []\n",
      "        # Traverse URLs.\n",
      "        for u in set(expandedUrls):\n",
      "            # Find any for which we have end destinations.\n",
      "            if u in urls:\n",
      "                # If we do, find end URL information and append to urls field.\n",
      "                postEndUrls.append(urls[u]['endUrl']['netloc']+urls[u]['endUrl']['path'])\n",
      "        # Make a string.\n",
      "        thisPost['urls'] = json.dumps(postEndUrls)\n",
      "\n",
      "        # Whew. Finally. Now append to the list.\n",
      "        postInfo.append(thisPost)\n",
      "\n",
      "# Now perform a second pass to calculate the final clicks metric.\n",
      "# Apply the following rules:\n",
      "#     - Retweeted URLs should be nullified.\n",
      "#     - Links posted multiple times are assumed to only be included in the observed tweets/posts.\n",
      "#      Distribute their clicks among all observations.\n",
      "#    - Nullify any links posted both to Facbook and Twitter.\n",
      "#       Disaggregation is impossible and they make comparisons dangerous.\n",
      "# Note that there is always a possibility that a link was posted outside of the observed content. We have no way of knowing.\n",
      "\n",
      "# Traverse tweets.\n",
      "for i, tI in enumerate(tweetInfo):\n",
      "    # There will be lots of nullifications, so treat that as our base case.\n",
      "    tweetInfo[i]['clicks'] = None\n",
      "    # Handle retweets.\n",
      "    if not tI['retweeted']:\n",
      "        # If not one, traverse all links.\n",
      "        for u, c in tI['raw_clicks'].iteritems():\n",
      "            # Do nothing for URLs that have been retweeted, have no click info, or are shared across networks.\n",
      "            # Otherwise, divide clicks by number of times link posted.\n",
      "            if c and u not in retweetedUrls and len(urlNetworks[u]) == 1:\n",
      "                tweetInfo[i]['clicks'] = (tweetInfo[i]['clicks'] or 0) + (float(c) / urlCounts[u]) # Divisor will always be >= 1.\n",
      "    # Clean up unneeded raw click info.\n",
      "    del tweetInfo[i]['raw_clicks']\n",
      "\n",
      "# Traverse posts.\n",
      "# There is no retweeting, so we don't need to worry about that. Still, other rules will be the same.\n",
      "for i, pI in enumerate(postInfo):\n",
      "    # There will be lots of nullifications, so treat that as our base case.\n",
      "    postInfo[i]['clicks'] = None\n",
      "    # Otherwise, traverse all links.\n",
      "    for u, c in pI['raw_clicks'].iteritems():\n",
      "        # Do nothing for URLs that have been retweeted, have no click info, or are shared across networks.\n",
      "        # Otherwise, divide clicks by number of times link posted.\n",
      "        if c and u not in retweetedUrls and len(urlNetworks[u]) == 1:\n",
      "            postInfo[i]['clicks'] = (postInfo[i]['clicks'] or 0) + (float(c) / urlCounts[u]) # Divisor will always be >= 1.\n",
      "    # Clean up unneeded raw click info.\n",
      "    del postInfo[i]['raw_clicks']\n",
      "\n",
      "# Whew, now the easy part: turn both arrays into dataframes.\n",
      "tweetDf = pd.DataFrame(tweetInfo)\n",
      "postDf = pd.DataFrame(postInfo)\n",
      "\n",
      "# Write out.\n",
      "tweetDf.to_csv(filePath_output % 'twitter', encoding='utf-8', index=False)\n",
      "postDf.to_csv(filePath_output % 'facebook', encoding='utf-8', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When performing analysis, we calculated several other features on an *ad hoc* basis, particularly for time series analysis and log/log-normal outcomes. Further, because scikit-learn models cannot accommodate non-binary categorical variables, we augmented the dataset with dummy variables for each factor level. Throughout, we attempted to maintain lists of groups of useful columns (particularly those that are specific to Twitter or Facebook), allowing us to avoid unnecessarily long lists of variables later on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Track special columns.\n",
      "outcomeCols_lin = ['clicks','retweets','favorites','likes','shares','comments']\n",
      "twSpecificCols = set(tw.columns).difference(fb.columns)\n",
      "fbSpecificCols = set(fb.columns).difference(tw.columns)\n",
      "binaryCols = ['weekend_utc','weekend_est','retweeted']\n",
      "categoricalCols = binaryCols+['org','org_category','medium','agg_sentiment_class','sentiment_class','service','day_of_week_utc','day_of_week_est']\n",
      "\n",
      "# Calculate oscillations of time for regression.\n",
      "tw['time_est_sin'] = np.sin(tw['time_est'])\n",
      "tw['time_est_cos'] = np.cos(tw['time_est'])\n",
      "tw['time_est_2'] = np.power(tw['time_est'],2)\n",
      "tw['time_est_3'] = np.power(tw['time_est'],3)\n",
      "fb['time_est_sin'] = np.sin(fb['time_est'])\n",
      "fb['time_est_cos'] = np.cos(fb['time_est'])\n",
      "fb['time_est_2'] = np.power(fb['time_est'],2)\n",
      "fb['time_est_3'] = np.power(fb['time_est'],3)\n",
      "tw['minute_est_sin'] = np.sin(tw['minute_est'])\n",
      "tw['minute_est_cos'] = np.cos(tw['minute_est'])\n",
      "tw['minute_est_2'] = np.power(tw['minute_est'],2)\n",
      "tw['minute_est_3'] = np.power(tw['minute_est'],3)\n",
      "fb['minute_est_sin'] = np.sin(fb['minute_est'])\n",
      "fb['minute_est_cos'] = np.cos(fb['minute_est'])\n",
      "fb['minute_est_2'] = np.power(fb['minute_est'],2)\n",
      "fb['minute_est_3'] = np.power(fb['minute_est'],3)\n",
      "\n",
      "# Calculate normalized columns. Unclear that there is any use for these.\n",
      "for o in twSpecificCols.intersection(outcomeCols_lin):\n",
      "    tw[o+'_normalized'] = tw[o].astype('float') / tw['followers_count'].astype('float')\n",
      "tw['clicks_normalized'] = tw['clicks'].astype('float') / tw['followers_count'].astype('float')\n",
      "for o in fbSpecificCols.intersection(outcomeCols_lin):\n",
      "    fb[o+'_normalized'] = fb[o].astype('float') / fb['account_likes'].astype('float')\n",
      "fb['clicks_normalized'] = fb['clicks'].astype('float') / fb['account_likes'].astype('float')\n",
      "\n",
      "# Keep track of new columns.\n",
      "outcomeCols_norm = [c+'_normalized' for c in outcomeCols_lin]\n",
      "\n",
      "# Log columns as desired.\n",
      "tw['clicks_log'] = np.log(tw['clicks'])\n",
      "fb['clicks_log'] = np.log(fb['clicks'])\n",
      "tw['clicks_normalized_log'] = np.log(tw['clicks_normalized'])\n",
      "fb['clicks_normalized_log'] = np.log(fb['clicks_normalized'])\n",
      "tw['retweets_log'] = np.log(tw['retweets'])\n",
      "fb['likes_log'] = np.log(fb['likes'])\n",
      "\n",
      "# Keep track of new columns.\n",
      "outcomeCols_norm_log = ['clicks_log','clicks_normalized_log']\n",
      "\n",
      "# Combine. Not all columns overlap.\n",
      "twfb = tw.append(fb)\n",
      "\n",
      "# Generate categorical columns.\n",
      "for c in categoricalCols:\n",
      "    dummyVals = pd.get_dummies(twfb[c])\n",
      "    dummyVals.columns = [c+'_'+str(val) for val in dummyVals.columns]\n",
      "    twfb.merge(dummyVals.iloc[:,1:], left_index=True, right_index=True)\n",
      "    if c not in fbSpecificCols:\n",
      "        dummyVals_tw = pd.get_dummies(tw[c])\n",
      "        dummyVals_tw.columns = [c+'_'+str(val) for val in dummyVals_tw.columns]\n",
      "        tw = tw.merge(dummyVals_tw.iloc[:,1:], left_index=True, right_index=True)\n",
      "    if c not in twSpecificCols:\n",
      "        dummyVals_fb = pd.get_dummies(fb[c])\n",
      "        dummyVals_fb.columns = [c+'_'+str(val) for val in dummyVals_fb.columns]\n",
      "        fb = fb.merge(dummyVals_fb.iloc[:,1:], left_index=True, right_index=True, copy=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Predicting Clicks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Naturally, predicting clicks is an important measure of the social media response to news content. To accomplish this, we attempted to build Ridge Regression and Random Forest Regression models based on the outcomes and features calculated in the last step."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Linear Regression Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following code generates linear regression models for click prediction."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##### Set up matrices/vectors for training, cross-validation, and testing.\n",
      "\n",
      "# Set up our Twitter outcomes/features.\n",
      "outcome = 'clicks_log'\n",
      "features_lm_tw = (\n",
      "    ['time_est','time_est_sin','time_est_cos','time_est_2','time_est_3','minute_est','minute_est_sin','minute_est_cos','minute_est_2','minute_est_3',\n",
      "     'followers_count','hashtags','sentiment_score_negative','sentiment_score_positive','word_count'] +\n",
      "    [f for f in tw.columns if f.startswith((\n",
      "        'day_of_week_est_', 'org_', 'medium_', 'sentiment_class_'\n",
      "    )) and not f.startswith('org_category')]\n",
      ")\n",
      "X_lm_tw = tw[[outcome]+features_lm_tw]\n",
      "X_lm_tw = X_lm_tw.dropna()\n",
      "y_lm_tw = X_lm_tw[outcome]\n",
      "del X_lm_tw[outcome]\n",
      "# Finally, randomly select a testing set for final model evaluation.\n",
      "X_lm_tw, Xtest_lm_tw, y_lm_tw, ytest_lm_tw = cross_validation.train_test_split(X_lm_tw, y_lm_tw, test_size=0.1)\n",
      "# Set up our Facebook outcomes/features.\n",
      "outcome = 'clicks_log'\n",
      "features_lm_fb = (\n",
      "    ['time_est','time_est_sin','time_est_cos','time_est_2','time_est_3','minute_est','minute_est_sin','minute_est_cos','minute_est_2','minute_est_3',\n",
      "     'account_likes','agg_sentiment_score_negative','agg_sentiment_score_positive','sentiment_score_negative','sentiment_score_positive','word_count'] +\n",
      "    [f for f in fb.columns if f.startswith((\n",
      "        'day_of_week_est_', 'org_', 'medium_','agg_sentiment_class_', 'sentiment_class_'\n",
      "    )) and not f.startswith('org_category')]\n",
      ")\n",
      "X_lm_fb = fb[[outcome]+features_lm_fb]\n",
      "X_lm_fb = X_lm_fb.dropna()\n",
      "y_lm_fb = X_lm_fb[outcome]\n",
      "del X_lm_fb[outcome]\n",
      "# Finally, randomly select a testing set for final model evaluation.\n",
      "X_lm_fb, Xtest_lm_fb, y_lm_fb, ytest_lm_fb = cross_validation.train_test_split(X_lm_fb, y_lm_fb, test_size=0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##### Generate linear regression models.\n",
      "\n",
      "# Parameters for linear regression tuning. These have been iterated-over several times.\n",
      "alphas = np.arange(0.001,0.030,0.001)\n",
      "# Start models.\n",
      "kFold = cross_validation.KFold(n=len(X_lm_tw), n_folds=10)\n",
      "lm_clicks_tw = linear_model.RidgeCV(alphas=alphas,scoring='mean_absolute_error', cv=kFold)\n",
      "lm_clicks_tw.fit(X_lm_tw,y_lm_tw); pass # Suppress output.\n",
      "# Parameters for linear regression tuning. These have been iterated-over several times.\n",
      "alphas = np.arange(0.008,0.022,0.001)\n",
      "# Start models.\n",
      "kFold = cross_validation.KFold(n=len(X_lm_fb), n_folds=10)\n",
      "lm_clicks_fb = linear_model.RidgeCV(alphas=alphas,scoring='mean_absolute_error', cv=kFold)\n",
      "lm_clicks_fb.fit(X_lm_fb,y_lm_fb); pass # Suppress output."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also calculate several performance metrics below. They are compared in the \"Modeling Results\" section."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Twitter performance calculations.\n",
      "yhat_lm_tw = lm_clicks_tw.predict(Xtest_lm_tw)\n",
      "# Alpha.\n",
      "alpha_lm_tw = lm_clicks_tw.alpha_\n",
      "# Calculate R^2.\n",
      "r2_lm_tw = sklearn.metrics.r2_score(ytest_lm_tw, yhat_lm_tw)\n",
      "# Calculate mean absolute error on logged values.\n",
      "mae_lm_tw = sklearn.metrics.mean_absolute_error(ytest_lm_tw, yhat_lm_tw)\n",
      "# Use antilogs to get a more useful estimate.\n",
      "ytest_exp_lm_tw = np.exp(ytest_lm_tw)\n",
      "yhat_exp_lm_tw = np.exp(yhat_lm_tw)\n",
      "mae_exp_lm_tw = sklearn.metrics.mean_absolute_error(ytest_exp_lm_tw, yhat_exp_lm_tw)\n",
      "# Print information.\n",
      "print 'Twitter:'\n",
      "print 'Optimal alpha regularization setting:'.rjust(41), alpha_lm_tw\n",
      "print 'R^2:'.rjust(41), r2_lm_tw\n",
      "print 'Mean absolute error (log scale):'.rjust(41), mae_lm_tw\n",
      "print 'Mean absolute error (antilog scale):'.rjust(41), mae_exp_lm_tw\n",
      "print 'vs. average:'.rjust(41), yhat_exp_lm_tw.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Twitter:\n",
        "    Optimal alpha regularization setting: 0.029\n",
        "                                     R^2: 0.528173294186\n",
        "         Mean absolute error (log scale): 0.707327805419\n",
        "     Mean absolute error (antilog scale): 584.329400732\n",
        "                             vs. average: 574.710007082\n"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Facebook performance calculations.\n",
      "yhat_lm_fb = lm_clicks_fb.predict(Xtest_lm_fb)\n",
      "# Alpha.\n",
      "alpha_lm_fb = lm_clicks_fb.alpha_\n",
      "# Calculate R^2.\n",
      "r2_lm_fb = sklearn.metrics.r2_score(ytest_lm_fb, yhat_lm_fb)\n",
      "# Calculate mean absolute error on logged values.\n",
      "mae_lm_fb = sklearn.metrics.mean_absolute_error(ytest_lm_fb, yhat_lm_fb)\n",
      "# Use antilogs to get a more useful estimate.\n",
      "ytest_exp_lm_fb = np.exp(ytest_lm_fb)\n",
      "yhat_exp_lm_fb = np.exp(yhat_lm_fb)\n",
      "mae_exp_lm_fb = sklearn.metrics.mean_absolute_error(ytest_exp_lm_fb, yhat_exp_lm_fb)\n",
      "# Print information.\n",
      "print 'Facebook:'\n",
      "print 'Optimal alpha regularization setting:'.rjust(41), alpha_lm_fb\n",
      "print 'R^2:'.rjust(41), r2_lm_fb\n",
      "print 'Mean absolute error (log scale):'.rjust(41), mae_lm_fb\n",
      "print 'Mean absolute error (antilog scale):'.rjust(41), mae_exp_lm_fb\n",
      "print 'vs. average:'.rjust(41), ytest_exp_lm_fb.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Facebook:\n",
        "    Optimal alpha regularization setting: 0.014\n",
        "                                     R^2: 0.247630274134\n",
        "         Mean absolute error (log scale): 1.20235731875\n",
        "     Mean absolute error (antilog scale): 7416.10322545\n",
        "                             vs. average: 8613.11651519\n"
       ]
      }
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Random Forest Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Random Forest model was generated using a similar process to that above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##### Set up matrices/vectors for training, cross-validation, and testing."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##### Generate RF models."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u2026and as before, we calculated performance measurements for both models for comparison in the \"Modeling Results\" section."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Modeling Results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Model performance **against testing data** is summarized below:\n",
      "\n",
      "<table><caption>$R^2$ (Smaller is Better)</caption>\n",
      "<thead><th>Model</th><th>Social Network</th><th style=\"text-align: right\">$R^2$</th></thead>\n",
      "<tbody>\n",
      "    <tr><td>Ridge Regression</td><td>Twitter</td><td style=\"text-align: right\">TODO</td></tr>\n",
      "    <tr><td>Ridge Regression</td><td>Facebook</td><td style=\"text-align: right\">TODO</td></tr>\n",
      "    <tr><td>Random Forest</td><td>Twitter</td><td style=\"text-align: right\">TODO</td></tr>\n",
      "    <tr><td>Random Forest</td><td>Facebook</td><td style=\"text-align: right\">TODO</td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "\n",
      "<table><caption>Mean Absolute Error (Smaller is Better)</caption>\n",
      "<thead><th>Model</th><th>Social Network</th><th>MAE (Log Scale)</th><th>MAE (Antilog Scale)</th></thead>\n",
      "<tbody>\n",
      "    <tr><td>Ridge Regression</td><td>Twitter</td><td style=\"text-align: right\">TODO</td><td style=\"text-align: right\">TODO</td></tr>\n",
      "    <tr><td>Ridge Regression</td><td>Facebook</td><td style=\"text-align: right\">TODO</td><td style=\"text-align: right\">TODO</td></tr>\n",
      "    <tr><td>Random Forest</td><td>Twitter</td><td style=\"text-align: right\">TODO</td><td style=\"text-align: right\">TODO</td></tr>\n",
      "    <tr><td>Random Forest</td><td>Facebook</td><td style=\"text-align: right\">TODO</td><td style=\"text-align: right\">TODO</td></tr>\n",
      "</tbody>\n",
      "</table>\n",
      "\n",
      "Interestingly, the Ridge Regression model outperformed the Random Forest model for both social networks. This was unexpected given the linear assumptions of the regression model and the accurate reputation of Random Forests.\n",
      "\n",
      "Also of note: the Facebook model drastically underperformed the Twitter model. Features were selected to be as analogous as possible between models, so there is no intuitive reason why performance ought to differ so radically. **Our best theory is that Twitter's reverse-chronological timeline provides more consistent returns, while Facebook's algorithmic system (that selectively displays content, based only partially on its age) introduces variability that we failed to capture.**\n",
      "\n",
      "In terms of overall performance, $R^2$ values may be considered respectable (especially for Twitter prediction), but mean errors indicate that we are still missing the mark. Twitter MAE statistics indicate (on the log scale) that our typical estimate misses the actual value by 70%. This relationship may be visualized with the following charts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### TODO CHARTS"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Logistic Regression Coefficients"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The regression coefficients are listed below. Organizations coefficients are relative to ABC News, but defy easy interpretation \n",
      "\n",
      "<table style=\"float: left;\"><caption>Twitter</caption>\n",
      "</table>\n",
      "\n",
      "<table style=\"margin-left:10%;\"><caption>Facebook</caption>\n",
      "</table>\n",
      "\n",
      "Several notable values emerge from both models."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Predicting Retweets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Beyond cultivating link traffic in the short term, news organizations may wish to enhance their social media presences in order to attract long term engagement in the form of followers and likers. That audience, by virtue of \"subscribing\" to an organization's content, is more likely to view its links in the future and pass on interesting news to others. We cannot easily observe such activity, but we theorize that predicting retweets and \"likes\" of content\u2014as they represent visible, in-network activity in affirmation of a company's \"product\"\u2014may be useful for measuring companies' efforts to engage users of each network."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Inspired by the performance of the Ridge Regression model above, we attempted to fit the same model as above, with post retweets (rather than clicks) as the outcome."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Predicting Likes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Analogous (though not perfectly so) to retweets, Facebook \"likes\" represent a method of publicly affirming a post. We followed the same template as we did for retweets, altering our previously-generated regression to predict \"likes.\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}