{
 "metadata": {
  "name": "",
  "signature": "sha256:5af5d494f0b5b926bd42df04331ff4faabedf98e34e80f29c116877c40e64458"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<a id=\"s8\"></a>Predictive Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The richness of the data that we collected ought to be ideal for predictive analysis problems surrouning social media outcomes. We hoped that building such models would enable us to predict social media success (in terms of link clicks, and network-specific engagement like retweets and likes) and explain it in terms of strategic adjustments that news organizations might enact. This section describes the modeling process itself; [the next section](#9) delves into the strategic implications of our analysis."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Data for Prediction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We encountered difficulty repurposing the \"merged\" dataset from past analysis for use in prediction, as it did not retain all of the features that we expected to be useful for predictive purposes. The following code aims to summarize all Twitter, Facebook, URL, Bitly, and sentiment data for prediction. When calculating click outcomes, it makes several adjustments for multi-posted links, aiming to properly attribute link traffic to posts. It also performs additional cleaning, such as removing clicks to retweeted articles entirely."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##### Main Setup\n",
      "\n",
      "# Data directories.\n",
      "dirPath_consolidatedTw = '../../../data/twitter/consolidated/'\n",
      "dirPath_consolidatedFb = '../../../data/facebook/consolidated/'\n",
      "dirPath_bitly = '../../../data/bitly/%s/'\n",
      "dirPath_sentiment = '../../../data/sentiment/%s/'\n",
      "dirPath_urls = '../../../data/urls/%s/'\n",
      "filePath_dates = '../../conf/dates.json'\n",
      "filePath_output = '../../../data/unmergedPostSummaries/%s.csv'\n",
      "filePath_users = '../../data/users/%s.json'\n",
      "\n",
      "# Get organizational information.\n",
      "orgs = json.load(open('../../conf/organizations.json'))\n",
      "\n",
      "# Get account information.\n",
      "twAccounts = json.load(open(filePath_users % 'twitter'))\n",
      "fbAccounts = json.load(open(filePath_users % 'facebook'))\n",
      "\n",
      "# Tokens to skip for word count.\n",
      "# This is an inherently difficult task, but this should help us get to where we need to go.\n",
      "skipTokens = {'\\'s','.','?',',','(',')','#','\\\\',':',';','@','-','\\''}\n",
      "\n",
      "# Facebook status type mapping.\n",
      "fbStatusTypeToMedium = {'added_photos': 'photo', 'added_video': 'video'}\n",
      "\n",
      "# Information about proper dates.\n",
      "dates = json.load(open(filePath_dates))\n",
      "twStartDate = dp(dates['twitter']['start'])\n",
      "twEndDate   = dp(dates['twitter']['end'])\n",
      "fbStartDate  = dp(dates['facebook']['start'])\n",
      "fbEndDate    = dp(dates['facebook']['end'])\n",
      "\n",
      "# Get local timezone.\n",
      "localTz = pytz.timezone('America/New_York')\n",
      "\n",
      "# URL counts. Allows us to adjust URL traffic.\n",
      "urlCounts = dict()        # Click counts by URL.\n",
      "urlNetworks = dict()    # Distinct networks by URL.\n",
      "retweetedUrls = set()    # Set of all retweets.\n",
      "\n",
      "# Containers for statistics that will become DFs.\n",
      "tweetInfo = []\n",
      "postInfo  = []\n",
      "\n",
      "##### Helper Functions\n",
      "\n",
      "\"\"\"\n",
      "Remove URLs, clean syntax.\n",
      "\"\"\"\n",
      "def cleanMessage(message):\n",
      "    # Remove Twitter-specific syntax.\n",
      "    message = message.replace('@','')\n",
      "    message = message.replace('#','')\n",
      "    # Remove links.\n",
      "    links = socialUrlUtils.urlsInText(message)\n",
      "    # links.reverse()\n",
      "    for link in links:\n",
      "        message = message.replace(link,'')\n",
      "    # Return.\n",
      "    return message.strip()\n",
      "\n",
      "##### Main Execution\n",
      "\n",
      "# Traverse all organizations.\n",
      "for org, orgData in orgs.iteritems():\n",
      "    # Org setup.\n",
      "    orgName = orgData['name']\n",
      "\n",
      "    # Status.\n",
      "    print 'Consolidating data for %s.' % orgName\n",
      "\n",
      "    # Get all relevant Twitter information.\n",
      "    service   = 'twitter'\n",
      "    tweets    = json.load(open(dirPath_consolidatedTw + [f for f in os.listdir(dirPath_consolidatedTw) if f.startswith(org)][-1]))\n",
      "    bitly     = json.load(open((dirPath_bitly % service) + org + '.json'))\n",
      "    sentiment = json.load(open((dirPath_sentiment % service) + org + '.json'))\n",
      "    urls      = json.load(open((dirPath_urls % service) + org + '.json'))\n",
      "\n",
      "    # Traverse tweets.\n",
      "    for tweet in tweets:\n",
      "        # Get some initial information.\n",
      "        expandedUrls = [u['expanded_url'] for u in tweet['entities']['urls']]\n",
      "        # Ensure date in proper range. We want to exclude everything outside of it.\n",
      "        created_utc = dp(tweet['created_at'])\n",
      "        if created_utc < twStartDate or created_utc >= twEndDate:\n",
      "            continue\n",
      "\n",
      "        # Container to store stats that will eventually become a data frame.\n",
      "        thisTweet = { 'service': service, 'id': tweet['id']}\n",
      "\n",
      "        # Tweet features.\n",
      "        thisTweet['text'] = tweet['text']\n",
      "        thisTweet['hashtags'] = len(tweet['entities']['hashtags'])\n",
      "        thisTweet['retweeted'] = ('retweeted_status' in tweet or tweet['text'].startswith('RT '))\n",
      "        media = tweet['entities'].get('media')\n",
      "        thisTweet['medium'] = media[0]['type'] if media else 'text'\n",
      "        # Tweet datetime features.\n",
      "        thisTweet['date_utc'] = str(created_utc.date())\n",
      "        thisTweet['day_of_week_utc'] = created_utc.weekday()\n",
      "        thisTweet['weekend_utc'] = thisTweet['day_of_week_utc'] >= 5\n",
      "        thisTweet['time_utc'] = float(created_utc.hour) + float(created_utc.minute)/60\n",
      "        thisTweet['minute_utc'] = float(created_utc.minute)\n",
      "        thisTweet['day_time_utc'] = thisTweet['day_of_week_utc'] + float(created_utc.hour) / 24.0\n",
      "        created_est = created_utc.replace(tzinfo=pytz.utc).astimezone(localTz)\n",
      "        thisTweet['date_est'] = str(created_est.date())\n",
      "        thisTweet['day_of_week_est'] = created_est.weekday()\n",
      "        thisTweet['weekend_est'] = thisTweet['day_of_week_est'] >= 5\n",
      "        thisTweet['time_est'] = float(created_est.hour) + float(created_est.minute)/60\n",
      "        thisTweet['minute_est'] = float(created_est.minute)\n",
      "        thisTweet['day_time_est'] = thisTweet['day_of_week_est'] + float(created_est.hour) / 24.0\n",
      "        # Org features.\n",
      "        thisTweet['org'] = org\n",
      "        thisTweet['org_category'] = orgData['category'].lower()\n",
      "        thisTweet['social_flow_user'] = orgData['socialFlow']\n",
      "        thisTweet['followers_count'] = twAccounts[org]['followers_count']\n",
      "        # Sentiment features.\n",
      "        thisTweet['word_count'] = len([t for t in tokenize(cleanMessage(tweet['text'])) if t not in skipTokens])\n",
      "        sent = sentiment.get(str(tweet['id']))\n",
      "        if sent:\n",
      "            thisTweet['sentiment_class'] = sent['class']\n",
      "            thisTweet['sentiment_score_positive'] = sent['meanScorePosSig']\n",
      "            thisTweet['sentiment_score_negative'] = sent['meanScoreNegSig']\n",
      "        else:\n",
      "            thisTweet['sentiment_class'] = None\n",
      "            thisTweet['sentiment_score_positive'] = None\n",
      "            thisTweet['sentiment_score_negative'] = None\n",
      "\n",
      "        # Tweet outcomes.\n",
      "        thisTweet['favorites'] = tweet['favorite_count']\n",
      "        thisTweet['retweets']  = tweet['retweet_count']\n",
      "\n",
      "        # Bitly features. This is going to be tougher.\n",
      "        # Traverse all URLs and store those that we've traversed.\n",
      "        # We'll make a second pass later and clean up.\n",
      "        thisTweet['raw_clicks'] = dict()\n",
      "        for u in set(expandedUrls):\n",
      "            # Increment count in urlCounts.\n",
      "            urlCounts[u] = (urlCounts.get(u) or 0) + 1\n",
      "            # Identify retweeted URLs.\n",
      "            if (thisTweet['retweeted']):\n",
      "                retweetedUrls.add(u)\n",
      "            # Identify URL as posted on Twitter.\n",
      "            if (u not in urlNetworks): urlNetworks[u] = set()\n",
      "            urlNetworks[u].add(service)\n",
      "            # Update raw clicks.\n",
      "            thisTweet['raw_clicks'][u] = bitly[u]['data']['link_clicks'] if u in bitly and bitly[u]['data'] else None\n",
      "\n",
      "        # URL records. These are not really features, just JSON arrays.\n",
      "        tweetEndUrls = []\n",
      "        # Traverse URLs.\n",
      "        for u in set(expandedUrls):\n",
      "            # Find any for which we have end destinations.\n",
      "            if u in urls:\n",
      "                # If we do, find end URL information and append to urls field.\n",
      "                tweetEndUrls.append(urls[u]['endUrl']['netloc']+urls[u]['endUrl']['path'])\n",
      "        # Make a string.\n",
      "        thisTweet['urls'] = json.dumps(tweetEndUrls)\n",
      "\n",
      "        # Whew. Finally. Now append to the list.\n",
      "        tweetInfo.append(thisTweet)\n",
      "\n",
      "    # Get all relevant Facebook information.\n",
      "    service   = 'facebook'\n",
      "    posts     = json.load(open(dirPath_consolidatedFb + [f for f in os.listdir(dirPath_consolidatedFb) if f.startswith(org)][-1]))\n",
      "    bitly     = json.load(open((dirPath_bitly % service) + org + '.json'))\n",
      "    sentiment = json.load(open((dirPath_sentiment % service) + org + '.json'))\n",
      "    urls      = json.load(open((dirPath_urls % service) + org + '.json'))\n",
      "\n",
      "    # Traverse posts.\n",
      "    for post in posts:\n",
      "        # Get some initial information.\n",
      "        expandedUrls = []\n",
      "        if 'link' in post:\n",
      "            expandedUrls.append(post['link'])\n",
      "        if 'message' in post:\n",
      "            expandedUrls += socialUrlUtils.urlsInText(post['message'])\n",
      "        created_utc = dp(post['created_time'])\n",
      "        # Ensure date in proper range. We want to exclude everything outside of it.\n",
      "        if created_utc < fbStartDate or created_utc >= fbEndDate:\n",
      "            continue\n",
      "\n",
      "        # Container to store stats that will eventually become a data frame.\n",
      "        thisPost = { 'service': service, 'id': post['id']}\n",
      "\n",
      "        # Post features.\n",
      "        thisPost['text'] = post.get('message')\n",
      "        thisPost['medium'] = fbStatusTypeToMedium.get(post.get('status_type')) or 'other'\n",
      "        # Post datetime features.\n",
      "        thisPost['date_utc'] = str(created_utc.date())\n",
      "        thisPost['day_of_week_utc'] = created_utc.weekday()\n",
      "        thisPost['weekend_utc'] = thisPost['day_of_week_utc'] >= 5\n",
      "        thisPost['time_utc'] = float(created_utc.hour) + float(created_utc.minute)/60\n",
      "        thisPost['minute_utc'] = float(created_utc.minute)\n",
      "        thisPost['day_time_utc'] = thisPost['day_of_week_utc'] + float(created_utc.hour) / 24.0\n",
      "        created_est = created_utc.replace(tzinfo=pytz.utc).astimezone(localTz)\n",
      "        thisPost['date_est'] = str(created_est.date())\n",
      "        thisPost['day_of_week_est'] = created_est.weekday()\n",
      "        thisPost['weekend_est'] = thisPost['day_of_week_est'] >= 5\n",
      "        thisPost['time_est'] = float(created_est.hour) + float(created_est.minute)/60\n",
      "        thisPost['minute_est'] = float(created_est.minute)\n",
      "        thisPost['day_time_est'] = thisPost['day_of_week_est'] + float(created_est.hour) / 24.0\n",
      "        # Org features.\n",
      "        thisPost['org'] = org\n",
      "        thisPost['org_category'] = orgData['category'].lower()\n",
      "        thisPost['social_flow_user'] = orgData['socialFlow']\n",
      "        thisPost['account_likes'] = fbAccounts[org]['likes']\n",
      "        # Sentiment features.\n",
      "        thisPost['word_count'] = len([t for t in tokenize(cleanMessage(post['message'])) if t not in skipTokens]) if 'message' in post else 0\n",
      "        sent = sentiment.get(str(post['id']))\n",
      "        if sent:\n",
      "            thisPost['sentiment_class'] = sent['message']['class']\n",
      "            thisPost['sentiment_score_positive'] = sent['message']['meanScorePosSig']\n",
      "            thisPost['sentiment_score_negative'] = sent['message']['meanScoreNegSig']\n",
      "            thisPost['agg_sentiment_class'] = sent['agg']['class']\n",
      "            thisPost['agg_sentiment_score_positive'] = sent['agg']['meanScorePosSig']\n",
      "            thisPost['agg_sentiment_score_negative'] = sent['agg']['meanScoreNegSig']\n",
      "        else:\n",
      "            thisPost['message_sentiment_class'] = None\n",
      "            thisPost['message_sentiment_score_positive'] = None\n",
      "            thisPost['message_sentiment_score_negative'] = None\n",
      "            thisPost['agg_sentiment_class'] = None\n",
      "            thisPost['agg_sentiment_score_positive'] = None\n",
      "            thisPost['agg_sentiment_score_negative'] = None\n",
      "\n",
      "        # Post outcomes.\n",
      "        thisPost['comments'] = post['comments']['summary']['total_count'] if 'comments' in post else 0\n",
      "        thisPost['likes']    = post['likes']['summary']['total_count'] if 'likes' in post else 0\n",
      "        thisPost['shares']   = post['shares']['count'] if 'shares' in post else 0\n",
      "\n",
      "        # Bitly features. This is going to be tougher.\n",
      "        # Traverse all URLs and store those that we've traversed.\n",
      "        # We'll make a second pass later and clean up.\n",
      "        thisPost['raw_clicks'] = dict()\n",
      "        for u in set(expandedUrls):\n",
      "            # Increment count in urlCounts.\n",
      "            urlCounts[u] = (urlCounts.get(u) or 0) + 1\n",
      "            # Identify URL as posted on Twitter.\n",
      "            if (u not in urlNetworks): urlNetworks[u] = set()\n",
      "            urlNetworks[u].add(service)\n",
      "            # Update raw clicks.\n",
      "            thisPost['raw_clicks'][u] = bitly[u]['data']['link_clicks'] if u in bitly and bitly[u]['data'] else None\n",
      "\n",
      "        # URL records. These are not really features, just JSON arrays.\n",
      "        postEndUrls = []\n",
      "        # Traverse URLs.\n",
      "        for u in set(expandedUrls):\n",
      "            # Find any for which we have end destinations.\n",
      "            if u in urls:\n",
      "                # If we do, find end URL information and append to urls field.\n",
      "                postEndUrls.append(urls[u]['endUrl']['netloc']+urls[u]['endUrl']['path'])\n",
      "        # Make a string.\n",
      "        thisPost['urls'] = json.dumps(postEndUrls)\n",
      "\n",
      "        # Whew. Finally. Now append to the list.\n",
      "        postInfo.append(thisPost)\n",
      "\n",
      "# Now perform a second pass to calculate the final clicks metric.\n",
      "# Apply the following rules:\n",
      "#  - Retweeted URLs should be nullified.\n",
      "#  - Links posted multiple times are assumed to only be included in the observed tweets/posts.\n",
      "#    Distribute their clicks among all observations.\n",
      "#  - Nullify any links posted both to Facbook and Twitter.\n",
      "#    Disaggregation is impossible and they make comparisons dangerous.\n",
      "# Note that there is always a possibility that a link was posted outside of the observed content. We have no way of knowing.\n",
      "\n",
      "# Traverse tweets.\n",
      "for i, tI in enumerate(tweetInfo):\n",
      "    # There will be lots of nullifications, so treat that as our base case.\n",
      "    tweetInfo[i]['clicks'] = None\n",
      "    # Handle retweets.\n",
      "    if not tI['retweeted']:\n",
      "        # If not one, traverse all links.\n",
      "        for u, c in tI['raw_clicks'].iteritems():\n",
      "            # Do nothing for URLs that have been retweeted, have no click info, or are shared across networks.\n",
      "            # Otherwise, divide clicks by number of times link posted.\n",
      "            if c and u not in retweetedUrls and len(urlNetworks[u]) == 1:\n",
      "                tweetInfo[i]['clicks'] = (tweetInfo[i]['clicks'] or 0) + (float(c) / urlCounts[u]) # Divisor will always be >= 1.\n",
      "    # Clean up unneeded raw click info.\n",
      "    del tweetInfo[i]['raw_clicks']\n",
      "\n",
      "# Traverse posts.\n",
      "# There is no retweeting, so we don't need to worry about that. Still, other rules will be the same.\n",
      "for i, pI in enumerate(postInfo):\n",
      "    # There will be lots of nullifications, so treat that as our base case.\n",
      "    postInfo[i]['clicks'] = None\n",
      "    # Otherwise, traverse all links.\n",
      "    for u, c in pI['raw_clicks'].iteritems():\n",
      "        # Do nothing for URLs that have been retweeted, have no click info, or are shared across networks.\n",
      "        # Otherwise, divide clicks by number of times link posted.\n",
      "        if c and u not in retweetedUrls and len(urlNetworks[u]) == 1:\n",
      "            postInfo[i]['clicks'] = (postInfo[i]['clicks'] or 0) + (float(c) / urlCounts[u]) # Divisor will always be >= 1.\n",
      "    # Clean up unneeded raw click info.\n",
      "    del postInfo[i]['raw_clicks']\n",
      "\n",
      "# Whew, now the easy part: turn both arrays into dataframes.\n",
      "tweetDf = pd.DataFrame(tweetInfo)\n",
      "postDf = pd.DataFrame(postInfo)\n",
      "\n",
      "# Write out.\n",
      "tweetDf.to_csv(filePath_output % 'twitter', encoding='utf-8', index=False)\n",
      "postDf.to_csv(filePath_output % 'facebook', encoding='utf-8', index=False)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Predicting Clicks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Naturally, predicting clicks is an important measure of the social media response to news content. To accomplish this, we attempted to build Ridge Regression and Random Forest Regression models based on the outcomes and features calculated in the last step."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Predicting Retweets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Beyond cultivating link traffic in the short term, news organizations may wish to enhance their social media presences in order to attract long term engagement in the form of followers and likers. That audience, by virtue of \"subscribing\" to an organization's content, is more likely to view its links in the future and pass on interesting news to others. We cannot easily observe such activity, but we theorize that predicting retweets and \"likes\" of content\u2014as they represent visible, in-network activity in affirmation of a company's \"product\"\u2014may be useful for measuring companies' efforts to engage users of each network."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Inspired by the performance of the Ridge Regression model above, we attempted to fit the same model as above, with post retweets (rather than clicks) as the outcome."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Predicting Likes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Analogous (though not perfectly so) to retweets, Facebook \"likes\" represent a method of publicly affirming a post. We followed the same template as we did for retweets, altering our previously-generated regression to predict \"likes.\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}