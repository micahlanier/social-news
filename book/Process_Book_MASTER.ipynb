{
 "metadata": {
  "name": "",
  "signature": "sha256:d2c8ff4d248e83f444883e2ed7463a41e6d66238d7ddf5e75e4f31aaa3faefaf"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Data Wrangling#"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###The following code extracts URL data for twitter and facebook posts. Then, it finds the same articles that are posted both on twitter and facebook###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Libraries.\n",
      "import json\n",
      "import os\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from urlparse import urlparse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Imports.\n",
      "import re\n",
      "\n",
      "# Define pattern.\n",
      "urlPattern = '(?i)\\\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\\\s()<>{}\\\\[\\\\]]+|\\\\([^\\\\s()]*?\\\\([^\\\\s()]+\\\\)[^\\\\s()]*?\\\\)|\\\\([^\\\\s]+?\\\\))+(?:\\\\([^\\\\s()]*?\\\\([^\\\\s()]+\\\\)[^\\\\s()]*?\\\\)|\\\\([^\\\\s]+?\\\\)|[^\\\\s`!()\\\\[\\\\]{};:\\'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019])|(?:(?<!@)[a-z0-9]+(?:[.\\\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\\\b/?(?!@)))'\n",
      "\n",
      "\"\"\"\n",
      "Detect if a URL is a link to a Facebook photo.\n",
      "\n",
      "Examples:\n",
      "\turlIsFacebookPhoto('https://www.facebook.com/bbcworldnews/photos/pcb.10153822033402588/10153822032512588/?type=1') == True\n",
      "\turlIsFacebookPhoto('http://bbc.in/fgh543q') == False\t\n",
      "\"\"\"\n",
      "def urlIsFacebookPhoto(url):\n",
      "\treturn bool(re.match('^https?://(www\\\\.)?facebook.com/[a-zA-Z0-9]+/photos/.+$', url))\n",
      "\n",
      "\"\"\"\n",
      "Detect if a URL is a link to a Facebook video.\n",
      "\n",
      "Examples:\n",
      "\turlIsFacebookPhoto('https://www.facebook.com/video.php?v=10150471411394999') == True\n",
      "\turlIsFacebookPhoto('http://bbc.in/fgh543q') == False\t\n",
      "\"\"\"\n",
      "def urlIsFacebookVideo(url):\n",
      "\treturn bool(re.match('^https?://(www\\\\.)?facebook.com/video.php\\\\?.+$', url))\n",
      "\n",
      "\"\"\"\n",
      "TODO\n",
      "\"\"\"\n",
      "def urlsInText(t):\n",
      "\t# Perform search.\n",
      "\turlMatches = re.search(urlPattern, t)\n",
      "\tif urlMatches:\n",
      "\t\treturn [g for g in urlMatches.groups()]\n",
      "\treturn []\n",
      "\n",
      "def urlIsFacebookLink(url):\n",
      "    return bool(re.match('^https?://(www\\\\.)?facebook.com.+$', url))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Setup File Paths#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sources = ['twitter','facebook']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"The current working directory is\", os.getcwd()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The current working directory is C:\\Users\\JS\\OneDrive\\Documents\\Harvard\\1. AM 209 (Data Science)\\Final Project\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pathConsolidated = os.getcwd()+'/Social News/data/3. urls/%s/'\n",
      "pathExtractedUrl = os.getcwd()+'/Social News/data/4. extractedUrl/'\n",
      "pathMergedUrl = os.getcwd()+'/Social News/data/5. mergedUrls/'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get all files to traverse. Assume that we can get everything in consolidated folder.\n",
      "postFiles = []\n",
      "for source in sources:\n",
      "    sourcePath = pathConsolidated % source\n",
      "    postFiles += [(source,p[:p.find('-')],sourcePath + p) for p in os.listdir(sourcePath) if p[-5:] == '.json']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Uncomment to see all files read in\n",
      "#postFiles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Extract Organization Names\n",
      "organizationNames = [x[1].split(\".\")[0] for x in postFiles]\n",
      "organizationNames = set(organizationNames)\n",
      "organizationNames = list(organizationNames)\n",
      "organizationNames.sort()\n",
      "len(organizationNames)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "25"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Data Wrangling for URL Data#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "columns = ['NewsOrganization', 'PathUrl','matchUrl', 'FullUrl', 'ShortenedUrl', 'HistoryUrls']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "columns_tw = [\"TW_\" + name for name in columns]\n",
      "columns_fb = [\"FB_\" + name for name in columns]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Function used to extract urls from the json objects into a dataframe\n",
      "param[in] filenames: folder+filenames of the json objects\n",
      "param[in] columnNames: column names for the resulting dataframe\n",
      "param[in] socialMedia: 'twitter' or 'facebook'\n",
      "param[in] destFolder: destination folder to save output csvs\n",
      "\"\"\"\n",
      "def extractUrltoDF(filenames, columnNames, socialMedia, destFolder):\n",
      "    rowCount = 0\n",
      "    facebookLinks = 0 # fb photo + video links\n",
      "    facebookCount = 0 # all fb links\n",
      "    facebookDict = {}\n",
      "    \n",
      "    print columnNames\n",
      "    # Loop through each organization\n",
      "    for organization in filenames:\n",
      "        resultDF = pd.DataFrame(columns=columnNames) \n",
      "        if organization[0] == socialMedia:\n",
      "            print organization[1]\n",
      "            #print organization[2]\n",
      "            data = json.load(open(organization[2]))\n",
      "\n",
      "            #Iterate over dictionary to construct Dataframe\n",
      "            for key, value in data.items():\n",
      "                #print key\n",
      "                if urlIsFacebookLink(key) and not (urlIsFacebookPhoto(key) or urlIsFacebookVideo(key)):\n",
      "                    facebookCount += 1\n",
      "                    facebookDict[key] = organization[1]\n",
      "                if urlIsFacebookPhoto(key) or urlIsFacebookVideo(key):\n",
      "                    facebookLinks += 1\n",
      "                else:\n",
      "                    newOrganization = value['endUrl']['netloc']\n",
      "                    pathUrl = value['endUrl']['path']\n",
      "                    matchUrl = newOrganization+pathUrl\n",
      "                    fullUrl = value['endUrl']['url']\n",
      "                    shortenedUrl = key\n",
      "                    historyUrl = \"\"\n",
      "                    for hUrl in value['history']:\n",
      "                        historyUrl = historyUrl+\"???\"+hUrl\n",
      "                    #Save results\n",
      "                    resultDF.loc[rowCount] = [newOrganization, pathUrl, matchUrl, fullUrl, shortenedUrl, historyUrl]\n",
      "                    rowCount += 1\n",
      "            filename = organization[1].split('.')[0]\n",
      "            resultDF.to_csv(destFolder+filename+'_'+socialMedia+'.csv', encoding='utf-8')\n",
      "            del resultDF\n",
      "    print \"Number of rows: %d\" % rowCount\n",
      "    print \"Number of fb photo + video: %d\" % facebookLinks\n",
      "    print \"Number of fb links: %d\" % facebookCount\n",
      "    return rowCount, facebookDict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fbDF, fbArray = extractUrltoDF([postFiles[28]], columns_fb, 'facebook', pathExtractedUrl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['FB_NewsOrganization', 'FB_PathUrl', 'FB_matchUrl', 'FB_FullUrl', 'FB_ShortenedUrl', 'FB_HistoryUrls']\n",
        "bbc.jso\n",
        "Number of rows: 1620"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of fb photo + video: 1458\n",
        "Number of fb links: 13\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Twitter Data#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#NYTimes example\n",
      "#twitterDF = extractUrltoDF([postFiles[15]], columns_tw, 'twitter', pathExtractedUrl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Run all\n",
      "twitterDF = extractUrltoDF(postFiles, columns_tw, 'twitter', pathExtractedUrl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['TW_NewsOrganization', 'TW_PathUrl', 'TW_matchUrl', 'TW_FullUrl', 'TW_ShortenedUrl', 'TW_HistoryUrls']\n",
        "abcnews.jso\n",
        "afp.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ap.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "bbc.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "bostonglobe.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cbsnews.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cnn.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dailymail.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "foxnews.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "huffingtonpost.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "latimes.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "msnbc.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "nbcnews.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "newsweek.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "npr.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "nytimes.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "reuters.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "slate.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "thedailybeast.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "theguardian.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "time.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "usatoday.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "washingtonpost.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "wsj.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "yahoonews.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of rows: 128673"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of fb photo + video: 6\n",
        "Number of fb links: 21\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Facebook Data#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#NYTimes example\n",
      "#fbDF = extractUrltoDF([postFiles[40]], columns_fb, 'facebook', pathExtractedUrl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Run all\n",
      "fbDF, notCapturedfbLinks = extractUrltoDF(postFiles, columns_fb, 'facebook', pathExtractedUrl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['FB_NewsOrganization', 'FB_PathUrl', 'FB_matchUrl', 'FB_FullUrl', 'FB_ShortenedUrl', 'FB_HistoryUrls']\n",
        "abcnews.jso\n",
        "afp.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ap.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "bbc.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "bostonglobe.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cbsnews.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cnn.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dailymail.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "foxnews.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "huffingtonpost.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "latimes.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "msnbc.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "nbcnews.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "newsweek.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "npr.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "nytimes.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "reuters.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "slate.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "thedailybeast.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "theguardian.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "time.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "usatoday.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "washingtonpost.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "wsj.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "yahoonews.jso"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of rows: 60283"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of fb photo + video: 7410\n",
        "Number of fb links: 1842\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Merge Twitter and Facebook URLs#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Function merges twitter and facebook article URLs\n",
      "param[in] inputFolder: folder+filenames of the extracted url csv files\n",
      "param[in] outputFolder: destination to save the output csv files\n",
      "param[in] orgNames: names of the news organizations used to read and save csv files\n",
      "\"\"\"\n",
      "def mergeUrlFiles(inputFolder, outputFolder, orgNames):\n",
      "    \n",
      "    summaryDF = pd.DataFrame(columns=['newsOrg', 'tw_total', 'tw_unique', 'tw_repeat', \n",
      "                                      'fb_total', 'fb_unique', 'fb_repeat', 'merged_unique']) \n",
      "    rowCount = 0\n",
      "    for orgName in orgNames:\n",
      "        print orgName\n",
      "        # Read in fb and tw url files and drop duplicates\n",
      "        fb_DF = pd.read_csv( inputFolder+orgName+'_facebook.csv', index_col=0)\n",
      "        fb_DF_unique = fb_DF.drop_duplicates(cols='FB_matchUrl', take_last=False)\n",
      "        tw_DF = pd.read_csv( inputFolder+orgName+'_twitter.csv', index_col=0)\n",
      "        tw_DF_unique = tw_DF.drop_duplicates(cols='TW_matchUrl', take_last=False)\n",
      "        \n",
      "        # Calculate number of duplicates\n",
      "        fb_series = fb_DF['FB_matchUrl'].value_counts()\n",
      "        fb_url_count = fb_series.to_frame('Counts')\n",
      "        fb_url_count.to_csv(outputFolder+'DupStat/dup_count_'+orgName+'_fb.csv', encoding='utf-8')\n",
      "        tw_series = tw_DF['TW_matchUrl'].value_counts()\n",
      "        tw_url_count = tw_series.to_frame('Counts')\n",
      "        tw_url_count.to_csv(outputFolder+'DupStat/dup_count_'+orgName+'_tw.csv', encoding='utf-8')\n",
      "        \n",
      "        # Merge both twitter and facebook data\n",
      "        merged = pd.merge(tw_DF, fb_DF, left_on='TW_matchUrl', right_on='FB_matchUrl', how=\"inner\")\n",
      "        merged.to_csv(outputFolder+orgName+'_merged.csv', encoding='utf-8')\n",
      "        merged_unique = merged.drop_duplicates(cols='TW_matchUrl', take_last=False)\n",
      "        \n",
      "        #Save results\n",
      "        summaryDF.loc[rowCount] = [orgName, tw_DF.shape[0],tw_DF_unique.shape[0],tw_DF.shape[0]-tw_DF_unique.shape[0],\n",
      "                                   fb_DF.shape[0],fb_DF_unique.shape[0],fb_DF.shape[0]-fb_DF_unique.shape[0],\n",
      "                                   merged_unique.shape[0]]\n",
      "        rowCount += 1\n",
      "    return summaryDF\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "summaryStat = mergeUrlFiles(pathExtractedUrl, pathMergedUrl, organizationNames)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "abcnews\n",
        "afp"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ap"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "bbc"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "bostonglobe"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cbsnews"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cnn"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dailymail"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "foxnews"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "huffingtonpost"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "latimes"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "msnbc"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "nbcnews"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "newsweek"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "npr"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "nytimes"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "reuters"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "slate"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "thedailybeast"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "theguardian"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "time"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "usatoday"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "washingtonpost"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "wsj"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "yahoonews"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Users\\JS\\Anaconda\\lib\\site-packages\\pandas\\util\\decorators.py:53: FutureWarning: cols is deprecated, use subset instead\n",
        "  warnings.warn(msg, FutureWarning)\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "END"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Summary:\n",
      "\n",
      "###The following code extracts social metrics data such as Facebook, Twitter, Bitly, Sentiment Social Media Data and merges them with matched Url Data###\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Libraries.\n",
      "import json\n",
      "import os\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from urlparse import urlparse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Imports.\n",
      "import re\n",
      "\n",
      "# Define pattern.\n",
      "urlPattern = '(?i)\\\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\\\s()<>{}\\\\[\\\\]]+|\\\\([^\\\\s()]*?\\\\([^\\\\s()]+\\\\)[^\\\\s()]*?\\\\)|\\\\([^\\\\s]+?\\\\))+(?:\\\\([^\\\\s()]*?\\\\([^\\\\s()]+\\\\)[^\\\\s()]*?\\\\)|\\\\([^\\\\s]+?\\\\)|[^\\\\s`!()\\\\[\\\\]{};:\\'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019])|(?:(?<!@)[a-z0-9]+(?:[.\\\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\\\b/?(?!@)))'\n",
      "\n",
      "\"\"\"\n",
      "Detect if a URL is a link to a Facebook photo.\n",
      "\n",
      "Examples:\n",
      "\turlIsFacebookPhoto('https://www.facebook.com/bbcworldnews/photos/pcb.10153822033402588/10153822032512588/?type=1') == True\n",
      "\turlIsFacebookPhoto('http://bbc.in/fgh543q') == False\t\n",
      "\"\"\"\n",
      "def urlIsFacebookPhoto(url):\n",
      "\treturn bool(re.match('^https?://(www\\\\.)?facebook.com/[a-zA-Z0-9]+/photos/.+$', url))\n",
      "\n",
      "\"\"\"\n",
      "Detect if a URL is a link to a Facebook video.\n",
      "\n",
      "Examples:\n",
      "\turlIsFacebookPhoto('https://www.facebook.com/video.php?v=10150471411394999') == True\n",
      "\turlIsFacebookPhoto('http://bbc.in/fgh543q') == False\t\n",
      "\"\"\"\n",
      "def urlIsFacebookVideo(url):\n",
      "\treturn bool(re.match('^https?://(www\\\\.)?facebook.com/video.php\\\\?.+$', url))\n",
      "\n",
      "\"\"\"\n",
      "TODO\n",
      "\"\"\"\n",
      "def urlsInText(t):\n",
      "\t# Perform search.\n",
      "\turlMatches = re.search(urlPattern, t)\n",
      "\tif urlMatches:\n",
      "\t\treturn [g for g in urlMatches.groups()]\n",
      "\treturn []\n",
      "\n",
      "def urlIsFacebookLink(url):\n",
      "    return bool(re.match('^https?://(www\\\\.)?facebook.com.+$', url))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sources = ['twitter','facebook']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "address = ['1. twitter', '2. facebook']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# File paths needed for data processing\n",
      "pathConsolidated = os.getcwd()+'/Social News/data/%s/consolidated/'\n",
      "pathMergedUrls = os.getcwd()+'/Social News/data/5. mergedUrls/'\n",
      "pathSocialMediaMetrics = os.getcwd()+'/Social News/data/6. socialMetrics/'\n",
      "pathBitly = os.getcwd()+'/Social News/data/7. bitly/'\n",
      "pathSentiment = os.getcwd()+'/Social News/data/9. sentiment/'\n",
      "pathFinalTable = os.getcwd()+'/Social News/data/8. finalTable/'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get all files to traverse. Assume that we can get everything in consolidated folder.\n",
      "postFiles = []\n",
      "for source in address:\n",
      "    sourcePath = pathConsolidated % source\n",
      "    postFiles += [(source,p[:p.find('-')],sourcePath + p) for p in os.listdir(sourcePath) if p[-5:] == '.json']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Uncomment to see all files read in\n",
      "#postFiles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Extract Organization Names\n",
      "organizationNames = [x[1].split(\".\")[0] for x in postFiles]\n",
      "organizationNames = set(organizationNames)\n",
      "organizationNames = list(organizationNames)\n",
      "organizationNames.sort()\n",
      "len(organizationNames)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "25"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#1. Extract Social Metrics Data#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Function extracts different social metrics from RAW twitter and facebook posts JSON files depending on the social media medium\n",
      "Twitter social metrics: twitter ID, twitter article URL, favorite, retweet, twitter text, twitter post time\n",
      "Facebook social metrics: facebook ID, facebook article URL, like, share, facebook text, facebook post time\n",
      "param[in] filenames: folder+filenames of the extracted url csv files\n",
      "param[in] destFolder: destination to save the output csv files\n",
      "\"\"\"\n",
      "def extractSocialMetrics(filenames, destFolder):\n",
      "    tw_columns = ['tw_id','tw_expandedUrl','tw_favorites', 'tw_retweet', 'tw_text', 'tw_postTime']\n",
      "    fb_columns = ['fb_id','fb_expandedUrl','fb_likes', 'fb_share', 'fb_text', 'fb_postTime']\n",
      "    rowCount = 0\n",
      "    # Loop through each news organization to compile social metric statistics\n",
      "    for organization in filenames:\n",
      "        socialMedia = organization[0].split(' ')[-1]\n",
      "        print organization[1]+\" \"+socialMedia\n",
      "        if socialMedia == 'twitter':\n",
      "            columnNames = tw_columns\n",
      "        else:\n",
      "            columnNames = fb_columns\n",
      "        # Create DF with column names based on twitter or facebook\n",
      "        resultDF = pd.DataFrame(columns=columnNames)\n",
      "        \n",
      "        # Load fb/tw json objects\n",
      "        data = json.load(open(organization[2]))\n",
      "        for article in data:\n",
      "            # Data common to Twitter and Facebook.\n",
      "            postId = article['id']\n",
      "            #Twitter data\n",
      "            if socialMedia == 'twitter':\n",
      "                #Account for tweets from account holder or retweeting another tweet\n",
      "                if 'retweeted_status' in article and 'favorite_count' in article['retweeted_status']:\n",
      "                    favorites = article['retweeted_status']['favorite_count']\n",
      "                elif 'favorite_count' in article:\n",
      "                    favorites = article['favorite_count']\n",
      "                else:\n",
      "                    favorites = np.nan\n",
      "                retweet = article['retweet_count']\n",
      "                postTime = article['created_at']\n",
      "                text = article['text']\n",
      "                if len(article['entities']['urls']) == 0:\n",
      "                    expandedUrl = np.nan\n",
      "                else:\n",
      "                    expandedUrl = article['entities']['urls'][0]['expanded_url']\n",
      "            #Facebook data\n",
      "            else:\n",
      "                if 'link' in article and (urlIsFacebookPhoto(article['link']) or urlIsFacebookVideo(article['link'])):\n",
      "                    #expandedUrl = urlsInText(article['message'])\n",
      "                    if 'message' in article:\n",
      "                        if len(urlsInText(article['message'])) > 0:\n",
      "                            expandedUrl = urlsInText(article['message'])[0]\n",
      "                        else:\n",
      "                            expandedUrl = np.nan\n",
      "                    else:\n",
      "                        expandedUrl = np.nan\n",
      "                elif 'link' in article:\n",
      "                    expandedUrl = article['link']\n",
      "                else:\n",
      "                    expandedUrl = np.nan\n",
      "                if 'likes' in article and 'summary' in article['likes'] and 'total_count' in article['likes']['summary']:\n",
      "                    favorites = article['likes']['summary']['total_count']\n",
      "                else:\n",
      "                    favorites = np.nan\n",
      "                if 'shares' in article and 'count' in article['shares']:\n",
      "                    retweet = article['shares']['count']\n",
      "                else:\n",
      "                    retweet = np.nan\n",
      "                postTime = article['created_time']\n",
      "                if 'message' in article:\n",
      "                    text = article['message']\n",
      "                elif 'description' in article:\n",
      "                    text = article['description']\n",
      "                else:\n",
      "                    text = np.nan\n",
      "            resultDF.loc[rowCount] = [postId, expandedUrl, favorites, retweet, text, postTime]\n",
      "            rowCount += 1\n",
      "        #Save DF to csv file\n",
      "        filename = organization[1].split('.')[0]\n",
      "        resultDF.to_csv(destFolder+filename+'_'+socialMedia+'.csv', encoding='utf-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# NYTimes Example\n",
      "#extractSocialMetrics([postFiles[15], postFiles[40]], tw_columns, pathSocialMediaMetrics)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# BBC Example\n",
      "#extractSocialMetrics([postFiles[3], postFiles[28]], pathSocialMediaMetrics)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Extract Social Metrics from all news organizations\n",
      "extractSocialMetrics(postFiles, pathSocialMediaMetrics)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Load in Bitly Data#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Function extracts different bitly link counts from bitly json objects\n",
      "param[in] bitlyInputFolder: folder+filenames of the bitly json objects\n",
      "param[in] destFolder: destination to save the output csv files\n",
      "param[in] orgNames: list of organization names used for looping through the files\n",
      "param[in] socialMedia: 'twitter' or 'facebook'\n",
      "\"\"\"\n",
      "def extractBitly(bitlyInputFolder, destFolder, orgNames, socialMedia):\n",
      "    # Loop through each orgagnization\n",
      "    for orgName in orgNames:\n",
      "        print orgName\n",
      "        rowCount = 0\n",
      "        # Use different column names depending on different social media medium\n",
      "        if socialMedia == 'twitter':\n",
      "            bitlyDF = pd.DataFrame(columns=['tw_url', 'tw_link_clicks', 'tw_timeRetrieved'])\n",
      "        else:\n",
      "            bitlyDF = pd.DataFrame(columns=['fb_url', 'fb_link_clicks', 'fb_timeRetrieved'])\n",
      "        \n",
      "        bitlyJson = json.load(open(bitlyInputFolder+orgName+'.json'))\n",
      "        \n",
      "        # \n",
      "        for key, value in bitlyJson.items():\n",
      "            # Make sure each bitly json object does not have an error\n",
      "            if ('data' in value) and (value['status_txt'] != 'NOT_FOUND' and value['status_txt'] != 'UNKNOWN_ERROR'):\n",
      "                links_clicks = value['data']['link_clicks']\n",
      "            # Skip bitly links if corrupted\n",
      "            else:\n",
      "                links_clicks = np.nan\n",
      "            value['retrieved']\n",
      "            #Save results\n",
      "            bitlyDF.loc[rowCount] = [key, links_clicks, value['retrieved']]\n",
      "            rowCount += 1\n",
      "        print bitlyDF.shape\n",
      "        bitlyDF.to_csv(destFolder+orgName+'_'+socialMedia+'.csv', encoding='utf-8')\n",
      "    return bitlyDF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Example\n",
      "#extractBitly(pathBitly+'twitter/', pathBitly+'1. extractedBitly/', ['nbcnews'], 'twitter').head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Extract bitly links from twitter example\n",
      "#extractBitly(pathBitly+'twitter/', pathBitly+'1. extractedBitly/', organizationNames, 'twitter').head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Extract bitly links from facebook example\n",
      "#extractBitly(pathBitly+'facebook/', pathBitly+'1. extractedBitly/', organizationNames, 'facebook').head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Load in Sentiment Data#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Function extracts sentiment data from sentiment analysis\n",
      "For both Facebook and Twitter: social media ID, post's classified sentiment, negative sentiment raw score, positive sentiment raw score\n",
      "param[in] sentimentInputFolder: folder+filenames of the sentiment data files\n",
      "param[in] destFolder: destination to save the output csv files\n",
      "param[in] orgNames: list of organization names used for looping through the files\n",
      "param[in] socialMedia: 'twitter' or 'facebook'\n",
      "\"\"\"\n",
      "def extractSentiment(sentimentInputFolder, destFolder, orgNames, socialMedia):\n",
      "    # Extract sentiment data for each news organization\n",
      "    for orgName in orgNames:\n",
      "        print orgName\n",
      "        rowCount = 0\n",
      "        cols = ['%s_id', '%s_sentiment_class', '%s_sentiment_negative', '%s_sentiment_positive']\n",
      "        sentDF = pd.DataFrame(columns=[c % ('tw' if socialMedia == 'twitter' else 'fb') for c in cols])\n",
      "        \n",
      "        sentJson = json.load(open(sentimentInputFolder+orgName+'.json'))\n",
      "        \n",
      "        for key, value in sentJson.items():\n",
      "            #print key\n",
      "            #print value\n",
      "            if value is not None:\n",
      "                if socialMedia == 'twitter':\n",
      "                    sent_class = value['class']\n",
      "                    sent_neg = value['meanScoreNegSig']\n",
      "                    sent_pos = value['meanScorePosSig']\n",
      "                else:\n",
      "                    sent_class = value['message']['class']\n",
      "                    sent_neg = value['message']['meanScoreNegSig']\n",
      "                    sent_pos = value['message']['meanScorePosSig']\n",
      "            else:\n",
      "                sent_class = sent_neg = sent_pos = None\n",
      "            #Save results\n",
      "            sentDF.loc[rowCount] = [key, sent_class, sent_neg, sent_pos]\n",
      "            rowCount += 1\n",
      "        print sentDF.shape\n",
      "        sentDF.to_csv(destFolder+orgName+'_'+socialMedia+'.csv', encoding='utf-8')\n",
      "    \n",
      "    return sentDF    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Extract sentiment from twitter\n",
      "#extractSentiment(pathSentiment+'twitter/', pathSentiment+'1. extractedSentiment/', organizationNames, 'twitter').head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Extract sentiment from facebook\n",
      "#extractSentiment(pathSentiment+'facebook/', pathSentiment+'1. extractedSentiment/', organizationNames, 'facebook').head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#2. Merge the Facebook, Twitter, Bitly, Sentiment Social Media Data with Url Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Function uses mergedUrls files to merge with social metrics data\n",
      "param[in] socialMedia: 'twitter' or 'facebook'\n",
      "param[in] socialMedia_DF: data frame containing social media metrics data\n",
      "param[in] socialMedia_mergedUrls_DF: data containing merged urls from twitter and facebook data\n",
      "param[in] destFolder: destination to save the output intermediate csv files\n",
      "param[in] orgName: python list of news organizations\n",
      "\"\"\"\n",
      "def sumSocialMetrics(socialMedia, socialMedia_DF, socialMedia_mergedUrls_DF, destFolder, orgName):\n",
      "    # Twitter\n",
      "    if socialMedia == 'twitter':\n",
      "        # Drop duplicates in mergedUrls files\n",
      "        social_w_Urls = socialMedia_DF.merge(socialMedia_mergedUrls_DF, left_on='tw_expandedUrl', right_on='TW_ShortenedUrl', how=\"inner\")\n",
      "        social_w_Urls = social_w_Urls.drop_duplicates(cols=['tw_expandedUrl', 'tw_postTime'] , take_last=True)\n",
      "        social_w_Urls.to_csv(destFolder+'1. Intermediates/PreSum/'+orgName+'_PreSumSocialwUrls_twitter.csv', encoding='utf-8')\n",
      "        # Account for posts that are reposted. Sum up the social metrics and keep count how many times the post is reposted\n",
      "        social_w_Urls = social_w_Urls.groupby(['tw_expandedUrl']).agg([np.sum, len])\n",
      "        social_w_Urls.to_csv(destFolder+'1. Intermediates/'+orgName+'_SocialwUrls_twitter.csv', encoding='utf-8')\n",
      "    \n",
      "        # Merge with social metrics data files\n",
      "        DF_summed = socialMedia_DF.merge(social_w_Urls[['tw_favorites', 'tw_retweet', 'tw_text', 'tw_postTime']], \n",
      "                                   left_on='tw_expandedUrl', right_index=True , how=\"inner\")\n",
      "        DF_summed.columns = ['tw_id','tw_expandedUrl', 'tw_favorites', 'tw_retweet', 'tw_text', 'tw_postTime',\n",
      "                                 'tw_favorites_Sum', 'tw_favorities_Count', 'tw_retweet_Sum', 'tw_retweet_Count',\n",
      "                                 'tw_text_Sum', 'tw_text_Count', 'tw_text_Sum', 'tw_article_appearance_Count']\n",
      "        DF_summed = DF_summed.drop(['tw_favorities_Count', 'tw_retweet_Count', 'tw_text_Count'], axis=1)\n",
      "        DF_summed = DF_summed.drop_duplicates(cols='tw_expandedUrl', take_last=True)\n",
      "        DF_summed.to_csv(destFolder+'1. Intermediates/'+orgName+'_SocialwUrls_twitter_merged.csv', encoding='utf-8')\n",
      "    # Facebook\n",
      "    else:\n",
      "        # Drop duplicates in mergedUrls files\n",
      "        social_w_Urls = socialMedia_DF.merge(socialMedia_mergedUrls_DF, left_on='fb_expandedUrl', right_on='FB_ShortenedUrl', how=\"inner\")\n",
      "        social_w_Urls = social_w_Urls.drop_duplicates(cols=['fb_expandedUrl', 'fb_postTime'] , take_last=True)\n",
      "        social_w_Urls.to_csv(destFolder+'1. Intermediates/PreSum/'+orgName+'_PreSumSocialwUrls_facebook.csv', encoding='utf-8')\n",
      "        # Account for posts that are reposted. Sum up the social metrics and keep count how many times the post is reposted\n",
      "        social_w_Urls = social_w_Urls.groupby(['fb_expandedUrl']).agg([np.sum, len])\n",
      "        social_w_Urls.to_csv(destFolder+'1. Intermediates/'+orgName+'_SocialwUrls_facebook.csv', encoding='utf-8')\n",
      "    \n",
      "        # Merge with social metrics data files\n",
      "        DF_summed = socialMedia_DF.merge(social_w_Urls[['fb_likes', 'fb_share', 'fb_text', 'fb_postTime']], \n",
      "                                   left_on='fb_expandedUrl', right_index=True , how=\"inner\")\n",
      "        DF_summed.columns = ['fb_id','fb_expandedUrl', 'fb_likes', 'fb_share', 'fb_text', 'fb_postTime',\n",
      "                                 'fb_likes_Sum', 'fb_likes_Count', 'fb_share_Sum', 'fb_share_Count',\n",
      "                                 'fb_text_Sum', 'fb_text_Count', 'fb_text_Sum', 'fb_article_appearance_Count']\n",
      "        DF_summed = DF_summed.drop(['fb_likes_Count', 'fb_share_Count', 'fb_text_Count'], axis=1)\n",
      "        DF_summed = DF_summed.drop_duplicates(cols='fb_expandedUrl', take_last=True)\n",
      "        DF_summed.to_csv(destFolder+'1. Intermediates/'+orgName+'_SocialwUrls_facebook_merged.csv', encoding='utf-8')\n",
      "    return DF_summed"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Function used to create the final data table used for the analysis\n",
      "Finaltable includes, matched url, social metrics, bitly data, and sentiment data\n",
      "param[in] mergedUrlsInputFolder: folder containing twitter and facebook merged URLs data\n",
      "param[in] socialMetricsInputFolder: folder containing twitter and facebook social metrics data\n",
      "param[in] bitlyInputFolder: folder containing bitly data\n",
      "param[in] sentimentInputFolder: folder containing sentiment data\n",
      "param[in] orgNames: python list of news organizations\n",
      "param[in] destFolder: destination to save the output final table csv files\n",
      "\"\"\"\n",
      "def mergeSocialMetricsData(mergedUrlsInputFolder, socialMetricsInputFolder, bitlyInputFolder, sentimentInputFolder, orgNames, destFolder):\n",
      "    rowCount = 0\n",
      "    facebookLinks = 0 # fb photo + video links\n",
      "    facebookCount = 0 # all fb links\n",
      "    facebookDict = {}\n",
      "    tw_repostCounts = 0 # twitter repost counter\n",
      "    fb_repostCounts = 0 # facebook repost counter\n",
      "    \n",
      "    for orgName in orgNames:\n",
      "        print orgName\n",
      "        # Read in social metrics data\n",
      "        fb_DF = pd.read_csv( socialMetricsInputFolder+orgName+'_facebook.csv', index_col=0)\n",
      "        tw_DF = pd.read_csv( socialMetricsInputFolder+orgName+'_twitter.csv', index_col=0)\n",
      "        # Read in bitly data\n",
      "        bitly_tw = pd.read_csv( bitlyInputFolder+'1. extractedBitly/'+orgName+'_twitter.csv', index_col=0)\n",
      "        bitly_fb = pd.read_csv( bitlyInputFolder+'1. extractedBitly/'+orgName+'_facebook.csv', index_col=0)\n",
      "        # Read in sentiment data\n",
      "        sent_tw = pd.read_csv( sentimentInputFolder+'1. extractedSentiment/'+orgName+'_twitter.csv', index_col=0)\n",
      "        sent_fb = pd.read_csv( sentimentInputFolder+'1. extractedSentiment/'+orgName+'_facebook.csv', index_col=0)\n",
      "        # Read in merged articles' urls\n",
      "        mergedUrls_DF = pd.read_csv( mergedUrlsInputFolder+orgName+'_merged.csv', index_col=0)\n",
      "        mergedUrls_DF = mergedUrls_DF.drop_duplicates(cols='TW_matchUrl', take_last=False)\n",
      "        \n",
      "        #Merging fb and tw social data with merged articles' urls\n",
      "        tw_DF_summed = sumSocialMetrics('twitter', tw_DF, mergedUrls_DF, destFolder, orgName)\n",
      "        fb_DF_summed = sumSocialMetrics('facebook', fb_DF, mergedUrls_DF, destFolder, orgName)\n",
      "        finalmerged_DF = tw_DF_summed.merge(mergedUrls_DF, left_on='tw_expandedUrl', right_on='TW_ShortenedUrl', how=\"inner\")\n",
      "        finalmerged_DF = finalmerged_DF.merge(fb_DF_summed, left_on='FB_ShortenedUrl', right_on='fb_expandedUrl', how=\"inner\")\n",
      "        \n",
      "        # Merge in Bitly\n",
      "        finalmerged_DF = finalmerged_DF.merge(bitly_tw, left_on='TW_ShortenedUrl', right_on='tw_url', how=\"left\", left_index=True)\n",
      "        finalmerged_DF = finalmerged_DF.merge(bitly_fb, left_on='FB_ShortenedUrl', right_on='fb_url', how=\"left\", left_index=True)\n",
      "        \n",
      "        # Merge in Sentiment\n",
      "        finalmerged_DF = finalmerged_DF.merge(sent_tw, left_on='tw_id', right_on='tw_id', how=\"left\", copy=False)\n",
      "        finalmerged_DF = finalmerged_DF.merge(sent_fb, left_on='fb_id', right_on='fb_id', how=\"left\", copy=False)\n",
      "        \n",
      "        # Rename column names\n",
      "        finalmerged_DF.rename(columns={'tw_article_appearance_Count_x':'tw_article_appearance_Count', \n",
      "                                       'FB_HistoryUrls_x':'FB_HistoryUrls',\n",
      "                                       'fb_article_appearance_Count_x':'fb_article_appearance_Count',\n",
      "                                       'tw_timeRetrieved_x':'tw_timeRetrieved',\n",
      "                                       'fb_timeRetrieved_x':'fb_timeRetrieved',\n",
      "                                       'tw_sentiment_positive_x':'tw_sentiment_positive'}, inplace=True)\n",
      "        # To csv\n",
      "        finalmerged_DF.to_csv(destFolder+orgName+'_finaltable.csv', encoding='utf-8')\n",
      "        rowCount += finalmerged_DF.shape[0]\n",
      "        print finalmerged_DF.shape\n",
      "        tw_repostCounts += finalmerged_DF[finalmerged_DF['tw_article_appearance_Count'] > 1].count()['tw_expandedUrl']\n",
      "        fb_repostCounts += finalmerged_DF[finalmerged_DF.fb_article_appearance_Count > 1].count()['tw_expandedUrl']\n",
      "        \n",
      "        \n",
      "    print \"Number of articles: %d\" % rowCount\n",
      "    print \"Number of tw reposts: %d\" % tw_repostCounts\n",
      "    print \"Number of fb reposts: %d\" % fb_repostCounts\n",
      "    return fb_DF, tw_DF, mergedUrls_DF, finalmerged_DF\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# BBC and NBCnews Example\n",
      "test_fb_DF, test_tw_DF, test_mergedUrls_DF, test_finalmerged_DF = mergeSocialMetricsData(pathMergedUrls, pathSocialMediaMetrics, \n",
      "                                                                                         pathBitly, pathSentiment, [organizationNames[3], organizationNames[12]], pathFinalTable)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "bbc\n",
        "(602, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "nbcnews\n",
        "(1759, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of articles: 2361\n",
        "Number of tw reposts: 12\n",
        "Number of fb reposts: 18\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Users\\JS\\Anaconda\\lib\\site-packages\\pandas\\util\\decorators.py:53: FutureWarning: cols is deprecated, use subset instead\n",
        "  warnings.warn(msg, FutureWarning)\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Full Run\n",
      "fb_DF, tw_DF, mergedUrls_DF, finalmerged_DF = mergeSocialMetricsData(pathMergedUrls, pathSocialMediaMetrics, \n",
      "                                                                                         pathBitly, pathSentiment, organizationNames, pathFinalTable)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "abcnews\n",
        "(717, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "afp\n",
        "(101, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ap\n",
        "(126, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "bbc\n",
        "(602, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "bostonglobe\n",
        "(1106, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cbsnews\n",
        "(1279, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cnn\n",
        "(1157, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dailymail\n",
        "(1585, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "foxnews\n",
        "(407, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "huffingtonpost\n",
        "(1537, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "latimes\n",
        "(737, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "msnbc\n",
        "(1272, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "nbcnews\n",
        "(1759, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "newsweek\n",
        "(472, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "npr\n",
        "(368, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "nytimes\n",
        "(1600, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "reuters\n",
        "(381, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "slate\n",
        "(741, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "thedailybeast\n",
        "(1802, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "theguardian\n",
        "(1192, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "time\n",
        "(2209, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "usatoday\n",
        "(444, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "washingtonpost\n",
        "(908, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "wsj\n",
        "(477, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "yahoonews\n",
        "(654, 46)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of articles: 23633\n",
        "Number of tw reposts: 2019\n",
        "Number of fb reposts: 440\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "END"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    }
   ],
   "metadata": {}
  }
 ]
}