{
 "metadata": {
  "name": "",
  "signature": "sha256:f16a6beedaf7e16b7f7d50456064bf5fc419bebc42a6390bc618aef957c240be"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Topic Modeling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This section is the codes we used to generate the topic for each post of twitter and facebook.  We used the libraries nltk and gensim to build our model. In general, there are two steps of topic generation. The first step is to lemmatize each post and build dictionary. The second step is to build the topic model and generate topic for each post."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Installation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to perform topic modeling. We need to install the libraries nltk and gensim.\n",
      "\n",
      "`pip install gensim\n",
      "\n",
      "`pip install nltk"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import\n",
      "%matplotlib inline\n",
      "\n",
      "import string\n",
      "import nltk\n",
      "import itertools\n",
      "import gensim\n",
      "from gensim import corpora, models, similarities\n",
      "import collections\n",
      "from collections import defaultdict\n",
      "import sys\n",
      "import os\n",
      "import logging\n",
      "import random\n",
      "import operator\n",
      "import csv\n",
      "import time\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "# below for saving/loading files\n",
      "import pickle\n",
      "# below for search/replace of escape characters\n",
      "import re\n",
      "# below for bigrams\n",
      "from nltk.collocations import BigramCollocationFinder\n",
      "from nltk.metrics import BigramAssocMeasures\n",
      "from nltk import bigrams\n",
      "# below for part of speech tagging\n",
      "# uses 'taggers/maxent_treebank_pos_tagger/english.pickle'\n",
      "from nltk.corpus import wordnet\n",
      "# below for tokenizing\n",
      "from nltk.tokenize import word_tokenize, sent_tokenize\n",
      "# below for lemmatizer\n",
      "from nltk.stem import WordNetLemmatizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Lemmatization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pre-process the texts of posts to ntlk format. Generate the lemmatized post as input to train topic model."
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Load the cleaned data from the table"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fianltabledir = 'D:/Quezacot/Dropbox/Social News/data/8. finalTable/'\n",
      "# Get all files to traverse. Assume that we can get everything in consolidated folder.\n",
      "postFiles = []\n",
      "postFiles += [(fianltabledir + p) for p in os.listdir(fianltabledir) if p[-4:] == '.csv']\n",
      "filenames = []\n",
      "filenames += [re.search('(.+?)_finaltable.csv', p).group(1) for p in os.listdir(fianltabledir) if p[-4:] == '.csv']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####clean data for training topic model\n",
      "We eliminate the stop words defined by nltk and only preserve nouns."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tag_set = ['NN','NNS']\n",
      "\n",
      "# our stopsets\n",
      "custom_stopset = []\n",
      "\n",
      "stopset_unigram = set(nltk.corpus.stopwords.words('english'))\n",
      "stopset_unigram = stopset_unigram.union(set(custom_stopset))\n",
      "stopset_bigrams = []"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####function\n",
      "generate lemmatized text from original post."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rmwords(urllist, postlist):\n",
      "    # remove punctuation, remove stopwords, remove words < 3, remove non alpha words, spell correct, lemmatize\n",
      "    # you need to comment out the below decorator if you don't want to run parallel\n",
      "\n",
      "    # we need to strip escape characters so we compile a pattern\n",
      "    hexchars = re.compile('\\\\\\\\x(\\w{2})')\n",
      "    # http://\n",
      "    regex_urls = re.compile('https?:\\/\\/\\S+(\\s|$)')\n",
      "    # RT\n",
      "    regex_rt = re.compile('RT @\\w+: (.@\\w+ )?')\n",
      "    # @ #\n",
      "    regex_at = re.compile('(@|#)\\S+(\\s|$)')\n",
      "\n",
      "    # Instantiate a WordNet Lemmatizer\n",
      "    wnl = WordNetLemmatizer()\n",
      "\n",
      "    # convert penn_tag to morphy_tag used by WordNet Lemmatizer\n",
      "    # http://stackoverflow.com/questions/5364493/lemmatizing-pos-tagged-words-with-nltk\n",
      "    # morphy_tag = {'NN':wordnet.NOUN,'JJ':wordnet.ADJ,'VB':wordnet.VERB,'RB':wordnet.ADV}\n",
      "    morphy_tag = {'NN':wordnet.NOUN}\n",
      "\n",
      "    urls = []\n",
      "    texts = []\n",
      "    texts_pos = []\n",
      "\n",
      "    for i in xrange(len(postlist)):\n",
      "        review_text = postlist[i]\n",
      "        \n",
      "        # some text not recognized as string\n",
      "        if (type(review_text) != 'str'):\n",
      "            review_text = str(review_text)\n",
      "        \n",
      "        # deal with escape characters\n",
      "        review_text = re.sub(hexchars, '', review_text.encode(\"string-escape\"))\n",
      "        # remove http:// re.sub(pattern, repl, string, count=0, flags=0)\n",
      "        review_text = re.sub(regex_urls, '', review_text)\n",
      "        # remove RT @...:\n",
      "        review_text = re.sub(regex_rt, '', review_text)\n",
      "        # remove @ #\n",
      "        review_text = re.sub(regex_at, '', review_text)\n",
      "\n",
      "        # tokenize\n",
      "        review_text = [word for sent in sent_tokenize(review_text) for word in word_tokenize(sent)]\n",
      "\n",
      "        # get parts of speech\n",
      "        review_text_pos = [word for word in nltk.pos_tag(review_text)]\n",
      "        texts_pos.append(review_text_pos)\n",
      "\n",
      "        review_text = []\n",
      "        for word in review_text_pos:\n",
      "            # Only preserve (ADJ, ADV, NOUN, and VERB)\n",
      "            if (not word[1][:2] in morphy_tag):\n",
      "                continue\n",
      "            # remove smallwords\n",
      "            if (len(word[0]) < 3):\n",
      "                continue\n",
      "            # Only consider alpha words\n",
      "            if (not word[0].isalpha()):\n",
      "                continue\n",
      "            # lowercase all words except abbreviations\n",
      "            if (word[0][0].isupper() and word[0][1:].islower()):\n",
      "                word = (word[0].lower(),word[1])\n",
      "            # remove stopwords\n",
      "            if (word[0] in stopset_unigram):\n",
      "                continue\n",
      "            # Lemmatize using the Wordnet Lemmatizer\n",
      "            word = wnl.lemmatize(word[0],morphy_tag[word[1][:2]])    \n",
      "\n",
      "            review_text.append(word)\n",
      "\n",
      "        # add to our list\n",
      "        texts.append(review_text)\n",
      "        urls.append(urllist[i])\n",
      "        \n",
      "    return urls, texts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####function\n",
      "save the lemmatized lists of posts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "savedir = 'objs/'\n",
      "def savelist(filename, listname, namesuffix):\n",
      "    fullpath = savedir + filename + namesuffix\n",
      "    with open(fullpath, 'w') as f:\n",
      "        pickle.dump(listname, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#load all the tables and lemmatize the posts. Store all of the lemmatized posts in two lists. (twitter and facebook)\n",
      "tw_urls = []\n",
      "tw_all_text = []\n",
      "fb_urls = []\n",
      "fb_all_text = []\n",
      "\n",
      "for i in xrange(len(postFiles)):\n",
      "    #load file as csv\n",
      "    DF = pd.read_csv( postFiles[i], index_col=0 )\n",
      "    print i, \"file %s, size %d\" % (filenames[i], len(DF))\n",
      "    \n",
      "    #build tw nltk tag\n",
      "    urls, nltxt = rmwords(DF['tw_expandedUrl'].tolist(), DF['tw_text'].tolist())\n",
      "    tw_urls.extend(urls)\n",
      "    tw_all_text.extend(nltxt)\n",
      "\n",
      "    #build fb nltk tag\n",
      "    urls, nltxt = rmwords(DF['tw_expandedUrl'].tolist(), DF['fb_text'].tolist())\n",
      "    fb_urls.extend(urls)\n",
      "    fb_all_text.extend(nltxt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 file abcnews, size 717\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file afp, size 101\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file ap, size 126\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file bbc, size 602\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file bostonglobe, size 1106\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file cbsnews, size 1279\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file cnn, size 1157\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file dailymail, size 1585\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file foxnews, size 407\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file huffingtonpost, size 1537\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file latimes, size 737\n",
        "11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file msnbc, size 1272\n",
        "12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file nbcnews, size 1759\n",
        "13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file newsweek, size 472\n",
        "14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file npr, size 368\n",
        "15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file nytimes, size 1600\n",
        "16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file reuters, size 381\n",
        "17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file slate, size 741\n",
        "18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file thedailybeast, size 1802\n",
        "19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file theguardian, size 1192\n",
        "20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file time, size 2209\n",
        "21"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file usatoday, size 444\n",
        "22"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file washingtonpost, size 908\n",
        "23"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file wsj, size 477\n",
        "24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file yahoonews, size 654\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save lists\n",
      "savelist('tw_urls', tw_urls, '.pickle')\n",
      "savelist('tw_all_text', tw_all_text, '.pickle')\n",
      "savelist('fb_urls', fb_urls, '.pickle')\n",
      "savelist('fb_all_text', fb_all_text, '.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####function\n",
      "load the lemmatized lists of posts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loaddir = 'objs/'\n",
      "def loadlist(filename, namesuffix):\n",
      "    fullpath = loaddir + filename + namesuffix\n",
      "    print \"Load file %s\" % (filename + namesuffix)\n",
      "    with open(fullpath) as f:\n",
      "        load_list = pickle.load(f)\n",
      "    return load_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load lists\n",
      "namesuffix = '.pickle'\n",
      "tw_urls = loadlist('tw_urls', namesuffix)\n",
      "tw_all_text = loadlist('tw_all_text', namesuffix)\n",
      "fb_urls = loadlist('fb_urls', namesuffix)\n",
      "fb_all_text = loadlist('fb_all_text', namesuffix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Load file tw_urls.pickle\n",
        "Load file tw_all_text.pickle"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Load file fb_urls.pickle"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Load file fb_all_text.pickle"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Model Building"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate topic model from the lemmatized posts. We build the LDA model based on the dictionary of the posts."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Number of topics to use in LDA model.\n",
      "\n",
      "This is an important parameter of model training. For fewer topics we can have more posts of each topic, but the topics with the same topic may not be much related. For more topics the opposite would be true. The posts with the same topic would be more related but each topic may have fewer posts. Since we want to analyze the difference of topics from twitter and facebook or different news organizations, we choose a fairly large number of topics to ensure the posts classified in the same topic are convincingly similar."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_topics = 500"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def LDA(all_text):\n",
      "    dictionary = corpora.Dictionary(all_text)\n",
      "    \n",
      "    # filter dictionary\n",
      "    # 1. less than no_below documents (absolute number) or\n",
      "    # 2. more than no_above documents (fraction of total corpus size, not absolute number).\n",
      "    # after (1) and (2), keep only the first keep_n most frequent tokens (or keep all if None).\n",
      "    # After the pruning, shrink resulting gaps in word ids. (this means word ids may change after gap shrinking)\n",
      "    dictionary.filter_extremes(no_below=50, no_above=0.15, keep_n=None)\n",
      "    \n",
      "    corpus_bow = [dictionary.doc2bow(post) for post in all_text]\n",
      "    model_tfidf = models.TfidfModel(corpus_bow)\n",
      "    corpus_tfidf = model_tfidf[corpus_bow]\n",
      "    \n",
      "    # choose one\n",
      "    #tw_corpus = tw_corpus_tfidf\n",
      "    corpus = corpus_bow\n",
      "    \n",
      "    model = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=num_topics, chunksize=500, \n",
      "                                     passes=10, update_every=0, alpha=None, eta=None, decay=0.5,\n",
      "                                     distributed=False)\n",
      "    \n",
      "    # generate the list of topics\n",
      "    model_topics = model.print_topics(num_topics)\n",
      "    \n",
      "    return model, corpus, model_topics, dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "#model of twitter posts\n",
      "tw_model, tw_corpus, tw_model_topics, tw_dict = LDA(tw_all_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 0 ns\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "#model of facebook posts\n",
      "fb_model, fb_corpus, fb_model_topics, fb_dict = LDA(fb_all_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 0 ns\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outcsvdir = 'csv/'\n",
      "# generate topics table as pd dara frame\n",
      "tw_topic = pd.DataFrame({'topic_num': range(len(tw_model_topics)), 'topic': tw_model_topics},\n",
      "                        columns = ['topic_num', 'topic'])\n",
      "fb_topic = pd.DataFrame({'topic_num': range(len(fb_model_topics)), 'topic': fb_model_topics},\n",
      "                        columns = ['topic_num', 'topic'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save topics table\n",
      "outcsvdir = 'csv/'\n",
      "tw_topic.to_csv(outcsvdir + 'tw_topics_table.csv', index=False)\n",
      "fb_topic.to_csv(outcsvdir + 'fb_topics_table.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#load topic table\n",
      "incsvdir = 'csv/'\n",
      "tw_topic = pd.read_csv( incsvdir + 'tw_topics_table.csv')\n",
      "fb_topic = pd.read_csv( incsvdir + 'fb_topics_table.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save dictionary as csv\n",
      "pd.DataFrame({'words in dictionary': tw_dict.values()}).to_csv(outcsvdir + 'tw_dict.csv', index=False)\n",
      "pd.DataFrame({'words in dictionary': fb_dict.values()}).to_csv(outcsvdir + 'fb_dict.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Tuning dictionary\n",
      "\n",
      "This part code is for tuning the parameters of dictionary filter. The result is these parameters do not have a large impact. It is probably because the posts from twitter and facebook are relatively very short compared to general articles. Thus, the word frequency appeared on all posts may not be distinguishable for filtering."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictdir = 'dict/'\n",
      "tw_dictionary = corpora.Dictionary(tw_all_text)\n",
      "fb_dictionary = corpora.Dictionary(fb_all_text)\n",
      "steps = np.linspace(0.1, 0.01, 10)\n",
      "tw_df_dict = pd.DataFrame()\n",
      "fb_df_dict = pd.DataFrame()\n",
      "for k in steps:\n",
      "    tw_dictionary.filter_extremes(no_below=50, no_above=k, keep_n=None)\n",
      "    tw_l = tw_dictionary.values()\n",
      "    for n in range(len(tw_l), len(tw_df_dict)):\n",
      "        tw_l.append('')\n",
      "    tw_df_dict[str(k)] = tw_l\n",
      "    \n",
      "    fb_dictionary.filter_extremes(no_below=50, no_above=k, keep_n=None)\n",
      "    fb_l = fb_dictionary.values()\n",
      "    for n in range(len(fb_l), len(fb_df_dict)):\n",
      "        fb_l.append('')\n",
      "    fb_df_dict[str(k)] = fb_l\n",
      "tw_df_dict.to_csv(dictdir + 'tw_dict.csv', index=False)\n",
      "fb_df_dict.to_csv(dictdir + 'fb_dict.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####function\n",
      "put a post into out topic model and obtain the topic for this post. Some posts may be classified to more than one topic and given different probabilities for the topics. We only choose the one with the highest probability."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# obtain the most probable topic of one post\n",
      "def fetchtopicnum(model, onecorpus):\n",
      "    # get the topics from the model\n",
      "    if( len(onecorpus) == 0 ):\n",
      "        return -1, 'NA'\n",
      "    \n",
      "    topics = model[onecorpus]\n",
      "    topic_num = topics[0][0]\n",
      "    topic_prob = topics[0][1]\n",
      "    for topic in topics:\n",
      "        temp_topic = topic[0]\n",
      "        temp_prob = topic[1]\n",
      "        # we only take the one with highest probability\n",
      "        if( temp_prob > topic_prob ):\n",
      "            topic_num = temp_topic\n",
      "            topic_prob = temp_prob\n",
      "    return topic_num, topic_prob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####fucntion\n",
      "put a list of posts into topic model and obtain the list of topics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# full topic list of all posts\n",
      "def poststopics(model, corpus, model_topics):\n",
      "    topic_num_list = []\n",
      "    topic_prob_list = []\n",
      "    post_topic_list = []\n",
      "    for i in xrange(len(corpus)):\n",
      "        topic_num, topic_prob = fetchtopicnum(model, corpus[i])\n",
      "        topic_num_list.append(topic_num)\n",
      "        topic_prob_list.append(topic_prob_list)\n",
      "        post_topic_list.append(model_topics[topic_num])\n",
      "    return topic_num_list, topic_prob_list, post_topic_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "#obtain the topic of each twitter post\n",
      "tw_topic_num_list, tw_topic_prob_list, tw_post_topic_list = poststopics(tw_model, tw_corpus, tw_model_topics)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 11.1 s\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "#obtain the topic of each facebook post\n",
      "fb_topic_num_list, fb_topic_prob_list, fb_post_topic_list = poststopics(fb_model, fb_corpus, fb_model_topics)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 13.3 s\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# generate topics of posts index by urls as pd data frame\n",
      "outcsvdir = 'csv/'\n",
      "tw_df = pd.DataFrame({'tw_expandedUrl': tw_urls,\n",
      "                      'topic_num': tw_topic_num_list},\n",
      "                     columns = ['tw_expandedUrl', 'topic_num'])\n",
      "fb_df = pd.DataFrame({'tw_expandedUrl': fb_urls,\n",
      "              'topic_num': fb_topic_num_list},\n",
      "              columns = ['tw_expandedUrl', 'topic_num'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save topics of posts\n",
      "outcsvdir = 'csv/'\n",
      "tw_df.to_csv( outcsvdir + 'tw_topic_nums.csv', index=False)\n",
      "fb_df.to_csv( outcsvdir + 'fb_topic_nums.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load topics of posts\n",
      "incsvdir = 'csv/'\n",
      "tw_df = pd.read_csv( incsvdir + 'tw_topic_nums.csv' )\n",
      "fb_df = pd.read_csv( incsvdir + 'fb_topic_nums.csv' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# count the number of posts of each topic\n",
      "tw_topic_count = collections.Counter(tw_topic_num_list)\n",
      "fb_topic_count = collections.Counter(fb_topic_num_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####function\n",
      "sort the topics in the order of number of posts each topic has."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# count each topics\n",
      "def toptopics(topic_count, model_topics):\n",
      "    topic_num_seq = []\n",
      "    post_topic_seq = []\n",
      "    freq_seq = []\n",
      "    common = topic_count.most_common()\n",
      "    for i in xrange(len(common)):\n",
      "        row = common[i]\n",
      "        # no topic\n",
      "        if( row[0] < 0 ):\n",
      "            continue\n",
      "        topic_num_seq.append(row[0])\n",
      "        post_topic_seq.append(model_topics[row[0]])\n",
      "        freq_seq.append(row[1])\n",
      "    return topic_num_seq, post_topic_seq, freq_seq"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tw_topic_num_seq, tw_post_topic_seq, tw_freq_seq  = toptopics(tw_topic_count, tw_model_topics)\n",
      "fb_topic_num_seq, fb_post_topic_seq, fb_freq_seq  = toptopics(fb_topic_count, fb_model_topics)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# generate top topics as pd data frame\n",
      "outcsvdir = 'csv/'\n",
      "tw_top_topics = pd.DataFrame({'topic_num': tw_topic_num_seq,\n",
      "                              'topic': tw_post_topic_seq,\n",
      "                              'frequency': tw_freq_seq},\n",
      "                              columns = ['topic_num', 'topic', 'frequency'])\n",
      "fb_top_topics = pd.DataFrame({'topic_num': fb_topic_num_seq,\n",
      "                              'topic': fb_post_topic_seq,\n",
      "                              'frequency': fb_freq_seq},\n",
      "                              columns = ['topic_num', 'topic', 'frequency'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save top topics\n",
      "outcsvdir = 'csv/'\n",
      "tw_top_topics.to_csv( outcsvdir + 'tw_top_topics.csv', index=False)\n",
      "fb_top_topics.to_csv( outcsvdir + 'fb_top_topics.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load top topics\n",
      "outcsvdir = 'csv/'\n",
      "tw_top_topics = pd.read_csv( outcsvdir + 'tw_top_topics.csv' )\n",
      "fb_top_topics = pd.read_csv( outcsvdir + 'fb_top_topics.csv' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Merge back to original tables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outtable = 'topic/'\n",
      "for i in xrange(len(postFiles)):\n",
      "    #load csv file\n",
      "    DF = pd.read_csv( postFiles[i], index_col=0 )\n",
      "    print i, \"file %s, size %d\" % (filenames[i], len(DF))\n",
      "    tw_merged = DF.merge(tw_df, on='tw_expandedUrl', how=\"left\")\n",
      "    tw_merged = tw_merged.merge(tw_topic, on='topic_num', how=\"left\")\n",
      "    tw_merged.rename(columns={'topic_num':'tw_topic_num', 'topic': 'tw_topic'}, inplace=True)\n",
      "\n",
      "    fb_merged = tw_merged.merge(fb_df, on='tw_expandedUrl', how=\"left\")\n",
      "    fb_merged = fb_merged.merge(fb_topic, on='topic_num', how=\"left\")\n",
      "    fb_merged.rename(columns={'topic_num':'fb_topic_num', 'topic': 'fb_topic'}, inplace=True)\n",
      "\n",
      "    fb_merged.to_csv( outtable + filenames[i] + '_merged.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 file abcnews, size 717\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file afp, size 101\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file ap, size 126\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file bbc, size 602\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file bostonglobe, size 1106\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file cbsnews, size 1279\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file cnn, size 1157\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file dailymail, size 1585\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file foxnews, size 407\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file huffingtonpost, size 1537\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file latimes, size 737\n",
        "11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file msnbc, size 1272\n",
        "12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file nbcnews, size 1759\n",
        "13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file newsweek, size 472\n",
        "14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file npr, size 368\n",
        "15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file nytimes, size 1600\n",
        "16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file reuters, size 381\n",
        "17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file slate, size 741\n",
        "18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file thedailybeast, size 1802\n",
        "19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file theguardian, size 1192\n",
        "20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file time, size 2209\n",
        "21"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file usatoday, size 444\n",
        "22"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file washingtonpost, size 908\n",
        "23"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file wsj, size 477\n",
        "24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file yahoonews, size 654\n"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Group Organizations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We analyze the different kind of topics that the different groups of news organizations prefer to use"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####function\n",
      "Input a list of news organiztions, output the data frame of top topics of the combined posts of the input news organizations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# output top topics for certain news organizations\n",
      "def toptopicsbyorg(orgs):\n",
      "    org_twtopics = []\n",
      "    org_fbtopics = []\n",
      "    for org in orgs:\n",
      "        fileidx = 0\n",
      "        for fileidx in xrange(len(filenames)):\n",
      "            if( filenames[fileidx] == org ):\n",
      "                break\n",
      "        #load file as csv\n",
      "        DF = pd.read_csv( postFiles[fileidx], index_col=0 )\n",
      "        print fileidx, \"file %s, size %d\" % (filenames[fileidx], len(DF))\n",
      "        tw_merged = DF.merge(tw_df, on='tw_expandedUrl', how=\"inner\")\n",
      "        tw_merged.rename(columns={'topic_num':'tw_topic_num'}, inplace=True)\n",
      "\n",
      "        fb_merged = tw_merged.merge(fb_df, on='tw_expandedUrl', how=\"inner\")\n",
      "        fb_merged.rename(columns={'topic_num':'fb_topic_num'}, inplace=True)\n",
      "\n",
      "        org_twtopics.extend(fb_merged.tw_topic_num)\n",
      "        org_fbtopics.extend(fb_merged.fb_topic_num)\n",
      "        \n",
      "    tw_topic_count = collections.Counter(org_twtopics)\n",
      "    fb_topic_count = collections.Counter(org_fbtopics)\n",
      "    tw_topic_num_seq, tw_post_topic_seq, tw_freq_seq  = toptopics(tw_topic_count, tw_topic['topic'])\n",
      "    fb_topic_num_seq, fb_post_topic_seq, fb_freq_seq  = toptopics(fb_topic_count, fb_topic['topic'])\n",
      "\n",
      "    tw_res = pd.DataFrame({'rank': range(1,len(tw_topic_num_seq)+1),\n",
      "                           'topic_num': tw_topic_num_seq,\n",
      "                           'topic': tw_post_topic_seq,\n",
      "                           'frequency': tw_freq_seq},\n",
      "                           columns = ['rank', 'topic_num', 'topic', 'frequency'])\n",
      "    fb_res = pd.DataFrame({'rank': range(1,len(fb_topic_num_seq)+1),\n",
      "                           'topic_num': fb_topic_num_seq,\n",
      "                           'topic': fb_post_topic_seq,\n",
      "                           'frequency': fb_freq_seq},\n",
      "                           columns = ['rank', 'topic_num', 'topic', 'frequency'])\n",
      "    return tw_res, fb_res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "orglists = [['huffingtonpost', 'foxnews', 'usatoday'],\n",
      "            ['npr', 'bostonglobe', 'nytimes', 'dailymail', 'cbsnews', 'cnn', 'time',\n",
      "             'washingtonpost', 'reuters', 'nbcnews', 'newsweek', 'ap'],\n",
      "            ['abcnews', 'slate', 'thedailybeast'],\n",
      "            ['wsj', 'bbc', 'latimes', 'yahoonews', 'msnbc']]\n",
      "dflists_tw = []\n",
      "dflists_fb = []\n",
      "for i in xrange(len(orglists)):\n",
      "    twtempdf, fbtempdf = toptopicsbyorg(orglists[i])\n",
      "    dflists_tw.append(twtempdf)\n",
      "    dflists_fb.append(fbtempdf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "9 file huffingtonpost, size 1537\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file foxnews, size 407\n",
        "21 file usatoday, size 444\n",
        "14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file npr, size 368\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file bostonglobe, size 1106\n",
        "15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file nytimes, size 1600\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file dailymail, size 1585\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file cbsnews, size 1279\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file cnn, size 1157\n",
        "20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file time, size 2209\n",
        "22"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file washingtonpost, size 908\n",
        "16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file reuters, size 381\n",
        "12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file nbcnews, size 1759\n",
        "13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file newsweek, size 472\n",
        "2 file ap, size 126\n",
        "0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file abcnews, size 717\n",
        "17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file slate, size 741\n",
        "18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file thedailybeast, size 1802\n",
        "23"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file wsj, size 477\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file bbc, size 602\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file latimes, size 737\n",
        "24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file yahoonews, size 654\n",
        "11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file msnbc, size 1272\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####function\n",
      "This function is for sanity check. Output all the posts of one specifiec topic."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# output the posts of one certain topic\n",
      "outtable = 'topic/'\n",
      "\n",
      "def fetchposts(topic_num):\n",
      "    topicFiles = []\n",
      "    topicFiles += [(outtable + p) for p in os.listdir(outtable) if p[-4:] == '.csv']\n",
      "    \n",
      "    tw_posts = []\n",
      "    fb_posts = []\n",
      "    for i in xrange(len(topicFiles)):\n",
      "        #load file as csv\n",
      "        DF = pd.read_csv( topicFiles[i], index_col=0 )\n",
      "        DF = DF[['tw_text', 'fb_text', 'tw_topic_num', 'fb_topic_num']]\n",
      "        #print DF.head()\n",
      "        df0 = DF[['tw_text']][DF['tw_topic_num'] == topic_num]\n",
      "        tw_posts.extend(df0['tw_text'].tolist())\n",
      "        #print df0.head()\n",
      "        \n",
      "        df1 = DF[['fb_text']][DF['fb_topic_num'] == topic_num]\n",
      "        fb_posts.extend(df1['fb_text'].tolist())\n",
      "        #print df1.head()\n",
      "        \n",
      "    return tw_posts, fb_posts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# output the posts of a certain topic for sanity check\n",
      "twposts, fbposts = fetchposts(2)\n",
      "twposts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# output the ranking of topics among each organization group. Show the different importance of the same topics in different groups\n",
      "tw_orgtopictrank = []\n",
      "fb_orgtopictrank = []\n",
      "for i in xrange(len(dflists_tw)):\n",
      "    tw_orgtopictrank.append(tw_top_topics.merge(dflists_tw[i][['topic_num', 'rank']], on='topic_num', how=\"left\")[['topic_num', 'topic', 'rank']])\n",
      "for i in xrange(len(dflists_fb)):\n",
      "    fb_orgtopictrank.append(fb_top_topics.merge(dflists_fb[i][['topic_num', 'rank']], on='topic_num', how=\"left\")[['topic_num', 'topic', 'rank']])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save the ranking of topics among each organization group. Show the different importance of the same topics in different groups\n",
      "outcsvdir = 'csv/'\n",
      "for i in xrange(len(dflists_tw)):\n",
      "    tw_orgtopictrank[i].to_csv( outcsvdir + 'org' + str(i+1) + '_of_tw_top_topics.csv', index=False)\n",
      "for i in xrange(len(dflists_fb)):\n",
      "    fb_orgtopictrank[i].to_csv( outcsvdir + 'org' + str(i+1) + '_of_fb_top_topics.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load the ranking of topics among each organization group. Show the different importance of the same topics in different groups\n",
      "# output the ranking of topics among each organization group. Show the different importance of the same topics in different groups\n",
      "tw_orgtopictrank = []\n",
      "fb_orgtopictrank = []\n",
      "for i in xrange(len(dflists_tw)):\n",
      "    tw_orgtopictrank.append(pd.read_csv( outcsvdir + 'org' + str(i+1) + '_of_tw_top_topics.csv'))\n",
      "for i in xrange(len(dflists_fb)):\n",
      "    fb_orgtopictrank.append(pd.read_csv( outcsvdir + 'org' + str(i+1) + '_of_tw_top_topics.csv'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 88
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "The table of topics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tw_top_topics\n",
      "fb_top_topics\n",
      "tw_orgtopictrank = []\n",
      "fb_orgtopictrank = []"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}