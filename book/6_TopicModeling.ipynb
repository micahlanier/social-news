{
 "metadata": {
  "name": "",
  "signature": "sha256:c24127ee593e771a63ff4014b7cb1c5230a870f2189736895fb4eb9796ba3f11"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Topic Modeling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This section is the codes we used to generate the topic for each post of twitter and facebook.  We used the libraries nltk and gensim to build our model. In general, there are two steps of topic generation. The first step is to lemmatize each post and build dictionary. The second step is to build the topic model and generate topic for each post."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Installation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to perform topic modeling. We need to install the libraries nltk and gensim.\n",
      "\n",
      "`pip install gensim\n",
      "\n",
      "`pip install nltk"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import\n",
      "%matplotlib inline\n",
      "\n",
      "import string\n",
      "import nltk\n",
      "import itertools\n",
      "import gensim\n",
      "from gensim import corpora, models, similarities\n",
      "import collections\n",
      "from collections import defaultdict\n",
      "import sys\n",
      "import os\n",
      "import logging\n",
      "import random\n",
      "import operator\n",
      "import csv\n",
      "import time\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "# below for saving/loading files\n",
      "import pickle\n",
      "# below for search/replace of escape characters\n",
      "import re\n",
      "# below for bigrams\n",
      "from nltk.collocations import BigramCollocationFinder\n",
      "from nltk.metrics import BigramAssocMeasures\n",
      "from nltk import bigrams\n",
      "# below for part of speech tagging\n",
      "# uses 'taggers/maxent_treebank_pos_tagger/english.pickle'\n",
      "from nltk.corpus import wordnet\n",
      "# below for tokenizing\n",
      "from nltk.tokenize import word_tokenize, sent_tokenize\n",
      "# below for lemmatizer\n",
      "from nltk.stem import WordNetLemmatizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Lemmatization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pre-process the texts of posts to ntlk format. Generate the lemmatized post as input to train topic model."
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Load the cleaned data from the table"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fianltabledir = 'D:/Quezacot/Dropbox/Social News/data/8. finalTable/'\n",
      "# Get all files to traverse. Assume that we can get everything in consolidated folder.\n",
      "postFiles = []\n",
      "postFiles += [(fianltabledir + p) for p in os.listdir(fianltabledir) if p[-4:] == '.csv']\n",
      "filenames = []\n",
      "filenames += [re.search('(.+?)_finaltable.csv', p).group(1) for p in os.listdir(fianltabledir) if p[-4:] == '.csv']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####clean data for training topic model\n",
      "We eliminate the stop words defined by nltk and only preserve nouns."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tag_set = ['NN','NNS']\n",
      "\n",
      "# our stopsets\n",
      "custom_stopset = []\n",
      "\n",
      "stopset_unigram = set(nltk.corpus.stopwords.words('english'))\n",
      "stopset_unigram = stopset_unigram.union(set(custom_stopset))\n",
      "stopset_bigrams = []"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####function\n",
      "generate lemmatized text from original post."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rmwords(urllist, postlist):\n",
      "    # remove punctuation, remove stopwords, remove words < 3, remove non alpha words, spell correct, lemmatize\n",
      "    # you need to comment out the below decorator if you don't want to run parallel\n",
      "\n",
      "    # we need to strip escape characters so we compile a pattern\n",
      "    hexchars = re.compile('\\\\\\\\x(\\w{2})')\n",
      "    # http://\n",
      "    regex_urls = re.compile('https?:\\/\\/\\S+(\\s|$)')\n",
      "    # RT\n",
      "    regex_rt = re.compile('RT @\\w+: (.@\\w+ )?')\n",
      "    # @ #\n",
      "    regex_at = re.compile('(@|#)\\S+(\\s|$)')\n",
      "\n",
      "    # Instantiate a WordNet Lemmatizer\n",
      "    wnl = WordNetLemmatizer()\n",
      "\n",
      "    # convert penn_tag to morphy_tag used by WordNet Lemmatizer\n",
      "    # http://stackoverflow.com/questions/5364493/lemmatizing-pos-tagged-words-with-nltk\n",
      "    # morphy_tag = {'NN':wordnet.NOUN,'JJ':wordnet.ADJ,'VB':wordnet.VERB,'RB':wordnet.ADV}\n",
      "    morphy_tag = {'NN':wordnet.NOUN}\n",
      "\n",
      "    urls = []\n",
      "    texts = []\n",
      "    texts_pos = []\n",
      "\n",
      "    for i in xrange(len(postlist)):\n",
      "        review_text = postlist[i]\n",
      "        \n",
      "        # some text not recognized as string\n",
      "        if (type(review_text) != 'str'):\n",
      "            review_text = str(review_text)\n",
      "        \n",
      "        # deal with escape characters\n",
      "        review_text = re.sub(hexchars, '', review_text.encode(\"string-escape\"))\n",
      "        # remove http:// re.sub(pattern, repl, string, count=0, flags=0)\n",
      "        review_text = re.sub(regex_urls, '', review_text)\n",
      "        # remove RT @...:\n",
      "        review_text = re.sub(regex_rt, '', review_text)\n",
      "        # remove @ #\n",
      "        review_text = re.sub(regex_at, '', review_text)\n",
      "\n",
      "        # tokenize\n",
      "        review_text = [word for sent in sent_tokenize(review_text) for word in word_tokenize(sent)]\n",
      "\n",
      "        # get parts of speech\n",
      "        review_text_pos = [word for word in nltk.pos_tag(review_text)]\n",
      "        texts_pos.append(review_text_pos)\n",
      "\n",
      "        review_text = []\n",
      "        for word in review_text_pos:\n",
      "            # Only preserve (ADJ, ADV, NOUN, and VERB)\n",
      "            if (not word[1][:2] in morphy_tag):\n",
      "                continue\n",
      "            # remove smallwords\n",
      "            if (len(word[0]) < 3):\n",
      "                continue\n",
      "            # Only consider alpha words\n",
      "            if (not word[0].isalpha()):\n",
      "                continue\n",
      "            # lowercase all words except abbreviations\n",
      "            if (word[0][0].isupper() and word[0][1:].islower()):\n",
      "                word = (word[0].lower(),word[1])\n",
      "            # remove stopwords\n",
      "            if (word[0] in stopset_unigram):\n",
      "                continue\n",
      "            # Lemmatize using the Wordnet Lemmatizer\n",
      "            word = wnl.lemmatize(word[0],morphy_tag[word[1][:2]])    \n",
      "\n",
      "            review_text.append(word)\n",
      "\n",
      "        # add to our list\n",
      "        texts.append(review_text)\n",
      "        urls.append(urllist[i])\n",
      "        \n",
      "    return urls, texts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####function\n",
      "save the lemmatized lists of posts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "savedir = 'objs/'\n",
      "def savelist(filename, listname, namesuffix):\n",
      "    fullpath = savedir + filename + namesuffix\n",
      "    with open(fullpath, 'w') as f:\n",
      "        pickle.dump(listname, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#load all the tables and lemmatize the posts. Store all of the lemmatized posts in two lists. (twitter and facebook)\n",
      "tw_urls = []\n",
      "tw_all_text = []\n",
      "fb_urls = []\n",
      "fb_all_text = []\n",
      "\n",
      "for i in xrange(len(postFiles)):\n",
      "    #load file as csv\n",
      "    DF = pd.read_csv( postFiles[i], index_col=0 )\n",
      "    print i, \"file %s, size %d\" % (filenames[i], len(DF))\n",
      "    \n",
      "    #build tw nltk tag\n",
      "    urls, nltxt = rmwords(DF['tw_expandedUrl'].tolist(), DF['tw_text'].tolist())\n",
      "    tw_urls.extend(urls)\n",
      "    tw_all_text.extend(nltxt)\n",
      "\n",
      "    #build fb nltk tag\n",
      "    urls, nltxt = rmwords(DF['tw_expandedUrl'].tolist(), DF['fb_text'].tolist())\n",
      "    fb_urls.extend(urls)\n",
      "    fb_all_text.extend(nltxt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 file abcnews, size 717\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file afp, size 101\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file ap, size 126\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file bbc, size 602\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file bostonglobe, size 1106\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file cbsnews, size 1279\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file cnn, size 1157\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file dailymail, size 1585\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file foxnews, size 407\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file huffingtonpost, size 1537\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file latimes, size 737\n",
        "11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file msnbc, size 1272\n",
        "12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file nbcnews, size 1759\n",
        "13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file newsweek, size 472\n",
        "14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file npr, size 368\n",
        "15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file nytimes, size 1600\n",
        "16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file reuters, size 381\n",
        "17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file slate, size 741\n",
        "18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file thedailybeast, size 1802\n",
        "19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file theguardian, size 1192\n",
        "20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file time, size 2209\n",
        "21"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file usatoday, size 444\n",
        "22"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file washingtonpost, size 908\n",
        "23"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file wsj, size 477\n",
        "24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file yahoonews, size 654\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save lists\n",
      "savelist('tw_urls', tw_urls, '.pickle')\n",
      "savelist('tw_all_text', tw_all_text, '.pickle')\n",
      "savelist('fb_urls', fb_urls, '.pickle')\n",
      "savelist('fb_all_text', fb_all_text, '.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####function\n",
      "load the lemmatized lists of posts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "loaddir = 'objs/'\n",
      "def loadlist(filename, namesuffix):\n",
      "    fullpath = loaddir + filename + namesuffix\n",
      "    print \"Load file %s\" % (filename + namesuffix)\n",
      "    with open(fullpath) as f:\n",
      "        load_list = pickle.load(f)\n",
      "    return load_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load lists\n",
      "namesuffix = '.pickle'\n",
      "tw_urls = loadlist('tw_urls', namesuffix)\n",
      "tw_all_text = loadlist('tw_all_text', namesuffix)\n",
      "fb_urls = loadlist('fb_urls', namesuffix)\n",
      "fb_all_text = loadlist('fb_all_text', namesuffix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Load file tw_urls.pickle\n",
        "Load file tw_all_text.pickle"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Load file fb_urls.pickle"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Load file fb_all_text.pickle"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Model Building"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate topic model from the lemmatized posts. We build the LDA model based on the dictionary of the posts."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Number of topics to use in LDA model.\n",
      "\n",
      "This is an important parameter of model training. For fewer topics we can have more posts of each topic, but the topics with the same topic may not be much related. For more topics the opposite would be true. The posts with the same topic would be more related but each topic may have fewer posts. Since we want to analyze the difference of topics from twitter and facebook or different news organizations, we choose a fairly large number of topics to ensure the posts classified in the same topic are convincingly similar."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_topics = 500"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def LDA(all_text):\n",
      "    dictionary = corpora.Dictionary(all_text)\n",
      "    \n",
      "    # filter dictionary\n",
      "    # 1. less than no_below documents (absolute number) or\n",
      "    # 2. more than no_above documents (fraction of total corpus size, not absolute number).\n",
      "    # after (1) and (2), keep only the first keep_n most frequent tokens (or keep all if None).\n",
      "    # After the pruning, shrink resulting gaps in word ids. (this means word ids may change after gap shrinking)\n",
      "    dictionary.filter_extremes(no_below=50, no_above=0.15, keep_n=None)\n",
      "    \n",
      "    corpus_bow = [dictionary.doc2bow(post) for post in all_text]\n",
      "    model_tfidf = models.TfidfModel(corpus_bow)\n",
      "    corpus_tfidf = model_tfidf[corpus_bow]\n",
      "    \n",
      "    # choose one\n",
      "    #tw_corpus = tw_corpus_tfidf\n",
      "    corpus = corpus_bow\n",
      "    \n",
      "    model = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=num_topics, chunksize=500, \n",
      "                                     passes=10, update_every=0, alpha=None, eta=None, decay=0.5,\n",
      "                                     distributed=False)\n",
      "    \n",
      "    # generate the list of topics\n",
      "    model_topics = model.print_topics(num_topics)\n",
      "    \n",
      "    return model, corpus, model_topics, dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "#model of twitter posts\n",
      "tw_model, tw_corpus, tw_model_topics, tw_dict = LDA(tw_all_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 0 ns\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "#model of facebook posts\n",
      "fb_model, fb_corpus, fb_model_topics, fb_dict = LDA(fb_all_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 0 ns\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outcsvdir = 'csv/'\n",
      "# generate topics table as pd dara frame\n",
      "tw_topic = pd.DataFrame({'topic_num': range(len(tw_model_topics)), 'topic': tw_model_topics},\n",
      "                        columns = ['topic_num', 'topic'])\n",
      "fb_topic = pd.DataFrame({'topic_num': range(len(fb_model_topics)), 'topic': fb_model_topics},\n",
      "                        columns = ['topic_num', 'topic'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save topics table\n",
      "outcsvdir = 'csv/'\n",
      "tw_topic.to_csv(outcsvdir + 'tw_topics_table.csv', index=False)\n",
      "fb_topic.to_csv(outcsvdir + 'fb_topics_table.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#load topic table\n",
      "incsvdir = 'csv/'\n",
      "tw_topic = pd.read_csv( incsvdir + 'tw_topics_table.csv')\n",
      "fb_topic = pd.read_csv( incsvdir + 'fb_topics_table.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save dictionary as csv\n",
      "pd.DataFrame({'words in dictionary': tw_dict.values()}).to_csv(outcsvdir + 'tw_dict.csv', index=False)\n",
      "pd.DataFrame({'words in dictionary': fb_dict.values()}).to_csv(outcsvdir + 'fb_dict.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Tuning dictionary\n",
      "\n",
      "This part code is for tuning the parameters of dictionary filter. The result is these parameters do not have a large impact. It is probably because the posts from twitter and facebook are relatively very short compared to general articles. Thus, the word frequency appeared on all posts may not be distinguishable for filtering."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictdir = 'dict/'\n",
      "tw_dictionary = corpora.Dictionary(tw_all_text)\n",
      "fb_dictionary = corpora.Dictionary(fb_all_text)\n",
      "steps = np.linspace(0.1, 0.01, 10)\n",
      "tw_df_dict = pd.DataFrame()\n",
      "fb_df_dict = pd.DataFrame()\n",
      "for k in steps:\n",
      "    tw_dictionary.filter_extremes(no_below=50, no_above=k, keep_n=None)\n",
      "    tw_l = tw_dictionary.values()\n",
      "    for n in range(len(tw_l), len(tw_df_dict)):\n",
      "        tw_l.append('')\n",
      "    tw_df_dict[str(k)] = tw_l\n",
      "    \n",
      "    fb_dictionary.filter_extremes(no_below=50, no_above=k, keep_n=None)\n",
      "    fb_l = fb_dictionary.values()\n",
      "    for n in range(len(fb_l), len(fb_df_dict)):\n",
      "        fb_l.append('')\n",
      "    fb_df_dict[str(k)] = fb_l\n",
      "tw_df_dict.to_csv(dictdir + 'tw_dict.csv', index=False)\n",
      "fb_df_dict.to_csv(dictdir + 'fb_dict.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####function\n",
      "put a post into out topic model and obtain the topic for this post. Some posts may be classified to more than one topic and given different probabilities for the topics. We only choose the one with the highest probability."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# obtain the most probable topic of one post\n",
      "def fetchtopicnum(model, onecorpus):\n",
      "    # get the topics from the model\n",
      "    if( len(onecorpus) == 0 ):\n",
      "        return -1, 'NA'\n",
      "    \n",
      "    topics = model[onecorpus]\n",
      "    topic_num = topics[0][0]\n",
      "    topic_prob = topics[0][1]\n",
      "    for topic in topics:\n",
      "        temp_topic = topic[0]\n",
      "        temp_prob = topic[1]\n",
      "        # we only take the one with highest probability\n",
      "        if( temp_prob > topic_prob ):\n",
      "            topic_num = temp_topic\n",
      "            topic_prob = temp_prob\n",
      "    return topic_num, topic_prob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####fucntion\n",
      "put a list of posts into topic model and obtain the list of topics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# full topic list of all posts\n",
      "def poststopics(model, corpus, model_topics):\n",
      "    topic_num_list = []\n",
      "    topic_prob_list = []\n",
      "    post_topic_list = []\n",
      "    for i in xrange(len(corpus)):\n",
      "        topic_num, topic_prob = fetchtopicnum(model, corpus[i])\n",
      "        topic_num_list.append(topic_num)\n",
      "        topic_prob_list.append(topic_prob_list)\n",
      "        post_topic_list.append(model_topics[topic_num])\n",
      "    return topic_num_list, topic_prob_list, post_topic_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "#obtain the topic of each twitter post\n",
      "tw_topic_num_list, tw_topic_prob_list, tw_post_topic_list = poststopics(tw_model, tw_corpus, tw_model_topics)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 11.1 s\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "#obtain the topic of each facebook post\n",
      "fb_topic_num_list, fb_topic_prob_list, fb_post_topic_list = poststopics(fb_model, fb_corpus, fb_model_topics)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Wall time: 13.3 s\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# generate topics of posts index by urls as pd data frame\n",
      "outcsvdir = 'csv/'\n",
      "tw_df = pd.DataFrame({'tw_expandedUrl': tw_urls,\n",
      "                      'topic_num': tw_topic_num_list},\n",
      "                     columns = ['tw_expandedUrl', 'topic_num'])\n",
      "fb_df = pd.DataFrame({'tw_expandedUrl': fb_urls,\n",
      "              'topic_num': fb_topic_num_list},\n",
      "              columns = ['tw_expandedUrl', 'topic_num'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save topics of posts\n",
      "outcsvdir = 'csv/'\n",
      "tw_df.to_csv( outcsvdir + 'tw_topic_nums.csv', index=False)\n",
      "fb_df.to_csv( outcsvdir + 'fb_topic_nums.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load topics of posts\n",
      "incsvdir = 'csv/'\n",
      "tw_df = pd.read_csv( incsvdir + 'tw_topic_nums.csv' )\n",
      "fb_df = pd.read_csv( incsvdir + 'fb_topic_nums.csv' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# count the number of posts of each topic\n",
      "tw_topic_count = collections.Counter(tw_topic_num_list)\n",
      "fb_topic_count = collections.Counter(fb_topic_num_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####function\n",
      "sort the topics in the order of number of posts each topic has."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# count each topics\n",
      "def toptopics(topic_count, model_topics):\n",
      "    topic_num_seq = []\n",
      "    post_topic_seq = []\n",
      "    freq_seq = []\n",
      "    common = topic_count.most_common()\n",
      "    for i in xrange(len(common)):\n",
      "        row = common[i]\n",
      "        # no topic\n",
      "        if( row[0] < 0 ):\n",
      "            continue\n",
      "        topic_num_seq.append(row[0])\n",
      "        post_topic_seq.append(model_topics[row[0]])\n",
      "        freq_seq.append(row[1])\n",
      "    return topic_num_seq, post_topic_seq, freq_seq"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tw_topic_num_seq, tw_post_topic_seq, tw_freq_seq  = toptopics(tw_topic_count, tw_model_topics)\n",
      "fb_topic_num_seq, fb_post_topic_seq, fb_freq_seq  = toptopics(fb_topic_count, fb_model_topics)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# generate top topics as pd data frame\n",
      "outcsvdir = 'csv/'\n",
      "tw_top_topics = pd.DataFrame({'topic_num': tw_topic_num_seq,\n",
      "                              'topic': tw_post_topic_seq,\n",
      "                              'frequency': tw_freq_seq},\n",
      "                              columns = ['topic_num', 'topic', 'frequency'])\n",
      "fb_top_topics = pd.DataFrame({'topic_num': fb_topic_num_seq,\n",
      "                              'topic': fb_post_topic_seq,\n",
      "                              'frequency': fb_freq_seq},\n",
      "                              columns = ['topic_num', 'topic', 'frequency'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save top topics\n",
      "outcsvdir = 'csv/'\n",
      "tw_top_topics.to_csv( outcsvdir + 'tw_top_topics.csv', index=False)\n",
      "fb_top_topics.to_csv( outcsvdir + 'fb_top_topics.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load top topics\n",
      "outcsvdir = 'csv/'\n",
      "tw_top_topics = pd.read_csv( outcsvdir + 'tw_top_topics.csv' )\n",
      "fb_top_topics = pd.read_csv( outcsvdir + 'fb_top_topics.csv' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Merge back to original tables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outtable = 'topic/'\n",
      "for i in xrange(len(postFiles)):\n",
      "    #load csv file\n",
      "    DF = pd.read_csv( postFiles[i], index_col=0 )\n",
      "    print i, \"file %s, size %d\" % (filenames[i], len(DF))\n",
      "    tw_merged = DF.merge(tw_df, on='tw_expandedUrl', how=\"left\")\n",
      "    tw_merged = tw_merged.merge(tw_topic, on='topic_num', how=\"left\")\n",
      "    tw_merged.rename(columns={'topic_num':'tw_topic_num', 'topic': 'tw_topic'}, inplace=True)\n",
      "\n",
      "    fb_merged = tw_merged.merge(fb_df, on='tw_expandedUrl', how=\"left\")\n",
      "    fb_merged = fb_merged.merge(fb_topic, on='topic_num', how=\"left\")\n",
      "    fb_merged.rename(columns={'topic_num':'fb_topic_num', 'topic': 'fb_topic'}, inplace=True)\n",
      "\n",
      "    fb_merged.to_csv( outtable + filenames[i] + '_merged.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 file abcnews, size 717\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file afp, size 101\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file ap, size 126\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file bbc, size 602\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file bostonglobe, size 1106\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file cbsnews, size 1279\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file cnn, size 1157\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file dailymail, size 1585\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file foxnews, size 407\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file huffingtonpost, size 1537\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file latimes, size 737\n",
        "11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file msnbc, size 1272\n",
        "12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file nbcnews, size 1759\n",
        "13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file newsweek, size 472\n",
        "14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file npr, size 368\n",
        "15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file nytimes, size 1600\n",
        "16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file reuters, size 381\n",
        "17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file slate, size 741\n",
        "18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file thedailybeast, size 1802\n",
        "19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file theguardian, size 1192\n",
        "20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file time, size 2209\n",
        "21"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file usatoday, size 444\n",
        "22"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file washingtonpost, size 908\n",
        "23"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file wsj, size 477\n",
        "24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file yahoonews, size 654\n"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Group Organizations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We analyze the different kind of topics that the different groups of news organizations prefer to use"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####function\n",
      "Input a list of news organiztions, output the data frame of top topics of the combined posts of the input news organizations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# output top topics for certain news organizations\n",
      "def toptopicsbyorg(orgs):\n",
      "    org_twtopics = []\n",
      "    org_fbtopics = []\n",
      "    for org in orgs:\n",
      "        fileidx = 0\n",
      "        for fileidx in xrange(len(filenames)):\n",
      "            if( filenames[fileidx] == org ):\n",
      "                break\n",
      "        #load file as csv\n",
      "        DF = pd.read_csv( postFiles[fileidx], index_col=0 )\n",
      "        print fileidx, \"file %s, size %d\" % (filenames[fileidx], len(DF))\n",
      "        tw_merged = DF.merge(tw_df, on='tw_expandedUrl', how=\"inner\")\n",
      "        tw_merged.rename(columns={'topic_num':'tw_topic_num'}, inplace=True)\n",
      "\n",
      "        fb_merged = tw_merged.merge(fb_df, on='tw_expandedUrl', how=\"inner\")\n",
      "        fb_merged.rename(columns={'topic_num':'fb_topic_num'}, inplace=True)\n",
      "\n",
      "        org_twtopics.extend(fb_merged.tw_topic_num)\n",
      "        org_fbtopics.extend(fb_merged.fb_topic_num)\n",
      "        \n",
      "    tw_topic_count = collections.Counter(org_twtopics)\n",
      "    fb_topic_count = collections.Counter(org_fbtopics)\n",
      "    tw_topic_num_seq, tw_post_topic_seq, tw_freq_seq  = toptopics(tw_topic_count, tw_topic['topic'])\n",
      "    fb_topic_num_seq, fb_post_topic_seq, fb_freq_seq  = toptopics(fb_topic_count, fb_topic['topic'])\n",
      "\n",
      "    tw_res = pd.DataFrame({'rank': range(1,len(tw_topic_num_seq)+1),\n",
      "                           'topic_num': tw_topic_num_seq,\n",
      "                           'topic': tw_post_topic_seq,\n",
      "                           'frequency': tw_freq_seq},\n",
      "                           columns = ['rank', 'topic_num', 'topic', 'frequency'])\n",
      "    fb_res = pd.DataFrame({'rank': range(1,len(fb_topic_num_seq)+1),\n",
      "                           'topic_num': fb_topic_num_seq,\n",
      "                           'topic': fb_post_topic_seq,\n",
      "                           'frequency': fb_freq_seq},\n",
      "                           columns = ['rank', 'topic_num', 'topic', 'frequency'])\n",
      "    return tw_res, fb_res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "orglists = [['huffingtonpost', 'foxnews', 'usatoday'],\n",
      "            ['npr', 'bostonglobe', 'nytimes', 'dailymail', 'cbsnews', 'cnn', 'time',\n",
      "             'washingtonpost', 'reuters', 'nbcnews', 'newsweek', 'ap'],\n",
      "            ['abcnews', 'slate', 'thedailybeast'],\n",
      "            ['wsj', 'bbc', 'latimes', 'yahoonews', 'msnbc']]\n",
      "dflists_tw = []\n",
      "dflists_fb = []\n",
      "for i in xrange(len(orglists)):\n",
      "    twtempdf, fbtempdf = toptopicsbyorg(orglists[i])\n",
      "    dflists_tw.append(twtempdf)\n",
      "    dflists_fb.append(fbtempdf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "9 file huffingtonpost, size 1537\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file foxnews, size 407\n",
        "21 file usatoday, size 444\n",
        "14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file npr, size 368\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file bostonglobe, size 1106\n",
        "15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file nytimes, size 1600\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file dailymail, size 1585\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file cbsnews, size 1279\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file cnn, size 1157\n",
        "20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file time, size 2209\n",
        "22"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file washingtonpost, size 908\n",
        "16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file reuters, size 381\n",
        "12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file nbcnews, size 1759\n",
        "13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file newsweek, size 472\n",
        "2 file ap, size 126\n",
        "0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file abcnews, size 717\n",
        "17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file slate, size 741\n",
        "18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file thedailybeast, size 1802\n",
        "23"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file wsj, size 477\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file bbc, size 602\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file latimes, size 737\n",
        "24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file yahoonews, size 654\n",
        "11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " file msnbc, size 1272\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####function\n",
      "This function is for sanity check. Output all the posts of one specifiec topic."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# output the posts of one certain topic\n",
      "outtable = 'topic/'\n",
      "\n",
      "def fetchposts(topic_num):\n",
      "    topicFiles = []\n",
      "    topicFiles += [(outtable + p) for p in os.listdir(outtable) if p[-4:] == '.csv']\n",
      "    \n",
      "    tw_posts = []\n",
      "    fb_posts = []\n",
      "    for i in xrange(len(topicFiles)):\n",
      "        #load file as csv\n",
      "        DF = pd.read_csv( topicFiles[i], index_col=0 )\n",
      "        DF = DF[['tw_text', 'fb_text', 'tw_topic_num', 'fb_topic_num']]\n",
      "        #print DF.head()\n",
      "        df0 = DF[['tw_text']][DF['tw_topic_num'] == topic_num]\n",
      "        tw_posts.extend(df0['tw_text'].tolist())\n",
      "        #print df0.head()\n",
      "        \n",
      "        df1 = DF[['fb_text']][DF['fb_topic_num'] == topic_num]\n",
      "        fb_posts.extend(df1['fb_text'].tolist())\n",
      "        #print df1.head()\n",
      "        \n",
      "    return tw_posts, fb_posts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# output the posts of a certain topic for sanity check\n",
      "twposts, fbposts = fetchposts(2)\n",
      "twposts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# output the ranking of topics among each organization group. Show the different importance of the same topics in different groups\n",
      "tw_orgtopictrank = []\n",
      "fb_orgtopictrank = []\n",
      "for i in xrange(len(dflists_tw)):\n",
      "    tw_orgtopictrank.append(tw_top_topics.merge(dflists_tw[i][['topic_num', 'rank']], on='topic_num', how=\"left\")[['topic_num', 'topic', 'rank']])\n",
      "for i in xrange(len(dflists_fb)):\n",
      "    fb_orgtopictrank.append(fb_top_topics.merge(dflists_fb[i][['topic_num', 'rank']], on='topic_num', how=\"left\")[['topic_num', 'topic', 'rank']])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save the ranking of topics among each organization group. Show the different importance of the same topics in different groups\n",
      "outcsvdir = 'csv/'\n",
      "for i in xrange(len(dflists_tw)):\n",
      "    tw_orgtopictrank[i].to_csv( outcsvdir + 'org' + str(i+1) + '_of_tw_top_topics.csv', index=False)\n",
      "for i in xrange(len(dflists_fb)):\n",
      "    fb_orgtopictrank[i].to_csv( outcsvdir + 'org' + str(i+1) + '_of_fb_top_topics.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load the ranking of topics among each organization group. Show the different importance of the same topics in different groups\n",
      "# output the ranking of topics among each organization group. Show the different importance of the same topics in different groups\n",
      "tw_orgtopictrank = []\n",
      "fb_orgtopictrank = []\n",
      "for i in xrange(len(dflists_tw)):\n",
      "    tw_orgtopictrank.append(pd.read_csv( outcsvdir + 'org' + str(i+1) + '_of_tw_top_topics.csv'))\n",
      "for i in xrange(len(dflists_fb)):\n",
      "    fb_orgtopictrank.append(pd.read_csv( outcsvdir + 'org' + str(i+1) + '_of_fb_top_topics.csv'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 166
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The table of topics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This part represents the result of our topic modeling. We interpreted it based on one result we ran because the result would not be the same everytime."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tw_top_topics.head(11)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## manually interpreted topics of twitter\n",
      "tw_man_topics = {  79: 'New York',\n",
      "                   15: 'President Obama',\n",
      "                   41: 'ISIS',\n",
      "                  106: 'Woman Issues',\n",
      "                   74: 'Highlights of the Year',\n",
      "                  196: 'Listicles',\n",
      "                  134: 'White House',\n",
      "                  474: 'Ebola',\n",
      "                   50: 'Ebola in Texas',\n",
      "                  293: 'Ebola in Texas',\n",
      "                  283: 'Bill Cosby'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 179
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fb_top_topics.head(12)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## manually interpreted topics of facebook\n",
      "fb_man_topics = { 335: 'New York City',\n",
      "                  332: 'Ebola in US',\n",
      "                  488: 'President Obama\\'s Immigration Policy',\n",
      "                   99: 'Hong Kong protest',\n",
      "                  409: 'Life Issues',\n",
      "                  180: 'Things with Photography',\n",
      "                  314: 'Family Related Topics',\n",
      "                  420: 'Midterm Senate Elections',\n",
      "                   87: 'White House',\n",
      "                  143: 'ISIS',\n",
      "                  212: 'Parents with Teenage Girls'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 180
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We manually deleted the topic number 16 because its words \"boston globe\" were added intentionally by Boston Globe in all its post. They are nothing to do with the topic of posts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mantopictop(top_topics, man_topics):\n",
      "    man_df = pd.DataFrame({'topic_num': man_topics.keys(),\n",
      "                           'man_topic': man_topics.values()},\n",
      "                           columns = ['topic_num', 'man_topic'])\n",
      "    re_df = top_topics.merge(man_df, on='topic_num', how=\"inner\")\n",
      "    re_df['rank'] = range(1,len(re_df)+1)\n",
      "    re_df.set_index('rank')\n",
      "    return re_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 181
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Twitter Top 10 Topics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tw_top10_df = mantopictop(tw_top_topics, tw_man_topics)\n",
      "tw_top10_df[['rank', 'man_topic']].head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>rank</th>\n",
        "      <th>man_topic</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>  1</td>\n",
        "      <td>               New York</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>  2</td>\n",
        "      <td>        President Obama</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>  3</td>\n",
        "      <td>                   ISIS</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>  4</td>\n",
        "      <td>           Woman Issues</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>  5</td>\n",
        "      <td> Highlights of the Year</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td>  6</td>\n",
        "      <td>              Listicles</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td>  7</td>\n",
        "      <td>            White House</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td>  8</td>\n",
        "      <td>                  Ebola</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td>  9</td>\n",
        "      <td>         Ebola in Texas</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td> 10</td>\n",
        "      <td>         Ebola in Texas</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 182,
       "text": [
        "   rank               man_topic\n",
        "0     1                New York\n",
        "1     2         President Obama\n",
        "2     3                    ISIS\n",
        "3     4            Woman Issues\n",
        "4     5  Highlights of the Year\n",
        "5     6               Listicles\n",
        "6     7             White House\n",
        "7     8                   Ebola\n",
        "8     9          Ebola in Texas\n",
        "9    10          Ebola in Texas"
       ]
      }
     ],
     "prompt_number": 182
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Facebook Top 10 Topics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fb_top10_df = mantopictop(fb_top_topics, fb_man_topics)\n",
      "fb_top10_df[['rank', 'man_topic']].head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>rank</th>\n",
        "      <th>man_topic</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>  1</td>\n",
        "      <td>                        New York City</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>  2</td>\n",
        "      <td>                          Ebola in US</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>  3</td>\n",
        "      <td> President Obama's Immigration Policy</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>  4</td>\n",
        "      <td>                    Hong Kong protest</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>  5</td>\n",
        "      <td>                          Life Issues</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td>  6</td>\n",
        "      <td>              Things with Photography</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td>  7</td>\n",
        "      <td>                Family Related Topics</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td>  8</td>\n",
        "      <td>             Midterm Senate Elections</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td>  9</td>\n",
        "      <td>                          White House</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td> 10</td>\n",
        "      <td>                                 ISIS</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 183,
       "text": [
        "   rank                             man_topic\n",
        "0     1                         New York City\n",
        "1     2                           Ebola in US\n",
        "2     3  President Obama's Immigration Policy\n",
        "3     4                     Hong Kong protest\n",
        "4     5                           Life Issues\n",
        "5     6               Things with Photography\n",
        "6     7                 Family Related Topics\n",
        "7     8              Midterm Senate Elections\n",
        "8     9                           White House\n",
        "9    10                                  ISIS"
       ]
      }
     ],
     "prompt_number": 183
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The Rank of Top 10 Topics of Twitter in each group of News Organizations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Image](1. KMeans - socialMetrics.png)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clusterNames = ['Green', 'Blue', 'Teal', 'Red']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 187
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def orgtopicrank(top10_df, orgtopictrank):\n",
      "    for i in xrange(len(orgtopictrank)):\n",
      "        rank_df = orgtopictrank[i].rename(columns={'rank':clusterNames[i]})\n",
      "        top10_df = top10_df.merge(rank_df, on='topic_num', how=\"left\")\n",
      "    return top10_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 188
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fb_orgtopicrank = orgtopicrank(fb_top10_df, fb_orgtopictrank)\n",
      "fb_orgtopicrank.rename(columns={'rank':'rank in all facebook posts'}, inplace=True)\n",
      "fb_orgtopicrank[['rank in all facebook posts', 'man_topic', clusterNames[0], clusterNames[1], clusterNames[2], clusterNames[3]]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>rank in all facebook posts</th>\n",
        "      <th>man_topic</th>\n",
        "      <th>Green</th>\n",
        "      <th>Blue</th>\n",
        "      <th>Teal</th>\n",
        "      <th>Red</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td>  1</td>\n",
        "      <td>                        New York City</td>\n",
        "      <td>  11</td>\n",
        "      <td>  1</td>\n",
        "      <td>  3</td>\n",
        "      <td>  1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td>  2</td>\n",
        "      <td>                          Ebola in US</td>\n",
        "      <td>   3</td>\n",
        "      <td>  2</td>\n",
        "      <td>  8</td>\n",
        "      <td>  6</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td>  3</td>\n",
        "      <td> President Obama's Immigration Policy</td>\n",
        "      <td>  12</td>\n",
        "      <td>  4</td>\n",
        "      <td>  5</td>\n",
        "      <td>  4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td>  4</td>\n",
        "      <td>                    Hong Kong protest</td>\n",
        "      <td> 301</td>\n",
        "      <td>  5</td>\n",
        "      <td> 42</td>\n",
        "      <td>  3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td>  5</td>\n",
        "      <td>                          Life Issues</td>\n",
        "      <td>  17</td>\n",
        "      <td>  7</td>\n",
        "      <td>  6</td>\n",
        "      <td>  7</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td>  6</td>\n",
        "      <td>              Things with Photography</td>\n",
        "      <td>  39</td>\n",
        "      <td> 12</td>\n",
        "      <td> 18</td>\n",
        "      <td>  2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td>  7</td>\n",
        "      <td>                Family Related Topics</td>\n",
        "      <td>  10</td>\n",
        "      <td>  8</td>\n",
        "      <td>  7</td>\n",
        "      <td> 13</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td>  8</td>\n",
        "      <td>             Midterm Senate Elections</td>\n",
        "      <td>  18</td>\n",
        "      <td> 21</td>\n",
        "      <td>  1</td>\n",
        "      <td>  8</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td>  9</td>\n",
        "      <td>                          White House</td>\n",
        "      <td>   7</td>\n",
        "      <td> 11</td>\n",
        "      <td>  4</td>\n",
        "      <td> 16</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td> 10</td>\n",
        "      <td>                                 ISIS</td>\n",
        "      <td> 132</td>\n",
        "      <td> 10</td>\n",
        "      <td>  2</td>\n",
        "      <td> 66</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td> 11</td>\n",
        "      <td>           Parents with Teenage Girls</td>\n",
        "      <td>  25</td>\n",
        "      <td> 20</td>\n",
        "      <td> 12</td>\n",
        "      <td> 14</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 195,
       "text": [
        "    rank in all facebook posts                             man_topic  Green  \\\n",
        "0                            1                         New York City     11   \n",
        "1                            2                           Ebola in US      3   \n",
        "2                            3  President Obama's Immigration Policy     12   \n",
        "3                            4                     Hong Kong protest    301   \n",
        "4                            5                           Life Issues     17   \n",
        "5                            6               Things with Photography     39   \n",
        "6                            7                 Family Related Topics     10   \n",
        "7                            8              Midterm Senate Elections     18   \n",
        "8                            9                           White House      7   \n",
        "9                           10                                  ISIS    132   \n",
        "10                          11            Parents with Teenage Girls     25   \n",
        "\n",
        "    Blue  Teal  Red  \n",
        "0      1     3    1  \n",
        "1      2     8    6  \n",
        "2      4     5    4  \n",
        "3      5    42    3  \n",
        "4      7     6    7  \n",
        "5     12    18    2  \n",
        "6      8     7   13  \n",
        "7     21     1    8  \n",
        "8     11     4   16  \n",
        "9     10     2   66  \n",
        "10    20    12   14  "
       ]
      }
     ],
     "prompt_number": 195
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####The different groups of news organizations have very different preference of topics:\n",
      "The above table shows the top 10 topic in Facebook posts posted by the 25 news organizations. We believe that the topic modeling performed reasonably well in that we see big topics that were heavily posted on during the September - November time frame. Some of the topics that readers might recognize are Ebola, President Obama's Immigration Policy, Hong Kong Protest, Midterm Senate Elections, White House, and ISIS. We are reasonably happy with the topic modeling results and find them to be within our expectation of what news organizations were reporting on.\n",
      "\n",
      "Using the results from the prior clustering analysis with social media metrics, we can also derive where the top 10 topics ranked for each of the groups. For example, from the above table, for the Green Group - New York City is ranked as the 11th most posted topic while for the entire 25 news organizations, it is ranked as the most posted topic.\n",
      "\n",
      "There are several interesting observations we can draw. First, we find the Green group's content on Facebook to be very different from the rest of the three groups. The main differences are on the topics Hong Kong Protests (ranked 301) and ISIS (ranked 132). While these topics are ranked in the top 10 for the entire 25 news organizations, these topics are not heavily reported by Huffington Post, USA Today, and Fox News. This is within our expectation of sensational news organizations compared to more serious journalistic organizations such as New York Times. In addition, the Teal group does not post much on Hong Kong Protests and the Red group does not put an emphasis on ISIS. While, for the Blue group, the ranking within the group is very similar to the overall 25 news organizations. This makes sense, since the Blue group has the majority of news organizations. We should note that Blue group is also comprised of the main traditional news outlets that would report on the more serious topics."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tw_orgtopicrank = orgtopicrank(tw_top10_df, tw_orgtopictrank)\n",
      "tw_orgtopicrank.rename(columns={'rank':'rank in all twitter posts'}, inplace=True)\n",
      "tw_orgtopicrank[['rank in all twitter posts', 'man_topic', clusterNames[0], clusterNames[1], clusterNames[2], clusterNames[3]]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>rank in all twitter posts</th>\n",
        "      <th>man_topic</th>\n",
        "      <th>Green</th>\n",
        "      <th>Blue</th>\n",
        "      <th>Teal</th>\n",
        "      <th>Red</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td>  1</td>\n",
        "      <td>               New York</td>\n",
        "      <td> 11</td>\n",
        "      <td>  1</td>\n",
        "      <td>  2</td>\n",
        "      <td>  3</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td>  2</td>\n",
        "      <td>        President Obama</td>\n",
        "      <td> 50</td>\n",
        "      <td>  3</td>\n",
        "      <td>  4</td>\n",
        "      <td>  1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td>  3</td>\n",
        "      <td>                   ISIS</td>\n",
        "      <td> 14</td>\n",
        "      <td>  2</td>\n",
        "      <td> 95</td>\n",
        "      <td>  2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td>  4</td>\n",
        "      <td>           Woman Issues</td>\n",
        "      <td>  4</td>\n",
        "      <td>  4</td>\n",
        "      <td>  8</td>\n",
        "      <td> 18</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td>  5</td>\n",
        "      <td> Highlights of the Year</td>\n",
        "      <td>  8</td>\n",
        "      <td>  7</td>\n",
        "      <td>  5</td>\n",
        "      <td>  6</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td>  6</td>\n",
        "      <td>              Listicles</td>\n",
        "      <td>  2</td>\n",
        "      <td> 14</td>\n",
        "      <td>  6</td>\n",
        "      <td> 19</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td>  7</td>\n",
        "      <td>            White House</td>\n",
        "      <td>  6</td>\n",
        "      <td> 11</td>\n",
        "      <td>  7</td>\n",
        "      <td>  7</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td>  8</td>\n",
        "      <td>                  Ebola</td>\n",
        "      <td> 48</td>\n",
        "      <td>  5</td>\n",
        "      <td> 31</td>\n",
        "      <td> 30</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td>  9</td>\n",
        "      <td>         Ebola in Texas</td>\n",
        "      <td> 24</td>\n",
        "      <td>  8</td>\n",
        "      <td> 65</td>\n",
        "      <td>  4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td> 10</td>\n",
        "      <td>         Ebola in Texas</td>\n",
        "      <td> 19</td>\n",
        "      <td>  6</td>\n",
        "      <td> 60</td>\n",
        "      <td> 28</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td> 11</td>\n",
        "      <td>             Bill Cosby</td>\n",
        "      <td> 34</td>\n",
        "      <td> 24</td>\n",
        "      <td>  3</td>\n",
        "      <td> 15</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 197,
       "text": [
        "    rank in all twitter posts               man_topic  Green  Blue  Teal  Red\n",
        "0                           1                New York     11     1     2    3\n",
        "1                           2         President Obama     50     3     4    1\n",
        "2                           3                    ISIS     14     2    95    2\n",
        "3                           4            Woman Issues      4     4     8   18\n",
        "4                           5  Highlights of the Year      8     7     5    6\n",
        "5                           6               Listicles      2    14     6   19\n",
        "6                           7             White House      6    11     7    7\n",
        "7                           8                   Ebola     48     5    31   30\n",
        "8                           9          Ebola in Texas     24     8    65    4\n",
        "9                          10          Ebola in Texas     19     6    60   28\n",
        "10                         11              Bill Cosby     34    24     3   15"
       ]
      }
     ],
     "prompt_number": 197
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####The different groups of news organizations have very different preference of topics:\n",
      "The top 10 topics from Twitter are a bit different from the top 10 list from Facebook. Again, New York shows up at the very top of the list. Some similar issues that were discussed heavily on Twitter were President Obama (minus the immigration policy), ISIS, White House, and Ebola. However, there were a couple of topics that were not on Facebook. For example, on Twitter we find Women's Issues, Listicles, and Bill Cosby. Furthermore, we observe due to the 140 character limit, the LDA topic model gave us concise results that were much easier to interpret.\n",
      "\n",
      "From the above table, there are several interesting observations we can draw. First, we find the Teal group's content on twitter to be very different from the rest of the three groups. The main differences are on the topics ISIS (ranked 95), Ebola (31), and Ebola in Texas (ranked 60, 65). While these topics are ranked in the top 10 for the entire 25 news organizations, these topics are not heavily reported on by Slate, ABC News, and The Daily Beast. In addition, the topics reported by the Green group on Twitter is very different from those reported on Facebook, specifically, ISIS has now moved up from 301 to 14. However, overall, most of the top 10 topics for the entire 25 news organizations are still under represented in the Green group."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Limitations**\n",
      "Auto generated topics might not be representative. The topic model is forced to classify the posts as the number of topics we specified, rather than to classify the posts based on their similarity. Some posts with uncommon topic might be classified to the same group as another uncommon topic because their similar posts are too few. Some topics might be too popular so the classifier generates more than one topics for them because the model does not prefer too many posts classified in one topic, such as the topics \u201cEbola,\u201d \u201cEbola in Texas\u201d in twitter."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Conclusion**\n",
      "The topic modeling results reinforce the results we found in the clustering analysis with social media metrics. From the tables above, we see that the top 10 topics for the four groups are all very different. Therefore, it is possible to conclude that there is an association of different content/topics to differences in social metrics (likes, retweets, and etc). This makes intuitive sense as there are topics that are inherently more popular than others. This is what we observe in our data and in our news organizations clusters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}