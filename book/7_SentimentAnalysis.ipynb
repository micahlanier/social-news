{
 "metadata": {
  "name": "",
  "signature": "sha256:90df36746cdb0b5d76728d1cdda84165249ef3abbd5ec5c60e7b76535f1f822c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<a id=\"s7\"></a>Sentiment Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We performed sentiment analysis of our entire tweet and Facebook post corpus, calculating approximate measurements of \"postivity\" and \"negativity\" for each item, and classifying each (as positive, negative, or neutral). This enabled aggregate descriptive analysis of the sentimental differences between organizations, as well as enriching the predictive featureset discussed below.\n",
      "\n",
      "Before making use of sentiments, we need to generate them. Our process for doing so was fairly straightforward. Start by traversing every tweet and Facebook post. For each, divide the post into tokens (words, essentially) and use the [Lesk Algorithm](https://en.wikipedia.org/wiki/Lesk_algorithm) and [WordNet](http://wordnet.princeton.edu) to determine their definitions. Finally, match that definition to [SentiWordNet](http://sentiwordnet.isti.cnr.it) in order to detrmine its positivity and negativity (their sum is between 0 and 1). Finally, the post is classified based on its aggregate word-wise sentiment."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Code in this section made use of the following libraries:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime as dt\n",
      "import dateutil.parser\n",
      "import json\n",
      "import nltk\n",
      "from nltk.corpus import wordnet as wn\n",
      "from nltk.corpus import sentiwordnet as swn\n",
      "import os\n",
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<a id=\"s7.1\"></a>Sentiment Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following code performs sentiment classification; it is a truncated version of the code found at `/src/sentiment/sentiment.py`. As before, it omits much of the configuration and status code present in the full script."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "\n",
      "This script will traverse consolidated Twitter/Facebook data and apply sentiment analysis techniques to all posts therein.\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "##### Configuration\n",
      "\n",
      "# Organizations for which to perform sentiment analysis. Either a list or \"all\".\n",
      "orgs = 'all'\n",
      "\n",
      "##### Main Setup\n",
      "\n",
      "# Social data directories.\n",
      "consolidatedTweetsDirectory  = '../../../data/twitter/consolidated/'\n",
      "consolidatedFbPostsDirectory = '../../../data/facebook/consolidated/'\n",
      "# Sentiment directories.\n",
      "twitterSentimentDirectory  = '../../../data/sentiment/twitter/'\n",
      "fbSentimentDirectory       = '../../../data/sentiment/facebook/'\n",
      "\n",
      "# Organization setup.\n",
      "allOrgs = json.load(open('../../conf/organizations.json'))\n",
      "if type(orgs) == str and orgs == 'all':\n",
      "    orgs = allOrgs.keys()\n",
      "relevantOrgs = dict((k, allOrgs[k]) for k in orgs)\n",
      "\n",
      "##### Functions\n",
      "\n",
      "\"\"\"\n",
      "Remove all URLs and usernames. Hashtags *should* be fine.\n",
      "\"\"\"\n",
      "def cleanMessage(message):\n",
      "    # Remove @mentions.\n",
      "    message = re.sub('@[a-zA-Z0-9]+','',message)\n",
      "    # Remove links.\n",
      "    links = socialUrlUtils.urlsInText(message)\n",
      "    # links.reverse()\n",
      "    for link in links:\n",
      "        message = message.replace(link,'')\n",
      "    # Return.\n",
      "    return message.strip()\n",
      "\n",
      "\"\"\"\n",
      "Map POS from pos_tag to Wordnet.\n",
      "Reimplements logic from:\n",
      "    https://github.com/linkTDP/BigDataAnalysis_TweetSentiment/blob/master/SentiWordnet.py\n",
      "\"\"\"\n",
      "posMap = {'NN': wn.NOUN, 'VB': wn.VERB, 'JJ': wn.ADJ, 'RB': wn.ADV}\n",
      "def posTagToWn(posTag):\n",
      "    return posMap.get(posTag[0:2])\n",
      "\n",
      "\"\"\"\n",
      "Tokenize, tag, and sentiment-classify a message.\n",
      "No need to clean text before passing here.\n",
      "\"\"\"\n",
      "def sentimentClassify(message):\n",
      "    message = cleanMessage(message) # Clean.\n",
      "\n",
      "    # Tokenize. Get meanings.\n",
      "    sentences = nltk.sent_tokenize(message)\n",
      "    sentenceTokens = [nltk.word_tokenize(s) for s in sentences]\n",
      "    posTags = [nltk.pos_tag(st) for st in sentenceTokens]\n",
      "\n",
      "    # Translate to Wordnet POS tags.\n",
      "    wnPosTags = []\n",
      "    for s in posTags:\n",
      "        wnPosTags.append([(token,posTagToWn(pos)) for (token,pos) in s if posTagToWn(pos) is not None])\n",
      "\n",
      "    # Get sense. Use lesk() to start; if failed, just take first sense.\n",
      "    def wsd(tokens,token,pos):\n",
      "        leskWsd = nltk.wsd.lesk(tokens,token,pos)\n",
      "        if leskWsd:\n",
      "            return leskWsd\n",
      "        defaultWsd = wn.synsets(token,pos)\n",
      "        if defaultWsd:\n",
      "            return defaultWsd[0]\n",
      "        return None;\n",
      "\n",
      "    # This is a big design decision. Do we collapse the sentences into one long set of tokens for WSD?\n",
      "    # It might be good to do so, as each set of text is about a focused topic and we can understand more by\n",
      "    # considering it as a whole.\n",
      "    # On the other hand, it might cause confusion among some words.\n",
      "    # Decision: combine all text here. If we wanted to classify separately, the following code would have worked:\n",
      "    # senses = []\n",
      "    # for s in wnPosTags:\n",
      "    #     sentenceSenses = [(token, wsd(tokens,token,pos)) for (token,pos) in s]\n",
      "    #     sentenceSenses = [t for t in sentenceSenses if t[1] is not None]\n",
      "    #     senses.append(sentenceSenses)\n",
      "\n",
      "    # Collapse list.\n",
      "    wnPosTagsFlat = [item for sublist in wnPosTags for item in sublist]\n",
      "    allTokens = [token for (token,_) in wnPosTagsFlat]\n",
      "\n",
      "    # Calculate senses.\n",
      "    senses = [(token, wsd(allTokens,token,pos)) for (token,pos) in wnPosTagsFlat]\n",
      "    senses = [s for s in senses if s[1] is not None]\n",
      "\n",
      "    # Aggregate score containers.\n",
      "    aggScorePos = 0\n",
      "    aggScoreNeg = 0\n",
      "\n",
      "    # Score containers for \"significant\" (obj != 1) tokens.\n",
      "    significantTokens = 0\n",
      "    aggSigScorePos = 0\n",
      "    aggSigScoreNeg = 0\n",
      "\n",
      "    # Container for scoring information.\n",
      "    scoreInfo = { 'tokens': [] }\n",
      "\n",
      "    # Score.\n",
      "    for (token, sense) in senses:\n",
      "        # Score.\n",
      "        swnEntry = swn.senti_synset(sense.name())\n",
      "        if swnEntry is None:\n",
      "            continue\n",
      "        scoreInfo['tokens'].append((\n",
      "            token, sense.name(),\n",
      "            swnEntry.pos_score(),\n",
      "            swnEntry.neg_score()\n",
      "        ))\n",
      "        # Aggregates.\n",
      "        aggScorePos += swnEntry.pos_score()\n",
      "        aggScoreNeg += swnEntry.neg_score()\n",
      "        # Significants.\n",
      "        if swnEntry.pos_score() > 0 or swnEntry.neg_score() > 0:\n",
      "            significantTokens += 1\n",
      "            aggSigScorePos += swnEntry.pos_score()\n",
      "            aggSigScoreNeg += swnEntry.neg_score()\n",
      "\n",
      "    # Calculate means.\n",
      "    # Aggregates.\n",
      "    scoreInfo['aggScorePos'] = aggScorePos\n",
      "    scoreInfo['aggScoreNeg'] = aggScoreNeg\n",
      "    # Means.\n",
      "    scoreInfo['meanScorePos'] = aggScorePos / max(1,len(scoreInfo['tokens']))\n",
      "    scoreInfo['meanScoreNeg'] = aggScoreNeg / max(1,len(scoreInfo['tokens']))\n",
      "    # Significants.\n",
      "    scoreInfo['meanScorePosSig'] = aggSigScorePos / (significantTokens or 1)\n",
      "    scoreInfo['meanScoreNegSig'] = aggSigScoreNeg / (significantTokens or 1)\n",
      "\n",
      "    # Perform final classification.\n",
      "    if (scoreInfo['aggScorePos'] > scoreInfo['aggScoreNeg']):\n",
      "        scoreInfo['class'] = 'Positive'\n",
      "    elif (scoreInfo['aggScorePos'] < scoreInfo['aggScoreNeg']):\n",
      "        scoreInfo['class'] = 'Negative'\n",
      "    else:\n",
      "        scoreInfo['class'] = 'Neutral'\n",
      "    return scoreInfo, len(allTokens)\n",
      "\n",
      "##### Main Execution\n",
      "\n",
      "# Iterate over all organizations.\n",
      "for org, orgData in relevantOrgs.iteritems():\n",
      "    # General settings.\n",
      "    orgName = orgData['name']\n",
      "\n",
      "    # Twitter. Start by getting configuration and printing status.\n",
      "    sn = orgData['twitter']\n",
      "    twitterStart = dt.datetime.now()\n",
      "\n",
      "    # Find tweets.\n",
      "    tweetsFilename = [f for f in os.listdir(consolidatedTweetsDirectory) if f.find(org) == 0][-1]\n",
      "    tweets = json.load(open(consolidatedTweetsDirectory+tweetsFilename))\n",
      "\n",
      "    # Container for sentiments.\n",
      "    sentiments = dict()\n",
      "\n",
      "    # Stats.\n",
      "    postCount = len(tweets)\n",
      "    postsTagged = 0\n",
      "    tokensCounted = 0\n",
      "    wordsTagged = 0\n",
      "\n",
      "    # Iterate over tweets and extract sentiment information.\n",
      "    for tweet in tweets:\n",
      "        # Get text .\n",
      "        text = tweet['text']\n",
      "        # Classify.\n",
      "        scoreInfo, tokenCount = sentimentClassify(text)\n",
      "        # Stats.\n",
      "        wordsTagged += len(scoreInfo['tokens'])\n",
      "        tokensCounted += tokenCount\n",
      "        if (len(scoreInfo['tokens'])):\n",
      "            sentiments[tweet['id']] = scoreInfo\n",
      "            postsTagged += 1\n",
      "        else:\n",
      "            sentiments[tweet['id']] = None\n",
      "        # Append to dict.\n",
      "        \n",
      "\n",
      "    # Output.\n",
      "    json.dump(sentiments,open(twitterSentimentDirectory+org+'.json','w'))\n",
      "\n",
      "    # Facebook. Start by getting configuration and printing status.\n",
      "    user = orgData['facebook']\n",
      "    facebookStart = dt.datetime.now()\n",
      "\n",
      "    # Find posts.\n",
      "    postsFilename = [f for f in os.listdir(consolidatedFbPostsDirectory) if f.find(org) == 0][-1]\n",
      "    posts = json.load(open(consolidatedFbPostsDirectory+postsFilename))\n",
      "\n",
      "    # Container for sentiments.\n",
      "    sentiments = dict()\n",
      "\n",
      "    # Stats.\n",
      "    postCount = len(posts)\n",
      "    postsTagged = 0\n",
      "    tokensCounted = 0\n",
      "    wordsTagged = 0\n",
      "\n",
      "    # Iterate over posts and extract sentiment information.\n",
      "    # Note that we'll score three things: the post, the headline, and the description, if all are present.\n",
      "    # We will also put together an \"aggregate\" score using the combined text of all three.\n",
      "    for post in posts:\n",
      "        # Storage for all post sentiments.\n",
      "        postSentiments = {}\n",
      "        for field in ('message','name','description', 'agg'):\n",
      "            # Get field.\n",
      "            if (field == 'agg'):\n",
      "                text = '. '.join([\n",
      "                    post['message'] if 'message' in post else '',\n",
      "                    post['name'] if 'name' in post else '',\n",
      "                    post['description'] if 'description' in post else ''\n",
      "                ])\n",
      "            else:\n",
      "                text = post[field] if field in post else ''\n",
      "            # Classify.\n",
      "            scoreInfo, tokenCount = sentimentClassify(text)\n",
      "            # Stats.\n",
      "            wordsTagged += len(scoreInfo['tokens'])\n",
      "            tokensCounted += tokenCount\n",
      "            scoreInfo['scored'] = bool(len(scoreInfo['tokens']))\n",
      "            # Append to post sentiments.\n",
      "            postSentiments[field] = scoreInfo\n",
      "        # Stats.\n",
      "        postsTagged += 1 if sum([len(info['tokens']) for info in postSentiments.values()]) else 0\n",
      "        # Append to dict.\n",
      "        sentiments[post['id']] = postSentiments\n",
      "\n",
      "    # Output.\n",
      "    json.dump(sentiments,open(fbSentimentDirectory+org+'.json','w'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<a id=\"s7.2\"></a>Analyzing Organization Sentiment"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While sentiment classification applies directly to clustering and prediction, it is also an interesting avenue for analysis by itself."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}