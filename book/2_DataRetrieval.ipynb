{
 "metadata": {
  "name": "",
  "signature": "sha256:19f01fc09227e0cb22f62ee3b29cafab2a9c9df7a0d6caf7b0203bba7cfce9ed"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<a id=\"s2\"></a>Data Retrieval"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This section describes the process and code that we used to retrieve our raw data (from Twitter, Facebook, Bitly, and URL paths). Code for these tasks was (typically) written to be executed manually in *.py* files. Simplified versions are presented here for documentation, may not be runnable as-is without API configuration. Here, we strip out many rote tasks\u2014loading libraries, printing status updates, etc.\u2014in the interest of focusing on important functionality. If the reader is interested in full source code, it may be found in (and executed with) the noted files.\n",
      "\n",
      "The following code loads all libraries utilized in this section."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import datetime as dt\n",
      "import dateutil.parser\n",
      "from hashlib import md5\n",
      "import json\n",
      "import numpy as np\n",
      "import os\n",
      "import re\n",
      "import requests\n",
      "import shutil\n",
      "import sys\n",
      "import time\n",
      "from twython import Twython\n",
      "import urllib"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Tweets"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Recent Tweets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use the [Twitter OAuth2 REST APIs](https://dev.twitter.com/rest/public) and [Twython package](https://twython.readthedocs.org/en/latest/) to read recent tweets from all of our target news organizations. Twitter's API imposes a 3,200 tweet limit on historical retrieval of individual accounts\u2019 tweets, so this process simply pulled tweets in sets of 200 (an API-imposed limit) until no more remained. See below for information about retrieving older tweets. Twitter's API documentation may provides a [helpful example of the JSON data](https://dev.twitter.com/rest/reference/get/statuses/user_timeline) that that process retrieves (and store) from the API.\n",
      "\n",
      "See the full version of this script in: [/src/twitter/downloadRawTweets.py](https://github.com/micahlanier/social-news/blob/master/src/twitter/downloadRawTweets.py)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "\n",
      "This script will pull as many recent tweets as possible from given organizations and dump them to a timestamped file.\n",
      "If we pull tweets over time, this will allow us to arrange pulls in order and consolidate them into a master file.\n",
      "\n",
      "We have encountered rate limit failures around 17 consecutive organizations.\n",
      "Consider dividing the organization list in two and fetching both halves over time.\n",
      "If it fails, wait 15 minutes and try again. In the future, we can think about cycling through API keys.\n",
      "\n",
      "Relevant Twitter API documentation:\n",
      "    https://dev.twitter.com/rest/reference/get/statuses/user_timeline\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "##### Configuration\n",
      "\n",
      "# List of organizations to fetch. Either a list or \"all\".\n",
      "orgs = 'all'\n",
      "\n",
      "##### Main Setup\n",
      "\n",
      "# API setup.\n",
      "apiConfigPath = '../../../conf/twitter.json'\n",
      "\n",
      "# Data directory.\n",
      "rawTweetsDirectory = '../../../data/twitter/raw/'\n",
      "# Get timestamp of current run.\n",
      "timestampFilename = dt.datetime.now().strftime('%Y-%m-%dT%H-%M-%S') + '.json'\n",
      "\n",
      "# Twitter Setup\n",
      "# Read API settings.\n",
      "twitterSettings = json.load(open(apiConfigPath))\n",
      "# Set up twitter object.\n",
      "twitter = Twython(twitterSettings['apiKey'], access_token=twitterSettings['accessToken'])\n",
      "# Default number of tweets to grab per request.\n",
      "tweetsPerRequest = 200\n",
      "\n",
      "# Organization setup.\n",
      "allOrgs = json.load(open('../../conf/organizations.json'))\n",
      "if (type(orgs) == type('str')):\n",
      "    orgs = allOrgs.keys()\n",
      "relevantOrgs = dict((k, allOrgs[k]) for k in orgs);\n",
      "\n",
      "##### Get Tweets\n",
      "\n",
      "# Iterate over organizations.\n",
      "for org, orgData in relevantOrgs.iteritems():\n",
      "    # Get SN.\n",
      "    sn = orgData['twitter']\n",
      "\n",
      "    # Set up container for tweets.\n",
      "    tweets = []\n",
      "    # Keep track of min tweet ID; min/max dates.\n",
      "    minTweetId = None\n",
      "    minTweetDate = None\n",
      "    maxTweetDate = None\n",
      "    # Set up folder if it doesn't exist.\n",
      "    snDirectory = rawTweetsDirectory + org + '/'\n",
      "    if not os.path.exists(snDirectory):\n",
      "        os.makedirs(snDirectory)\n",
      "\n",
      "    # Start looping and grabbing tweets as we go. Loop will terminate at a break.\n",
      "    # We just set an arbitrarily high limit in case something goes haywire.\n",
      "    while len(tweets) < 4000:\n",
      "        # Get tweets.\n",
      "        if (minTweetId is None):\n",
      "            currentTweets = twitter.get_user_timeline(screen_name=sn, count=tweetsPerRequest)\n",
      "        else:\n",
      "            currentTweets = twitter.get_user_timeline(screen_name=sn, max_id=minTweetId, count=tweetsPerRequest)\n",
      "        # If nothing there (we've reached the end), break the loop\n",
      "        if (not len(currentTweets)):\n",
      "            break\n",
      "        # Iterate over tweets and find min ID, min/max date.\n",
      "        for t in currentTweets:\n",
      "            minTweetId = min([tId for tId in [minTweetId, t['id']-1] if tId is not None])\n",
      "            currentTweetDate = dateutil.parser.parse(t['created_at'])\n",
      "            minTweetDate = min([tDate for tDate in [minTweetDate, currentTweetDate] if tDate is not None])\n",
      "            maxTweetDate = max([tDate for tDate in [maxTweetDate, currentTweetDate] if tDate is not None])\n",
      "        # Append to list.\n",
      "        tweets += currentTweets\n",
      "\n",
      "    # Dump to file.\n",
      "    filename = rawTweetsDirectory + org + '/' + timestampFilename\n",
      "    json.dump(tweets, open(filename,'w'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Older Tweets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Twitter's API imposes a 3,200 tweet limit on historical retrieval of individual accounts\u2019 tweets. This extends quite far back for some users, but for more prolific tweeters (e.g., the *Huffington Post* and *New York Times*) this restricts pulls to only the last couple of weeks. In order to get a dataset that extends further back in time for the more prolific users, we devised a method for scraping Twitter's [search page](https://twitter.com/search-home) and using the API to get all information about each tweet that we found. See the full version in: [/src/twitter/scrapeOlderTweets.py](https://github.com/micahlanier/social-news/blob/master/src/twitter/scrapeOlderTweets.py)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "\n",
      "This script will scrape tweets beyond the 3200-tweet limit by scraping the Twitter web search interface.\n",
      "Expect this process to be slooow.\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "##### Configuration\n",
      "\n",
      "# We'll use these libraries immediately.\n",
      "import dateutil.parser\n",
      "\n",
      "# Enter a single organization to scrape. This should be changed between runs.\n",
      "org = 'nytimes'\n",
      "\n",
      "# Date to go back to.\n",
      "until = dateutil.parser.parse('2014/10/01')\n",
      "\n",
      "# Number of tweets to consolidate. Output will be in files with this many tweets.\n",
      "tweetsPerFile = 100\n",
      "\n",
      "##### Main Setup\n",
      "\n",
      "# Ensure \"until\" setting is comparable to minDate.\n",
      "# until = pytz.utc.localize(until)\n",
      "until = until.replace(tzinfo=dateutil.tz.tzutc())\n",
      "\n",
      "# Twitter Setup\n",
      "\n",
      "# API setup.\n",
      "apiConfigPath = '../../../conf/twitter.json'\n",
      "# Read API settings.\n",
      "twitterSettings = json.load(open(apiConfigPath))\n",
      "# Set up twitter object.\n",
      "twitter = Twython(twitterSettings['apiKey'], access_token=twitterSettings['accessToken'])\n",
      "# Wait time. Ensure we don't run afoul of rate limits.\n",
      "secondsToWait = 6\n",
      "\n",
      "# Organization/SN setup.\n",
      "allOrgs = json.load(open('../../conf/organizations.json'))\n",
      "sn = allOrgs[org]\n",
      "\n",
      "# Data directory.\n",
      "consolidatedTweetsDirectory = '../../../data/twitter/consolidated/%s/' % org\n",
      "if not os.path.exists(consolidatedTweetsDirectory):\n",
      "    raise Exception('No consolidated tweets for organization \"%s\" (@%s).' % (org,sn))\n",
      "\n",
      "# Data directory.\n",
      "rawTweetsDirectory = '../../../data/twitter/raw/%s/' % org\n",
      "# Get timestamp of current run.\n",
      "timestampFilename = dt.datetime.now().strftime('%Y-%m-%dT%H-%M-%S') + '.json'\n",
      "\n",
      "# Base search URL. Can just append an ID after this.\n",
      "baseSearchUrl = 'https://twitter.com/search?f=realtime&q=from%%3A%s%%20max_id%%3A' % sn\n",
      "\n",
      "### Find Starting Point\n",
      "\n",
      "# Find all consolidated tweet files and get the most recent.\n",
      "tweetFile = [f for f in os.listdir(consolidatedTweetsDirectory) if f[-5:] == '.json'][-1]\n",
      "\n",
      "# Load that file with json.\n",
      "consolidatedTweets = json.load(open(consolidatedTweetsDirectory+tweetFile))\n",
      "\n",
      "# Find min ID.\n",
      "minTweetId = None\n",
      "for tweet in consolidatedTweets:\n",
      "    minTweetId = min([tId for tId in [minTweetId, tweet['id']] if tId is not None])\n",
      "\n",
      "# Status.\n",
      "print 'Found minimum tweet ID for organization \"%s\" (@%s): %d' % (org,sn,minTweetId)\n",
      "print 'Will start search at %d - 1' % minTweetId\n",
      "\n",
      "### Scraping\n",
      "\n",
      "# Container for tweets.\n",
      "tweets = []\n",
      "\n",
      "# Statistics.\n",
      "totalTweets = 0\n",
      "minTweetDate = None\n",
      "maxTweetDate = None\n",
      "\n",
      "# Loop terminates with break condition inside once we've found a date.\n",
      "while True:\n",
      "    # Decrement ID to avoid repeats.\n",
      "    minTweetId -= 1\n",
      "\n",
      "    # Make request and convert to a bs4 object.\n",
      "    requestUrl = baseSearchUrl + str(minTweetId)\n",
      "    searchRequest = requests.get(requestUrl)\n",
      "    soup = BeautifulSoup(searchRequest.text)\n",
      "\n",
      "    # Traverse all content items and extract tweet data.\n",
      "    tweetContainers = soup.find_all('div', class_='tweet')\n",
      "\n",
      "    # Traverse all tweet containers.\n",
      "    for t in tweetContainers:\n",
      "        # Find tweet ID for this tweet.\n",
      "        tweetId = int(t.attrs['data-tweet-id'])\n",
      "\n",
      "        # Get tweet; append.\n",
      "        tweet = twitter.show_status(id=tweetId)\n",
      "        tweets.append(tweet)\n",
      "\n",
      "        # Get stats.\n",
      "        minTweetId = minTweetId = min([tId for tId in [minTweetId, tweetId] if tId is not None])\n",
      "        tweetDate = dateutil.parser.parse(tweet['created_at'])\n",
      "        minTweetDate = min([tDate for tDate in [minTweetDate, tweetDate] if tDate is not None])\n",
      "        maxTweetDate = max([tDate for tDate in [maxTweetDate, tweetDate] if tDate is not None])\n",
      "\n",
      "        # Wait before making another request.\n",
      "        time.sleep(secondsToWait)\n",
      "\n",
      "    # Write out if we've hit the per-file limit or have gone back far enough.\n",
      "    if len(tweets) >= tweetsPerFile or minTweetDate < until:\n",
      "        # Stats.\n",
      "        totalTweets += len(tweets)\n",
      "\n",
      "        # Status.\n",
      "        print 'Writing %d tweets to file %s' % (len(tweets), timestampFilename)\n",
      "\n",
      "        # Output to file.\n",
      "        filename = rawTweetsDirectory + timestampFilename\n",
      "        json.dump(tweets, open(filename,'w'))\n",
      "\n",
      "        # Reset relevant objects.\n",
      "        timestampFilename = dt.datetime.now().strftime('%Y-%m-%dT%H-%M-%S') + '.json'\n",
      "        tweets = []\n",
      "\n",
      "        # Break if we've gone back far enough.\n",
      "        if minTweetDate < until:\n",
      "            break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Facebook Posts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use Facebook's [Graph API](https://developers.facebook.com/docs/graph-api) to retrieve recent posts by each user. Facebook imposes no burdensome history length limit, so our code simply requests information in 75-post chunks until hitting a specified past date (set to September 1 when we gathered our data). See the full version in: [/src/fb/downloadRawFbPosts.py](https://github.com/micahlanier/social-news/blob/master/src/fb/downloadRawFbPosts.py)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "\n",
      "This script will pull recent Facebook posts from given usernames and dump them to a timestamped file.\n",
      "If we pull posts over time, this will allow us to arrange pulls in order and consolidate them into a master file.\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "##### Configuration\n",
      "\n",
      "# List of organizations to fetch. Either a list or \"all\".\n",
      "orgs = 'all'\n",
      "# Upper limit of date.\n",
      "maxDate = dt.datetime(2014,11,25,tzinfo=dateutil.tz.tzutc()) #dt.datetime.utcnow()\n",
      "# Lower limit of date.\n",
      "minDate = dt.datetime(2014, 8,31,tzinfo=dateutil.tz.tzutc())\n",
      "# Facebook is not as explicit as Twitter about limiting. Set this as needed if you run into any issues.\n",
      "postLimit = 75\n",
      "\n",
      "##### Main Setup\n",
      "\n",
      "# API setup.\n",
      "apiConfigPath = '../../../conf/facebook.json'\n",
      "\n",
      "# Data directory.\n",
      "rawPostsDirectory = '../../../data/facebook/raw/'\n",
      "# Get timestamp of current run.\n",
      "timestampFilename = dt.datetime.now().strftime('%Y-%m-%dT%H-%M-%S') + '.json'\n",
      "\n",
      "# Facebook Setup\n",
      "# Read API settings.\n",
      "fbSettings = json.load(open(apiConfigPath))\n",
      "\n",
      "# Ensure maxDate comparable to minDate.\n",
      "maxDate = maxDate.replace(tzinfo=dateutil.tz.tzutc())\n",
      "\n",
      "# Keep a one-second time delta handy for upper-bound limits.\n",
      "oneSecond = dt.timedelta(seconds=1)\n",
      "\n",
      "# Sleep interval for transient errors.\n",
      "sleepTimeSeconds = 4\n",
      "\n",
      "# Organization setup.\n",
      "allOrgs = json.load(open('../../conf/organizations.json'))\n",
      "if (type(orgs) == type('str')):\n",
      "    orgs = allOrgs.keys()\n",
      "relevantOrgs = dict((k, allOrgs[k]) for k in orgs)\n",
      "\n",
      "##### Facebook Helper Code\n",
      "\n",
      "\"\"\"\n",
      "Simple function for performing a Facebook GET request.\n",
      "Does not check parameters, so be careful!\n",
      "\"\"\"\n",
      "def facebookGet(path, accessToken, parameters, version = '2.1'):\n",
      "    fbUrl = 'https://graph.facebook.com/v' + version + '/'\n",
      "    # Add request path.\n",
      "    fbUrl += path.strip('/') + '/'\n",
      "    # Add access token.\n",
      "    fbUrl += '?access_token=' + accessToken\n",
      "    # Add additional parameters.\n",
      "    for pKey, pValue in parameters.iteritems():\n",
      "        fbUrl += '&%s=%s' % (pKey, urllib.quote(str(pValue)))\n",
      "    return requests.get(fbUrl)\n",
      "\n",
      "##### Get Facebook Posts\n",
      "\n",
      "# Iterate over organizations.\n",
      "for org, orgData in relevantOrgs.iteritems():\n",
      "    # Get account.\n",
      "    acct = orgData['facebook']\n",
      "\n",
      "    print 'Retrieving posts for organization \"%s\" (@%s).' % (org,acct)\n",
      "\n",
      "    # Set up folder if it doesn't exist.\n",
      "    acctDirectory = rawPostsDirectory + org + '/'\n",
      "    if not os.path.exists(acctDirectory):\n",
      "        os.makedirs(acctDirectory)\n",
      "\n",
      "    # Set up container for posts.\n",
      "    posts = []\n",
      "\n",
      "    # Set up a loop date.\n",
      "    loopMaxDate = maxDate\n",
      "\n",
      "    # Set up objects hold our request.\n",
      "    requestPath = acct + '/posts'\n",
      "    requestParams = dict()\n",
      "    requestParams['limit'] = postLimit\n",
      "\n",
      "    # Don't bother with getting ALL comments/likes. We're only interested in summary statistics.\n",
      "    # While it would be cool to analyze all of the people who comment/like, that's a PITA in the time we have.\n",
      "    # By doing this, we need to specify a whitelist of all fields that we want.\n",
      "    apiFields = [\n",
      "        'comments.limit(1).summary(true)','likes.limit(1).summary(true)',\n",
      "        'caption','created_time','description','from','id','link','message','name','shares','status_type','type'\n",
      "    ]\n",
      "    requestParams['fields'] = ','.join(apiFields)\n",
      "\n",
      "    # Reload settings just in case we've changed access tokens and are running manually.\n",
      "    fbSettings = json.load(open(apiConfigPath))\n",
      "\n",
      "    # Loop until we have gone back to minDate.\n",
      "    while loopMaxDate > minDate:\n",
      "        # Compose a request with the latest max date.\n",
      "        requestParams['until'] = str(loopMaxDate)\n",
      "        # Make request; parse JSON.\n",
      "        facebookRequest = facebookGet(requestPath, fbSettings['accessToken'], requestParams)\n",
      "        jsonFacebookRequest = json.loads(facebookRequest.text)\n",
      "\n",
      "        # Detect any errors.\n",
      "        if 'error' in jsonFacebookRequest:\n",
      "            if jsonFacebookRequest['error']['code'] == 2:\n",
      "                # Tranient, \"unexpected\" error. Sleep for a sec and then continue.\n",
      "                print 'Unexpected error. Sleeping %d seconds.' % sleepTimeSeconds\n",
      "                time.sleep(2)\n",
      "                continue\n",
      "            else:\n",
      "                print 'Encountered a major error. Breaking loop.'\n",
      "                print jsonFacebookRequest['error']\n",
      "                break\n",
      "\n",
      "        # If no errors, we got data.\n",
      "        currentPosts = jsonFacebookRequest['data']\n",
      "\n",
      "        # Break if we didn't get anything; otherwise append and continue.\n",
      "        if (not len(currentPosts)):\n",
      "            break\n",
      "        posts += currentPosts\n",
      "\n",
      "        # Get date for next request. Go back one second, because for some reason they use inclusive logic.\n",
      "        loopMaxDate = dateutil.parser.parse(currentPosts[-1]['created_time']) - oneSecond\n",
      "\n",
      "    # Dump to file.\n",
      "    filename = acctDirectory + timestampFilename\n",
      "    json.dump(posts, open(filename,'w'), indent=0)\n",
      "\n",
      "    # Get max/min dates retrieved.\n",
      "    minDateRetrieved = loopMaxDate + oneSecond\n",
      "    maxDateRetrieved = dateutil.parser.parse(posts[0]['created_time'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "User Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Twitter and Facebook APIs both provide easy access to user information. These scripts retrieve that information for display, and store it in the /data/users/ directory. See the full version in: [/src/users/user_statistics.ipynb](http://nbviewer.ipython.org/github/micahlanier/social-news/blob/master/src/users/user_statistics.ipynb)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##### Setup\n",
      "# Get information about organizations.\n",
      "orgs = json.load(open('../../conf/organizations.json'))\n",
      "# Convert to screen names data structure.\n",
      "screenNames = dict(\n",
      "    (orgData['name'],{'twitter': orgData['twitter'], 'facebook': orgData['facebook'], 'org': org})\n",
      "    for (org,orgData) in orgs.iteritems()\n",
      ")\n",
      "# Get Twitter settings from outside of the repository path.\n",
      "twitterSettingsPath = '../../../conf/twitter.json'\n",
      "twitterSettings = json.load(open(twitterSettingsPath))\n",
      "# Get access token if we need one.\n",
      "if ('accessToken' not in twitterSettings):\n",
      "    twitter = Twython(twitterSettings['apiKey'], twitterSettings['apiSecret'], oauth_version=2)\n",
      "    twitterSettings['accessToken'] = twitter.obtain_access_token()\n",
      "    json.dump(open(twitterSettings,'w'))\n",
      "# Set up our Twython object.\n",
      "twitter = Twython(twitterSettings['apiKey'], access_token=twitterSettings['accessToken'])\n",
      "\n",
      "##### Twitter\n",
      "# Get user information.\n",
      "twNames = []\n",
      "twNameRevMap = {}\n",
      "for name in screenNames:\n",
      "    twNames.append(screenNames[name]['twitter'])\n",
      "    twNameRevMap[screenNames[name]['twitter']] = name\n",
      "stats = twitter.lookup_user(screen_name=','.join(twNames))\n",
      "column_mapper = {\n",
      "    'favourites_count' : 'favourites_count',\n",
      "    'listed_count' : 'listed_count',\n",
      "    'followers_count' : 'followers_count',\n",
      "    'statuses_count' : 'statuses_count',\n",
      "    'friends_count' : 'friends_count',\n",
      "}\n",
      "res = {}\n",
      "for key in column_mapper:\n",
      "    res[key] = {}\n",
      "for st in stats:\n",
      "    for key in column_mapper:\n",
      "        res[key][twNameRevMap[st['screen_name'].lower()]] = st[column_mapper[key]]\n",
      "df_twitter = pd.DataFrame(res)\n",
      "# Save to data folder.\n",
      "# Transpose and convert to dict.\n",
      "orgStats = dict((screenNames[i]['org'],j) for (i,j) in df_twitter.transpose().to_dict().iteritems())\n",
      "# Save.\n",
      "json.dump(orgStats,open('../../data/users/twitter.json','w'),indent=4)\n",
      "\n",
      "### Facebook\n",
      "fbNameRevMap = {}\n",
      "for name in screenNames:\n",
      "    fbNameRevMap[screenNames[name]['facebook']] = name\n",
      "\n",
      "fb_stats = {}\n",
      "for name in screenNames:\n",
      "    fb_stats[name] = json.load(urllib2.urlopen(\"http://graph.facebook.com/\"+screenNames[name]['facebook']))\n",
      "    fb_column_mapper = {\n",
      "    'id' : 'id',\n",
      "    'likes' : 'likes',\n",
      "    'talking_about_count' : 'talking_about_count',\n",
      "}\n",
      "fb_res = {}\n",
      "\n",
      "for key in fb_column_mapper:\n",
      "    fb_res[key] = {}\n",
      "\n",
      "for name in screenNames:\n",
      "    for key in fb_column_mapper:\n",
      "        fb_res[key][name] = fb_stats[name][fb_column_mapper[key]]\n",
      "        \n",
      "df_facebook = pd.DataFrame(fb_res)\n",
      "# Save to data folder.\n",
      "# Transpose and convert to dict.\n",
      "orgStats = dict((screenNames[i]['org'],j) for (i,j) in df_facebook.transpose().to_dict().iteritems())\n",
      "# Save.\n",
      "json.dump(orgStats,open('../../data/users/facebook.json','w'),indent=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Twitter & Facebook Consolidation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above scripts are designed to be run progressively in order to collect data as time goes on. Their data may overlap significantly, so it is necessary to consolidate all retrieved data into a \"master\" JSON file for each organization-network. In retrospect, a NoSQL solution like MongoDB would have been a logical choice for storing this information, and we would certainly use it if we had more time available (and if we opt to continue our analysis in the future). See the full versions of these files in: [/src/fb/consolidateRawFbPosts.py](https://github.com/micahlanier/social-news/blob/master/src/fb/consolidateRawFbPosts.py) and [/src/twitter/consolidateRawTweets.py](https://github.com/micahlanier/social-news/blob/master/src/twitter/consolidateRawTweets.py)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Twitter Consolidation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "\n",
      "This script will consolidate raw tweet files into one large JSON file (timestamped by runtime).\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "##### Configuration\n",
      "\n",
      "# Organizations to consolidate. Either a list or \"all\".\n",
      "orgs = 'all'\n",
      "\n",
      "##### Main Setup\n",
      "\n",
      "# Data directory.\n",
      "rawTweetsDirectory = '../../../data/twitter/raw/'\n",
      "consolidatedTweetsDirectory = '../../../data/twitter/consolidated/'\n",
      "# Get timestamp of current run.\n",
      "timestampFilename = dt.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
      "\n",
      "# Organization setup.\n",
      "allOrgs = json.load(open('../../conf/organizations.json'))\n",
      "if (type(orgs) == type('str')):\n",
      "    orgs = allOrgs.keys()\n",
      "relevantOrgs = dict((k, allOrgs[k]) for k in orgs);\n",
      "\n",
      "##### Get Tweets\n",
      "\n",
      "# Iterate over organizations.\n",
      "for org, orgData in relevantOrgs.iteritems():\n",
      "    # Get SN.\n",
      "    sn = orgData['twitter']\n",
      "\n",
      "    # We're going to hold tweets in one dict.\n",
      "    # If we end up with a lot of them we may need to be more careful here.\n",
      "    consolidatedTweets = dict()\n",
      "\n",
      "    # Keep track of tweets read; min/max date.\n",
      "    tweetsRead = 0\n",
      "    minTweetDate = None\n",
      "    maxTweetDate = None\n",
      "\n",
      "    # Get list of files. Remove junk in the process.\n",
      "    rawTweetsFiles = [rtf for rtf in os.listdir(rawTweetsDirectory + org + '/') if rtf[-5:] == '.json']\n",
      "\n",
      "    # Iterate over files.\n",
      "    for rawTweetFile in rawTweetsFiles:\n",
      "        # Read in JSON.\n",
      "        filepath = rawTweetsDirectory + org + '/' + rawTweetFile\n",
      "        rawTweets = json.load(open(filepath))\n",
      "        # Iterate over tweets.\n",
      "        for rawTweet in rawTweets:\n",
      "            # Get tweet and use it as key for insertion into consolidatedTweets.\n",
      "            tId = rawTweet['id']\n",
      "            consolidatedTweets[tId] = rawTweet\n",
      "            # Track tweet date info.\n",
      "            currentTweetDate = dateutil.parser.parse(rawTweet['created_at'])\n",
      "            minTweetDate = min([tDate for tDate in [minTweetDate, currentTweetDate] if tDate is not None])\n",
      "            maxTweetDate = max([tDate for tDate in [maxTweetDate, currentTweetDate] if tDate is not None])\n",
      "        # Keep track of tweets read.\n",
      "        tweetsRead += len(rawTweets)\n",
      "\n",
      "    # Dump to file.\n",
      "    filename = consolidatedTweetsDirectory + '/' + org + '-' + timestampFilename + '.json'\n",
      "    json.dump(consolidatedTweets.values(), open(filename,'w'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Facebook Consolidation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "\n",
      "This script will consolidate raw post files into one large JSON file (timestamped by runtime).\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "##### Configuration\n",
      "\n",
      "# Edit things here to your liking. But please don't commit them needlessly.\n",
      "\n",
      "# List of organizations to fetch. Either a list or \"all\".\n",
      "orgs = 'all'\n",
      "\n",
      "##### Main Setup\n",
      "\n",
      "# Data directory.\n",
      "rawPostsDirectory = '../../../data/facebook/raw/'\n",
      "consolidatedPostsDirectory = '../../../data/facebook/consolidated/'\n",
      "# Get timestamp of current run.\n",
      "timestampFilename = dt.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
      "\n",
      "# Organization setup.\n",
      "allOrgs = json.load(open('../../conf/organizations.json'))\n",
      "if (type(orgs) == type('str')):\n",
      "    orgs = allOrgs.keys()\n",
      "relevantOrgs = dict((k, allOrgs[k]) for k in orgs)\n",
      "\n",
      "##### Get Tweets\n",
      "\n",
      "# Iterate over organizations.\n",
      "for org, orgData in relevantOrgs.iteritems():\n",
      "    # Get account.\n",
      "    acct = orgData['facebook']\n",
      "\n",
      "    # We're going to hold posts in one dict.\n",
      "    # If we end up with a lot of them we may need to be more careful here.\n",
      "    consolidatedPosts = dict()\n",
      "\n",
      "    # Keep track of posts read; min/max date.\n",
      "    postsRead = 0\n",
      "    minPostDate = None\n",
      "    maxPostDate = None\n",
      "\n",
      "    # Get list of files. Remove junk in the process.\n",
      "    rawPostsFiles = [rtf for rtf in os.listdir(rawPostsDirectory + org + '/') if rtf[-5:] == '.json']\n",
      "\n",
      "    # Iterate over files.\n",
      "    for rawPostFile in rawPostsFiles:\n",
      "        # Read in JSON.\n",
      "        filepath = rawPostsDirectory + org + '/' + rawPostFile\n",
      "        rawPosts = json.load(open(filepath))\n",
      "        # Iterate over tweets.\n",
      "        for rawPost in rawPosts:\n",
      "            # Get post and use it as key for insertion into consolidatedPosts.\n",
      "            pId = rawPost['id']\n",
      "            consolidatedPosts[pId] = rawPost\n",
      "            # Track tweet date info.\n",
      "            currentPostDate = dateutil.parser.parse(rawPost['created_time'])\n",
      "            minPostDate = min([tDate for tDate in [minPostDate, currentPostDate] if tDate is not None])\n",
      "            maxPostDate = max([tDate for tDate in [maxPostDate, currentPostDate] if tDate is not None])\n",
      "        # Keep track of posts read.\n",
      "        postsRead += len(rawPosts)\n",
      "\n",
      "    # Dump to file.\n",
      "    filename = consolidatedPostsDirectory + org + '-' + timestampFilename + '.json'\n",
      "    json.dump(consolidatedPosts.values(), open(filename,'w'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "URL Retrieval"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We were interested in determining where social media links actually *go*. That knowledge would enable us to (1) track overall performance of a link that is posted more than once (e.g., [@nytimes's football map tweets](https://twitter.com/search?q=from%3Anytimes%20football%20map&src=typd)) and (2) match Twitter and Facebook posts in order to determine relative effectiveness. This is obscured by the fact that 23 of the 25 news organizations that we examined use Bitly regularly for posting links on social media websites. But Bitly API calls cannot necessarily tell us where those links go: many redirect several more times (e.g., through [trib.al](http://trib.al/)) before reaching their final destinations. Thus, we were forced to determine link destinations on our own.\n",
      "\n",
      "We encountered two problems accomplishing this. First, we did not want to inflate Bitly click counts by following those links directly. This was alleviated by making our requests with HTTP `HEAD` requests, which avoid doing so. And second, we found that some news websites would return errors if we made high-frequency requests to them. To get around this, our code traverses the URL redirection chain until encountering the final link, but does not follow it.\n",
      "\n",
      "This code is designed to be executed from the command line in order to better handle parallelization and error recovery. See full versions of this code in [/src/urls/socialUrlUtils.py](https://github.com/micahlanier/social-news/blob/master/src/urls/socialUrlUtils.py) and [/src/urls/scrapeUrls.py](https://github.com/micahlanier/social-news/blob/master/src/urls/scrapeUrls.py). The former appears first; the latter, second."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "\n",
      "Utilities for detecting certain kinds of Facebook links and extracting links from text.\n",
      "\n",
      "Uses URL-detection regular expression from:\n",
      "    https://gist.github.com/gruber/8891611\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "# Define pattern.\n",
      "urlPattern = '(?i)\\\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\\\s()<>{}\\\\[\\\\]]+|\\\\([^\\\\s()]*?\\\\([^\\\\s()]+\\\\)[^\\\\s()]*?\\\\)|\\\\([^\\\\s]+?\\\\))+(?:\\\\([^\\\\s()]*?\\\\([^\\\\s()]+\\\\)[^\\\\s()]*?\\\\)|\\\\([^\\\\s]+?\\\\)|[^\\\\s`!()\\\\[\\\\]{};:\\'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019])|(?:(?<!@)[a-z0-9]+(?:[.\\\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\\\b/?(?!@)))'\n",
      "\n",
      "\"\"\"\n",
      "Detect if a URL is a link to a Facebook photo.\n",
      "Examples:\n",
      "    urlIsFacebookPhoto('https://www.facebook.com/bbcworldnews/photos/pcb.10153822033402588/10153822032512588/?type=1') == True\n",
      "    urlIsFacebookPhoto('http://bbc.in/fgh543q') == False    \n",
      "\"\"\"\n",
      "def urlIsFacebookPhoto(url):\n",
      "    return bool(re.match('^https?://(www\\\\.)?facebook.com/[a-zA-Z0-9]+/photos/.+$', url))\n",
      "\n",
      "\"\"\"\n",
      "Detect if a URL is a link to a Facebook video.\n",
      "Examples:\n",
      "    urlIsFacebookPhoto('https://www.facebook.com/video.php?v=10150471411394999') == True\n",
      "    urlIsFacebookPhoto('http://bbc.in/fgh543q') == False    \n",
      "\"\"\"\n",
      "def urlIsFacebookVideo(url):\n",
      "    return bool(re.match('^https?://(www\\\\.)?facebook.com/video.php\\\\?.+$', url))\n",
      "\n",
      "\"\"\"\n",
      "Return a list of all detected URLs, or an empty list if none.\n",
      "\"\"\"\n",
      "def urlsInText(t):\n",
      "    return re.findall(urlPattern, t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "\n",
      "This script scrapes URLs from posts and tweets. See /doc/URLs.txt for information on how it works and how it stores data between runs.\n",
      "At a high level, it begins by injesting all specified posts/tweets, and identifies the links therein.\n",
      "Any that have already been scraped are ignored. We may eventually need a system for fixing bad results.\n",
      "Any that have not been scraped are called with the requests module. For successful calls, we log ALL URLs traversed (just in case we need them).\n",
      "We also dump request text to a file named using an MD5 hash of the final URL minus any query string.\n",
      "\n",
      "Note that this script contains little inline configuration. URL scraping takes a while because we need to add delays so as to not cause 54 errors.\n",
      "The best solution is to parallelize execution. But parallelization in this script is a pain, so just do it by running the script several times simultaneously.\n",
      "\n",
      "To run, execute with the following parameters:\n",
      "    python scrapeUrls.py [network] [username] [delay] [scrapeContent]\n",
      "Parameters:\n",
      "    * organization: organization name for the given network\n",
      "    * network: one of 'facebook' or 'twitter'\n",
      "    * delay: delay in seconds to wait before retrieving subsequent URLs\n",
      "    * scrapeContent: 1 to scrape content, 0 to stop before making any request to the final link destination\n",
      "\n",
      "Scraping is complicated. Some hosts are fine with 1 or two seconds delay (e.g., nytimes). Others need 4+ seconds (cnn).\n",
      "Setting delays for this is more art than science, so good luck.\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "##### Configuration\n",
      "\n",
      "# This script is going to take a long time to run, so we want to store URLs in chunks in case anything goes wrong.\n",
      "incrementalUrlsToStore = 25\n",
      "\n",
      "# Timeout for hanging URLs.\n",
      "timeoutSeconds = 10.0\n",
      "\n",
      "##### Main Setup\n",
      "\n",
      "# Our own custom URL parsing code for URLs.\n",
      "import socialUrlUtils\n",
      "\n",
      "# Data directories.\n",
      "dirPath_consolidated = '../../../data/%s/consolidated/'\n",
      "filePath_urls = '../../../data/urls/%s/%s.json'\n",
      "dirPath_content = '../../../data/content/%s/%s/'\n",
      "dirPath_urlBackups = '../../../data/backup/urls/%s/'\n",
      "\n",
      "# Current timestamp.\n",
      "currTimestamp = dt.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
      "\n",
      "##### Configure Source\n",
      "\n",
      "# Set network, screen name, and sleep time based on inputs in sys.argv.\n",
      "org = str(sys.argv[1])\n",
      "network = str(sys.argv[2])\n",
      "sleepSeconds = float(sys.argv[3])\n",
      "scrapeContent = True if int(sys.argv[4]) else 0\n",
      "\n",
      "# Get organizational information.\n",
      "allOrgs = json.load(open('../../conf/organizations.json'))\n",
      "\n",
      "# Get information about the organization's URLs.\n",
      "targetDomain = allOrgs[org]['urls']['domain']\n",
      "bitlyUrl = allOrgs[org]['urls']['bitly']\n",
      "stillTraversePattern = allOrgs[org]['urls']['stillTraverse'] if 'stillTraverse' in allOrgs[org]['urls'] else None\n",
      "\n",
      "# Any other miscellanea.\n",
      "orgName = allOrgs[org]['name']\n",
      "\n",
      "##### Helper Functions\n",
      "\n",
      "\"\"\"\n",
      "Return a list of expanded URLs regardless of service. Don't do any cleaning of URLs.\n",
      "\"\"\"\n",
      "def associatedUrls(network,post):\n",
      "    if network == 'facebook':\n",
      "        links = []\n",
      "        if 'link' in post:\n",
      "            links += [post['link']]\n",
      "        if 'message' in post:\n",
      "            links += socialUrlUtils.urlsInText(post['message'])\n",
      "        return links\n",
      "    elif network == 'twitter':\n",
      "        return [u['expanded_url'] for u in post['entities']['urls']]\n",
      "    return []\n",
      "\n",
      "\"\"\"\n",
      "Traverse URLs until encountering a target domain.\n",
      "We may want to do this if a site makes an effort to counter scraping or raises \"connection reset\" errors.\n",
      "\n",
      "Returns two values: final request object, an error string, and listing of traversed URLs.\n",
      "If it encounters targetDomain, the final request object will be None. Otherwise, it will contain the final request.\n",
      "If any request errors, the error's string representation will be passed back.\n",
      "\"\"\"\n",
      "def traverseUrls(firstUrl, targetDomain, stillTraversePattern, bitlyUrl, scrape=False):\n",
      "    # Keep track of history.\n",
      "    hist = []\n",
      "    # Other return objects to keep.\n",
      "    errorText = None\n",
      "    lastRequest = None\n",
      "\n",
      "    # Keep looping until we no longer have a URL to traverse.\n",
      "    # We may manually break the loop inside if we want to exit immediately.\n",
      "    nextUrl = firstUrl\n",
      "    while nextUrl is not None:\n",
      "        # Append the URL to history.\n",
      "        hist += [nextUrl]\n",
      "        # Parse URL.\n",
      "        urlParsed = urlparse(nextUrl)\n",
      "        # If the destination url is our target and we're not scraping, return the list leading up to it.\n",
      "        # We test a LOT of conditions here. Break and return if the following is true:\n",
      "        #     The domain is indeed the \"target\" domain (e.g., nytimes.com).\n",
      "        #     The domain is not part of a bitly link (handles cases like the on.wsj.com bitly domain).\n",
      "        #    The domain does not match an internal redirection page (handles cases like http://www.theguardian.com/p/3p6jv/tw).\n",
      "        if (\n",
      "                not scrape\n",
      "            and    targetDomain is not None and (urlParsed.netloc == targetDomain or urlParsed.netloc[-len(targetDomain)-1:] == '.' + targetDomain)\n",
      "            and urlParsed.netloc != bitlyUrl\n",
      "            and (stillTraversePattern is None or not re.match(stillTraversePattern,nextUrl))\n",
      "        ):\n",
      "            break\n",
      "        # If it is not, request the URL without redirects.\n",
      "        # Use a get request if scraping.\n",
      "        try:\n",
      "            if (not scrape) or urlParsed.netloc in [bitlyUrl,'bit.ly','trib.al']:\n",
      "                singleRequest = requests.head(nextUrl, allow_redirects=False, timeout=timeoutSeconds)\n",
      "            else:\n",
      "                singleRequest = requests.get(nextUrl, allow_redirects=False, timeout=timeoutSeconds)\n",
      "        except:\n",
      "            # We can experience connection errors for various reasons.\n",
      "            # If we encounter one, append the URL to the end and move on.\n",
      "            e = sys.exc_info()[0]\n",
      "            errorText = str(e)\n",
      "            break\n",
      "        # We have a request. Look for a location in the header. If none, we've reached the end and can return.\n",
      "        if 'location' not in singleRequest.headers:\n",
      "            # Store our request as the last request.\n",
      "            lastRequest = singleRequest\n",
      "            break\n",
      "        # Set the next URL to traverse.\n",
      "        nextUrl = singleRequest.headers['location']\n",
      "        # To avoid loops, verify if address has been traversed before.\n",
      "        if nextUrl in hist:\n",
      "            errorText = 'Loop encountered'\n",
      "            break\n",
      "        \n",
      "    # Return.\n",
      "    return lastRequest, errorText, hist\n",
      "\n",
      "##### Get URLs\n",
      "\n",
      "# Statistics.\n",
      "urlsSkipped = 0\n",
      "urlsRetrieved = 0\n",
      "fbUrlsRetrieved = 0\n",
      "twUrlsRetrieved = 0\n",
      "postsSansUrl = 0\n",
      "urlsScraped = 0\n",
      "\n",
      "# Validation.\n",
      "if network not in ['facebook','twitter']:\n",
      "    print 'Network \"%s\" invalid; skipping user %s.' % (network, org)\n",
      "    quit()\n",
      "\n",
      "# Look for existing URLs.\n",
      "orgUrlsFilename = (filePath_urls % (network,org))\n",
      "urlsFileExists = os.path.exists(orgUrlsFilename)\n",
      "# Create a URL dict if we don't find it.\n",
      "urls = dict()\n",
      "if urlsFileExists:\n",
      "    # URLs file exists. Load it.\n",
      "    urls = json.load(open(orgUrlsFilename))\n",
      "\n",
      "    # Because we're going to be modifying it, create a backup.\n",
      "    backupFilename = (dirPath_urlBackups % network) + org + '-' + currTimestamp + '.json'\n",
      "    shutil.copyfile(orgUrlsFilename, backupFilename)\n",
      "    print 'Backed up URLs file to %s' % '/'.join(backupFilename.split('/')[-2:])\n",
      "\n",
      "# Get posts. Use the latest file if more than one.\n",
      "postsFilename = [f for f in os.listdir(dirPath_consolidated % network) if f.find(org) == 0][-1]\n",
      "posts = json.load(open(dirPath_consolidated % network + '/' + postsFilename))\n",
      "\n",
      "# Set up content directory path. Create if doesn't exist.\n",
      "contentDirectory = dirPath_content % (network, org)\n",
      "if not os.path.exists(contentDirectory):\n",
      "    os.makedirs(contentDirectory)\n",
      "\n",
      "# Traverse all posts, urls.\n",
      "# enumerate() pattern is for debugging. It allows us to break after a certain number of posts if necessary.\n",
      "# Can remove before release.\n",
      "for (i, post) in enumerate(posts):\n",
      "    # Get links and traverse.\n",
      "    postUrls = associatedUrls(network,post)\n",
      "    if len(postUrls) == 0:\n",
      "        postsSansUrl += 1\n",
      "    for url in postUrls:\n",
      "        # Check to see if URL has ever been checked before. If so, note it and continue.\n",
      "        # In practice, this is like breaking (most posts only have one link).\n",
      "        # But we must continue in case there is another link in the post that is valid.\n",
      "        if url in urls:\n",
      "            urlsSkipped += 1\n",
      "            continue\n",
      "\n",
      "        # Container for URL data. How we fill it depends on scrapeContent.\n",
      "        urlData = dict()\n",
      "        # Handle base content cases so we don't need to set \"defaults\" everywhere.\n",
      "        urlData['errorText'] = None\n",
      "        urlData['history'] = []\n",
      "        urlData['statusCode'] = None\n",
      "        urlData['scraped'] = scrapeContent\n",
      "        content = None\n",
      "\n",
      "        # Make request. Use a traverseUrl() call to avoid triggering connection resets or contaminating Bitly observations.\n",
      "        finalRequest, errorText, urlsTraversed = traverseUrls(url,targetDomain,stillTraversePattern,bitlyUrl,scrapeContent)\n",
      "\n",
      "        # Set up history, status codes. Codes only available if we made a final request.\n",
      "        urlData['history'] = urlsTraversed\n",
      "        if finalRequest is not None:\n",
      "            urlData['statusCode'] = finalRequest.status_code\n",
      "\n",
      "        # Handle errors.\n",
      "        if errorText is not None:\n",
      "            urlData['errorText'] = errorText\n",
      "\n",
      "        # Store MD5. Used for determining write location of content if we scraped it.\n",
      "        urlMd5 = md5(url.encode('utf8')).hexdigest()\n",
      "        urlData['md5'] = urlMd5\n",
      "\n",
      "        # Save content if we have it.\n",
      "        if finalRequest is not None and bool(finalRequest.content):\n",
      "            with open(contentDirectory+urlMd5+'.html','w') as f:\n",
      "                f.write(finalRequest.content)\n",
      "            urlsScraped += 1\n",
      "\n",
      "        # Parse final URL.\n",
      "        urlParsed = urlparse(urlData['history'][-1])\n",
      "        urlData['endUrl'] = {\n",
      "            'url': urlData['history'][-1],\n",
      "            'scheme': urlParsed.scheme,\n",
      "            'netloc': urlParsed.netloc,\n",
      "            'path': urlParsed.path,\n",
      "            'query': urlParsed.query,\n",
      "            'fragment': urlParsed.fragment\n",
      "        }\n",
      "\n",
      "        # Store in main URLs dictionary.\n",
      "        urls[url] = urlData\n",
      "\n",
      "        # Statistics.\n",
      "        urlsRetrieved += 1\n",
      "        if (network == 'twitter'):\n",
      "            twUrlsRetrieved += 1\n",
      "        else:\n",
      "            fbUrlsRetrieved += 1\n",
      "\n",
      "        # Dump URLs if we've gotten to that point.\n",
      "        if urlsRetrieved % incrementalUrlsToStore == 0:\n",
      "            print 'Storing %d URLs.' % incrementalUrlsToStore\n",
      "            json.dump(urls, open(orgUrlsFilename,'w'))\n",
      "\n",
      "        # Wait, if we want to.\n",
      "        time.sleep(sleepSeconds)\n",
      "\n",
      "# Store any last URLs for this user.\n",
      "json.dump(urls, open(orgUrlsFilename,'w'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bitly Traffic Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, Bitly provides a [REST API](http://dev.bitly.com/) for retrieving link click statistics. We simply built a script to traverse every link and retrieve its aggregate click count.\n",
      "\n",
      "Like URL retrieval code, this script is designed to be executed from the command line in order to better handle parallelization and error recovery. See the full version in: [/src/bitly/getBitlyTraffic.py](https://github.com/micahlanier/social-news/blob/master/src/bitly/getBitlyTraffic.py)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "\n",
      "This script fetches Bitly traffic for all such links contained in social media posts.\n",
      "All URLs for which traffic has already been fetched will be ignored.\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "##### Configuration\n",
      "\n",
      "# Last date/time to fetch traffic. This should be several days in the past. There is currently no way to re-fetch data.\n",
      "lastDate = dt.datetime(2014,11,24,8,tzinfo=dateutil.tz.tzutc())\n",
      "\n",
      "# Sleep time between requests. Bitly is opaque with their API limits, so there is no easy way to determine to set this.\n",
      "sleepSeconds = 0.1\n",
      "\n",
      "# This script is going to take a long time to run, so we want to store URLs in chunks in case anything goes wrong.\n",
      "incrementalUrlsToStore = 50\n",
      "\n",
      "# Timeout for hanging URLs.\n",
      "timeoutSeconds = 10.0\n",
      "\n",
      "##### Main Setup\n",
      "\n",
      "# Our own custom URL parsing code for Facebook.\n",
      "import socialUrlUtils\n",
      "\n",
      "# Data directories.\n",
      "dirPath_consolidated = '../../../data/%s/consolidated/'\n",
      "filePath_bitly = '../../../data/bitly/%s/%s.json'\n",
      "dirPath_bitlyBackups = '../../../data/backup/bitly/%s/'\n",
      "\n",
      "# Current timestamp.\n",
      "currTimestamp = dt.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
      "\n",
      "##### Main Configuration\n",
      "\n",
      "# Set network, screen name, and sleep time based on inputs in sys.argv.\n",
      "org = str(sys.argv[1])\n",
      "network = str(sys.argv[2])\n",
      "\n",
      "# Get organizational information.\n",
      "allOrgs = json.load(open('../../conf/organizations.json'))\n",
      "\n",
      "# Get organization information for reference.\n",
      "orgName = allOrgs[org]['name']\n",
      "bitlyUrl = allOrgs[org]['urls']['bitly']\n",
      "bitlyUrls = [bitlyUrl,'bit.ly']\n",
      "\n",
      "# API setup.\n",
      "bitlySettings = json.load(open('../../../conf/bitly.json'))\n",
      "bitlyAccessToken = bitlySettings['accessToken']\n",
      "bitlyClicksBaseUrl = 'https://api-ssl.bitly.com/v3/link/clicks?'\n",
      "\n",
      "##### Get Traffic\n",
      "\n",
      "# Statistics.\n",
      "urlsSkipped = 0\n",
      "urlsSansBitlyUrl = 0\n",
      "bitlyStatsRetrieved = 0\n",
      "recentPostsSkipped = 0\n",
      "\n",
      "# Validation.\n",
      "if network not in ['facebook','twitter']:\n",
      "    print 'Network \"%s\" invalid; skipping %s.' % (network, org)\n",
      "    quit()\n",
      "\n",
      "# Look for existing Bitly data.\n",
      "orgBitlyFilename = (filePath_bitly % (network,org))\n",
      "bitlyFileExists = os.path.exists(orgBitlyFilename)\n",
      "# Create a Bitly dict if we don't find it.\n",
      "bitly = dict()\n",
      "if bitlyFileExists:\n",
      "    # Bitly file exists. Load it.\n",
      "    bitly = json.load(open(orgBitlyFilename))\n",
      "\n",
      "    # Because we're going to be modifying it, create a backup.\n",
      "    backupFilename = (dirPath_bitlyBackups % network) + org + '-' + currTimestamp + '.json'\n",
      "    shutil.copyfile(orgBitlyFilename, backupFilename)\n",
      "    print 'Backed up bitly file to %s' % '/'.join(backupFilename.split('/')[-2:])\n",
      "\n",
      "# Get posts (for URLs).\n",
      "postFilename = [f for f in os.listdir(dirPath_consolidated % network) if f.startswith(org)][-1]\n",
      "posts = json.load(open((dirPath_consolidated % network) + postFilename))\n",
      "\n",
      "# Traverse posts and find URLs.\n",
      "urls = set()\n",
      "if network == 'facebook':\n",
      "    for p in posts:\n",
      "        # Verify date.\n",
      "        if dateutil.parser.parse(p['created_time']) < lastDate:\n",
      "            # Find URLs to add.\n",
      "            if 'link' in p:\n",
      "                urls.add(p['link'])\n",
      "            if 'message' in p:\n",
      "                for url in socialUrlUtils.urlsInText(p['message']):\n",
      "                    urls.add(url)\n",
      "        else:\n",
      "            recentPostsSkipped += 1\n",
      "elif network == 'twitter':\n",
      "    for p in posts:\n",
      "        # Verify date.\n",
      "        if dateutil.parser.parse(p['created_at']) < lastDate:\n",
      "            # Find URLs to add.\n",
      "            for url in p['entities']['urls']:\n",
      "                urls.add(url['expanded_url'])\n",
      "        else:\n",
      "            recentPostsSkipped += 1\n",
      "\n",
      "# Traverse all URLs.\n",
      "# enumerate() pattern is for debugging. It allows us to break after a certain number of URLs if necessary.\n",
      "for (i, url) in enumerate(urls):\n",
      "    # Parse URL to determine if we should read.\n",
      "    urlInfo = urlparse(url)\n",
      "    # Examine domain. Ensure a bitly link; else continue.\n",
      "    if (urlInfo.netloc not in bitlyUrls):\n",
      "        urlsSansBitlyUrl += 1\n",
      "        continue\n",
      "    # Any URL at this point is readable against bitly.\n",
      "\n",
      "    # Ensure that we haven't already retrieved the URL.\n",
      "    # Later: we will want to add capability to go back to certain dates later.\n",
      "    if (url in bitly):\n",
      "        urlsSkipped += 1\n",
      "        continue\n",
      "\n",
      "    # Okay, now we know that we want to retrieve URL stats.\n",
      "    # Construct our API call URL.\n",
      "    requestQsParams = {\n",
      "        'access_token': bitlyAccessToken,\n",
      "        'link': urllib.quote(url)\n",
      "    }\n",
      "    requestUrl = bitlyClicksBaseUrl + '&'.join([k+'='+v for (k,v) in requestQsParams.iteritems()])\n",
      "\n",
      "    # Make our request.\n",
      "    bitlyRequest = requests.get(requestUrl)\n",
      "    # Convert to dict.\n",
      "    bitlyRequestData = json.loads(bitlyRequest.text)\n",
      "    # Append a timestamp for our request.\n",
      "    bitlyRequestData['retrieved'] = bitlyRequest.headers['date']\n",
      "\n",
      "    # Store in main bitly container.\n",
      "    bitly[url] = bitlyRequestData\n",
      "\n",
      "    # Update statistics.\n",
      "    bitlyStatsRetrieved += 1\n",
      "\n",
      "    # Dump data if we've gotten to that point.\n",
      "    if bitlyStatsRetrieved % incrementalUrlsToStore == 0:\n",
      "        json.dump(bitly, open(orgBitlyFilename,'w'))\n",
      "\n",
      "    # Wait, if we want to.\n",
      "    time.sleep(sleepSeconds)\n",
      "\n",
      "# Store any last link stats for this user.\n",
      "json.dump(bitly, open(orgBitlyFilename,'w'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}