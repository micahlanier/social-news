{
 "metadata": {
  "name": "",
  "signature": "sha256:1f646f6ee2ac744a099bf08570c6c246819cc24d02822e90b32ccce2231e91a0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data Retrieval"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This section describes the process and code that we used to retrieve our raw data (from Twitter, Facebook, Bitly, and URL paths). Code for these tasks was (typically) written to be executed manually in *.py* files. Simplified versions are presented here for documentation. They strip out several hundred lines of rote tasks\u2014configuring directory paths, iterating over all news organizations, etc.\u2014in the interest of focusing on important functionality. If the reader is interested in full source code, it may be found in the noted files."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Tweets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use the [Twitter OAuth2 REST APIs](https://dev.twitter.com/rest/public) and [Twython package](https://twython.readthedocs.org/en/latest/) to read recent tweets from all of our target news organizations. Twitter's API imposes a 3,200 tweet limit on historical retrieval of individual accounts\u2019 tweets, so this process simply pulled tweets in sets of 200 (an API-imposed limit) until no more remained. See below for information about retrieving older tweets. See the full version in: src/twitter/downloadRawTweets.py."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Twitter's API imposes a 3,200 tweet limit on historical retrieval of individual accounts\u2019 tweets. This extends quite far back for some users, but for more prolific tweeters (e.g., the *Huffington Post* and *New York Times*) this restricts pulls to only the last couple of weeks. In order to get a dataset that extends further back in time for the more prolific users, we devised a method for scraping Twitter's [search page](https://twitter.com/search-home) and using the API to get all information about each tweet that we found. See the full version in: src/twitter/scrapeOlderTweets.py."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Facebook Posts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use Facebook's [Graph API](https://developers.facebook.com/docs/graph-api) to retrieve recent posts by each user. Facebook imposes no burdensome history length limit, so our code simply reque information in 75-posts \n",
      "\n",
      "See the full version in: src/fb/downloadRawFbPosts.py."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "User Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Twitter and Facebook APIs both provide easy access to user information. These scripts retrieve that information for display, and store it in the /data/users/ directory. See the full version in: /src/users/user_statistics.ipynb."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Twitter & Facebook Consolidation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above scripts are designed to be run progressively in order to collect data as time goes on. Their data may overlap significantly, so it is necessary to consolidate all retrieved data into a \"master\" JSON file for each organization-network. In retrospect, a NoSQL solution like MongoDB would have been a logical choice for storing this information, and we would certainly use it if we had more time available. See the full versions of these files in: /src/fb/consolidateRawFbPosts.py and /src/twitter/consolidateRawTweets.py."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "URL Retrieval"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We were interested in determining where social media links actually *go*. That knowledge would enable us to (1) track overall performance of a link that is posted more than once (e.g., [@nytimes's football map tweets](https://twitter.com/search?q=from%3Anytimes%20football%20map&src=typd)) and (2) match Twitter and Facebook posts in order to determine relative effectiveness. This is obscured by the fact that 23 of the 25 news organizations that we examined use Bitly regularly for posting links on social media websites. But Bitly API calls cannot necessarily tell us where those links go: many redirect several more times (e.g., through [trib.al](http://trib.al/)) before reaching their final destinations. Thus, we were forced to determine link destinations on our own.\n",
      "\n",
      "We encountered two problems accomplishing this. First, we did not want to inflate Bitly click counts by following those links directly. This was alleviated by making our requests with HTTP `HEAD` requests, which avoid doing so. And second, we found that some news websites would return errors if we made high-frequency requests to them. To get around this, our code traverses the URL redirection chain until encountering the final link."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Bitly Traffic Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, Bitly provides a [REST API](http://dev.bitly.com/) for retrieving link click statistics. We simply built a script to traverse every link and retrieve its aggregate click count. See the full version in: src/bitly/getBitlyTraffic.py."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}