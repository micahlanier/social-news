{
 "metadata": {
  "name": "",
  "signature": "sha256:ba38b82339a2771fa9b985792ad95a5f2b74710316798a08cb4f55eb71ca7eed"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data Retrieval"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This section describes the process and code that we used to retrieve our raw data (from Twitter, Facebook, Bitly, and URL paths). Code for these tasks was (typically) written to be executed manually in *.py* files. Simplified versions are presented here for documentation, but generally cannot be run as-is. Here, we  strip out many rote tasks\u2014loading libraries, printing status updates, etc.\u2014in the interest of focusing on important functionality. If the reader is interested in full source code, it may be found in (and executed with) the noted files."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Tweets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use the [Twitter OAuth2 REST APIs](https://dev.twitter.com/rest/public) and [Twython package](https://twython.readthedocs.org/en/latest/) to read recent tweets from all of our target news organizations. Twitter's API imposes a 3,200 tweet limit on historical retrieval of individual accounts\u2019 tweets, so this process simply pulled tweets in sets of 200 (an API-imposed limit) until no more remained. See below for information about retrieving older tweets. See the full version in: /src/twitter/downloadRawTweets.py."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "This script will pull as many recent tweets as possible from given organizations and dump them to a timestamped file.\n",
      "If we pull tweets over time, this will allow us to arrange pulls in order and consolidate them into a master file.\n",
      "\n",
      "We have encountered rate limit failures around 17 consecutive organizations.\n",
      "Consider dividing the organization list in two and fetching both halves over time.\n",
      "If it fails, wait 15 minutes and try again. In the future, we can think about cycling through API keys.\n",
      "\n",
      "Relevant Twitter API documentation:\n",
      "    https://dev.twitter.com/rest/reference/get/statuses/user_timeline\n",
      "\"\"\"\n",
      "\n",
      "##### Configuration\n",
      "\n",
      "# List of organizations to fetch. Either a list or \"all\".\n",
      "orgs = 'all'\n",
      "\n",
      "##### Main Setup\n",
      "\n",
      "# API setup.\n",
      "apiConfigPath = '../../../conf/twitter.json'\n",
      "\n",
      "# Data directory.\n",
      "rawTweetsDirectory = '../../../data/twitter/raw/'\n",
      "# Get timestamp of current run.\n",
      "timestampFilename = dt.datetime.now().strftime('%Y-%m-%dT%H-%M-%S') + '.json'\n",
      "\n",
      "# Twitter Setup\n",
      "# Read API settings.\n",
      "twitterSettings = json.load(open(apiConfigPath))\n",
      "# Set up twitter object.\n",
      "twitter = Twython(twitterSettings['apiKey'], access_token=twitterSettings['accessToken'])\n",
      "# Default number of tweets to grab per request.\n",
      "tweetsPerRequest = 200\n",
      "\n",
      "# Organization setup.\n",
      "allOrgs = json.load(open('../../conf/organizations.json'))\n",
      "if (type(orgs) == type('str')):\n",
      "    orgs = allOrgs.keys()\n",
      "relevantOrgs = dict((k, allOrgs[k]) for k in orgs);\n",
      "\n",
      "##### Get Tweets\n",
      "\n",
      "# Iterate over organizations.\n",
      "for org, orgData in relevantOrgs.iteritems():\n",
      "    # Get SN.\n",
      "    sn = orgData['twitter']\n",
      "\n",
      "    # Set up container for tweets.\n",
      "    tweets = []\n",
      "    # Keep track of min tweet ID; min/max dates.\n",
      "    minTweetId = None\n",
      "    minTweetDate = None\n",
      "    maxTweetDate = None\n",
      "    # Set up folder if it doesn't exist.\n",
      "    snDirectory = rawTweetsDirectory + org + '/'\n",
      "    if not os.path.exists(snDirectory):\n",
      "        os.makedirs(snDirectory)\n",
      "\n",
      "    # Start looping and grabbing tweets as we go. Loop will terminate at a break.\n",
      "    # We just set an arbitrarily high limit in case something goes haywire.\n",
      "    while len(tweets) < 4000:\n",
      "        # Get tweets.\n",
      "        if (minTweetId is None):\n",
      "            currentTweets = twitter.get_user_timeline(screen_name=sn, count=tweetsPerRequest)\n",
      "        else:\n",
      "            currentTweets = twitter.get_user_timeline(screen_name=sn, max_id=minTweetId, count=tweetsPerRequest)\n",
      "        # If nothing there (we've reached the end), break the loop\n",
      "        if (not len(currentTweets)):\n",
      "            break\n",
      "        # Iterate over tweets and find min ID, min/max date.\n",
      "        for t in currentTweets:\n",
      "            minTweetId = min([tId for tId in [minTweetId, t['id']-1] if tId is not None])\n",
      "            currentTweetDate = dateutil.parser.parse(t['created_at'])\n",
      "            minTweetDate = min([tDate for tDate in [minTweetDate, currentTweetDate] if tDate is not None])\n",
      "            maxTweetDate = max([tDate for tDate in [maxTweetDate, currentTweetDate] if tDate is not None])\n",
      "        # Append to list.\n",
      "        tweets += currentTweets\n",
      "        # Log.\n",
      "        print 'Retrieved %d tweets (%d overall).' % (len(currentTweets), len(tweets))\n",
      "\n",
      "    # Dump to file.\n",
      "    filename = rawTweetsDirectory + org + '/' + timestampFilename\n",
      "    json.dump(tweets, open(filename,'w'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Twitter's API imposes a 3,200 tweet limit on historical retrieval of individual accounts\u2019 tweets. This extends quite far back for some users, but for more prolific tweeters (e.g., the *Huffington Post* and *New York Times*) this restricts pulls to only the last couple of weeks. In order to get a dataset that extends further back in time for the more prolific users, we devised a method for scraping Twitter's [search page](https://twitter.com/search-home) and using the API to get all information about each tweet that we found. See the full version in: /src/twitter/scrapeOlderTweets.py."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "This script will scrape tweets beyond the 3200-tweet limit by scraping the Twitter web search interface.\n",
      "Expect this process to be slooow.\n",
      "\"\"\"\n",
      "\n",
      "##### Configuration\n",
      "\n",
      "# We'll use these libraries immediately.\n",
      "import dateutil.parser\n",
      "\n",
      "# Enter a single organization to scrape. This script must be run on a per-organization basis.\n",
      "org = 'nytimes'\n",
      "\n",
      "# Date to go back to.\n",
      "until = dateutil.parser.parse('2014/10/01')\n",
      "\n",
      "# Number of tweets to consolidate. Output will be in files with this many tweets.\n",
      "tweetsPerFile = 100\n",
      "\n",
      "##### Main Setup\n",
      "\n",
      "# Ensure \"until\" setting is comparable to minDate.\n",
      "# until = pytz.utc.localize(until)\n",
      "until = until.replace(tzinfo=dateutil.tz.tzutc())\n",
      "\n",
      "# Unclear how time zones returned work. But this seems to get us what we're looking for.\n",
      "# eastern = pytz.timezone('US/Eastern')\n",
      "\n",
      "# Twitter Setup\n",
      "\n",
      "# API setup.\n",
      "apiConfigPath = '../../../conf/twitter.json'\n",
      "# Read API settings.\n",
      "twitterSettings = json.load(open(apiConfigPath))\n",
      "# Set up twitter object.\n",
      "twitter = Twython(twitterSettings['apiKey'], access_token=twitterSettings['accessToken'])\n",
      "# Wait time. Ensure we don't run afoul of rate limits.\n",
      "secondsToWait = 6\n",
      "\n",
      "# Organization/SN setup.\n",
      "allOrgs = json.load(open('../../conf/organizations.json'))\n",
      "sn = allOrgs[org]\n",
      "\n",
      "# Data directory.\n",
      "consolidatedTweetsDirectory = '../../../data/twitter/consolidated/%s/' % org\n",
      "if not os.path.exists(consolidatedTweetsDirectory):\n",
      "    raise Exception('No consolidated tweets for organization \"%s\" (@%s).' % (org,sn))\n",
      "\n",
      "# Data directory.\n",
      "rawTweetsDirectory = '../../../data/twitter/raw/%s/' % org\n",
      "# Get timestamp of current run.\n",
      "timestampFilename = dt.datetime.now().strftime('%Y-%m-%dT%H-%M-%S') + '.json'\n",
      "\n",
      "# Base search URL. Can just append an ID after this.\n",
      "baseSearchUrl = 'https://twitter.com/search?f=realtime&q=from%%3A%s%%20max_id%%3A' % sn\n",
      "\n",
      "### Find Starting Point\n",
      "\n",
      "# Find all consolidated tweet files and get the most recent.\n",
      "tweetFile = [f for f in os.listdir(consolidatedTweetsDirectory) if f[-5:] == '.json'][-1]\n",
      "\n",
      "# Load that file with json.\n",
      "consolidatedTweets = json.load(open(consolidatedTweetsDirectory+tweetFile))\n",
      "\n",
      "# Find min ID.\n",
      "minTweetId = None\n",
      "for tweet in consolidatedTweets:\n",
      "    minTweetId = min([tId for tId in [minTweetId, tweet['id']] if tId is not None])\n",
      "\n",
      "### Scraping\n",
      "\n",
      "# Container for tweets.\n",
      "tweets = []\n",
      "\n",
      "# Statistics.\n",
      "totalTweets = 0\n",
      "minTweetDate = None\n",
      "maxTweetDate = None\n",
      "\n",
      "# Loop terminates with break condition inside once we've found a date.\n",
      "while True:\n",
      "    # Decrement ID to avoid repeats.\n",
      "    minTweetId -= 1\n",
      "\n",
      "    # Make request and convert to a bs4 object.\n",
      "    requestUrl = baseSearchUrl + str(minTweetId)\n",
      "    searchRequest = requests.get(requestUrl)\n",
      "    soup = BeautifulSoup(searchRequest.text)\n",
      "\n",
      "    # Traverse all content items and extract tweet data.\n",
      "    tweetContainers = soup.find_all('div', class_='tweet')\n",
      "\n",
      "    # Traverse all tweet containers.\n",
      "    for t in tweetContainers:\n",
      "        # Find tweet ID for this tweet.\n",
      "        tweetId = int(t.attrs['data-tweet-id'])\n",
      "\n",
      "        # Get tweet; append.\n",
      "        tweet = twitter.show_status(id=tweetId)\n",
      "        tweets.append(tweet)\n",
      "\n",
      "        # Get stats.\n",
      "        minTweetId = minTweetId = min([tId for tId in [minTweetId, tweetId] if tId is not None])\n",
      "        tweetDate = dateutil.parser.parse(tweet['created_at'])\n",
      "        minTweetDate = min([tDate for tDate in [minTweetDate, tweetDate] if tDate is not None])\n",
      "        maxTweetDate = max([tDate for tDate in [maxTweetDate, tweetDate] if tDate is not None])\n",
      "\n",
      "        # Wait before making another request.\n",
      "        time.sleep(secondsToWait)\n",
      "\n",
      "    # Write out if we've hit the per-file limit or have gone back far enough.\n",
      "    if len(tweets) >= tweetsPerFile or minTweetDate < until:\n",
      "        # Stats.\n",
      "        totalTweets += len(tweets)\n",
      "\n",
      "        # Output to file.\n",
      "        filename = rawTweetsDirectory + timestampFilename\n",
      "        json.dump(tweets, open(filename,'w'))\n",
      "\n",
      "        # Reset relevant objects.\n",
      "        timestampFilename = dt.datetime.now().strftime('%Y-%m-%dT%H-%M-%S') + '.json'\n",
      "        tweets = []\n",
      "\n",
      "        # Break if we've gone back far enough.\n",
      "        if minTweetDate < until:\n",
      "            break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Facebook Posts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use Facebook's [Graph API](https://developers.facebook.com/docs/graph-api) to retrieve recent posts by each user. Facebook imposes no burdensome history length limit, so our code simply requests information in 75-post chunks until hitting a specified past date (set to September 1 when we gathered our data). See the full version in: /src/fb/downloadRawFbPosts.py."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "This script will pull recent Facebook posts from given usernames and dump them to a timestamped file.\n",
      "If we pull posts over time, this will allow us to arrange pulls in order and consolidate them into a master file.\n",
      "\"\"\"\n",
      "\n",
      "##### Configuration\n",
      "\n",
      "# We'll use these libraries immediately.\n",
      "import datetime as dt\n",
      "import dateutil.parser\n",
      "\n",
      "# List of organizations to fetch. Either a list or \"all\".\n",
      "orgs = 'all'\n",
      "# Upper limit of date.\n",
      "maxDate = dt.datetime(2014,11,25,tzinfo=dateutil.tz.tzutc()) #dt.datetime.utcnow()\n",
      "# Lower limit of date.\n",
      "minDate = dt.datetime(2014, 8,31,tzinfo=dateutil.tz.tzutc())\n",
      "# Facebook is not as explicit as Twitter about limiting. Set this as needed if you run into any issues.\n",
      "postLimit = 75\n",
      "\n",
      "##### Main Setup\n",
      "\n",
      "# API setup.\n",
      "apiConfigPath = '../../../conf/facebook.json'\n",
      "\n",
      "# Data directory.\n",
      "rawPostsDirectory = '../../../data/facebook/raw/'\n",
      "# Get timestamp of current run.\n",
      "timestampFilename = dt.datetime.now().strftime('%Y-%m-%dT%H-%M-%S') + '.json'\n",
      "\n",
      "# Facebook Setup\n",
      "# Read API settings.\n",
      "fbSettings = json.load(open(apiConfigPath))\n",
      "\n",
      "# Ensure maxDate comparable to minDate.\n",
      "maxDate = maxDate.replace(tzinfo=dateutil.tz.tzutc())\n",
      "\n",
      "# Keep a one-second time delta handy for upper-bound limits.\n",
      "oneSecond = dt.timedelta(seconds=1)\n",
      "\n",
      "# Sleep interval for transient errors.\n",
      "sleepTimeSeconds = 4\n",
      "\n",
      "# Organization setup.\n",
      "allOrgs = json.load(open('../../conf/organizations.json'))\n",
      "if (type(orgs) == type('str')):\n",
      "    orgs = allOrgs.keys()\n",
      "relevantOrgs = dict((k, allOrgs[k]) for k in orgs)\n",
      "\n",
      "##### Facebook Helper Code\n",
      "\n",
      "\"\"\"\n",
      "Simple function for performing a Facebook GET request.\n",
      "Does not check parameters, so be careful!\n",
      "\"\"\"\n",
      "def facebookGet(path, accessToken, parameters, version = '2.1'):\n",
      "    fbUrl = 'https://graph.facebook.com/v' + version + '/'\n",
      "    # Add request path.\n",
      "    fbUrl += path.strip('/') + '/'\n",
      "    # Add access token.\n",
      "    fbUrl += '?access_token=' + accessToken\n",
      "    # Add additional parameters.\n",
      "    for pKey, pValue in parameters.iteritems():\n",
      "        fbUrl += '&%s=%s' % (pKey, urllib.quote(str(pValue)))\n",
      "    return requests.get(fbUrl)\n",
      "\n",
      "##### Get Facebook Posts\n",
      "\n",
      "# Iterate over organizations.\n",
      "for org, orgData in relevantOrgs.iteritems():\n",
      "    # Get account.\n",
      "    acct = orgData['facebook']\n",
      "\n",
      "    # Set up folder if it doesn't exist.\n",
      "    acctDirectory = rawPostsDirectory + org + '/'\n",
      "    if not os.path.exists(acctDirectory):\n",
      "        os.makedirs(acctDirectory)\n",
      "\n",
      "    # Set up container for posts.\n",
      "    posts = []\n",
      "\n",
      "    # Set up a loop date.\n",
      "    loopMaxDate = maxDate\n",
      "\n",
      "    # Set up objects hold our request.\n",
      "    requestPath = acct + '/posts'\n",
      "    requestParams = dict()\n",
      "    requestParams['limit'] = postLimit\n",
      "\n",
      "    # Don't bother with getting ALL comments/likes. We're only interested in summary statistics.\n",
      "    # While it would be cool to analyze all of the people who comment/like, that's a PITA in the time we have.\n",
      "    # By doing this, we need to specify a whitelist of all fields that we want.\n",
      "    apiFields = [\n",
      "        'comments.limit(1).summary(true)','likes.limit(1).summary(true)',\n",
      "        'caption','created_time','description','from','id','link','message','name','shares','status_type','type'\n",
      "    ]\n",
      "    requestParams['fields'] = ','.join(apiFields)\n",
      "\n",
      "    # Reload settings just in case we've changed access tokens and are running manually.\n",
      "    fbSettings = json.load(open(apiConfigPath))\n",
      "\n",
      "    # Loop until we have gone back to minDate.\n",
      "    while loopMaxDate > minDate:\n",
      "        # Compose a request with the latest max date.\n",
      "        requestParams['until'] = str(loopMaxDate)\n",
      "        # Make request; parse JSON.\n",
      "        facebookRequest = facebookGet(requestPath, fbSettings['accessToken'], requestParams)\n",
      "        jsonFacebookRequest = json.loads(facebookRequest.text)\n",
      "\n",
      "        # Detect any errors.\n",
      "        if 'error' in jsonFacebookRequest:\n",
      "            if jsonFacebookRequest['error']['code'] == 2:\n",
      "                # Tranient, \"unexpected\" error. Sleep for a sec and then continue.\n",
      "                print 'Unexpected error. Sleeping %d seconds.' % sleepTimeSeconds\n",
      "                time.sleep(2)\n",
      "                continue\n",
      "            else:\n",
      "                print 'Encountered a major error. Breaking loop.'\n",
      "                print jsonFacebookRequest['error']\n",
      "                break\n",
      "\n",
      "        # If no errors, we got data.\n",
      "        currentPosts = jsonFacebookRequest['data']\n",
      "\n",
      "        # Break if we didn't get anything; otherwise append and continue.\n",
      "        if (not len(currentPosts)):\n",
      "            break\n",
      "        posts += currentPosts\n",
      "\n",
      "        # Get date for next request. Go back one second, because for some reason they use inclusive logic.\n",
      "        loopMaxDate = dateutil.parser.parse(currentPosts[-1]['created_time']) - oneSecond\n",
      "\n",
      "    # Dump to file.\n",
      "    filename = acctDirectory + timestampFilename\n",
      "    json.dump(posts, open(filename,'w'), indent=0)\n",
      "\n",
      "    # Get max/min dates retrieved.\n",
      "    minDateRetrieved = loopMaxDate + oneSecond\n",
      "    maxDateRetrieved = dateutil.parser.parse(posts[0]['created_time'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "User Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Twitter and Facebook APIs both provide easy access to user information. These scripts retrieve that information for display, and store it in the /data/users/ directory. See the full version in: /src/users/user_statistics.ipynb."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Twitter & Facebook Consolidation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above scripts are designed to be run progressively in order to collect data as time goes on. Their data may overlap significantly, so it is necessary to consolidate all retrieved data into a \"master\" JSON file for each organization-network. In retrospect, a NoSQL solution like MongoDB would have been a logical choice for storing this information, and we would certainly use it if we had more time available. See the full versions of these files in: /src/fb/consolidateRawFbPosts.py and /src/twitter/consolidateRawTweets.py."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "URL Retrieval"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We were interested in determining where social media links actually *go*. That knowledge would enable us to (1) track overall performance of a link that is posted more than once (e.g., [@nytimes's football map tweets](https://twitter.com/search?q=from%3Anytimes%20football%20map&src=typd)) and (2) match Twitter and Facebook posts in order to determine relative effectiveness. This is obscured by the fact that 23 of the 25 news organizations that we examined use Bitly regularly for posting links on social media websites. But Bitly API calls cannot necessarily tell us where those links go: many redirect several more times (e.g., through [trib.al](http://trib.al/)) before reaching their final destinations. Thus, we were forced to determine link destinations on our own.\n",
      "\n",
      "We encountered two problems accomplishing this. First, we did not want to inflate Bitly click counts by following those links directly. This was alleviated by making our requests with HTTP `HEAD` requests, which avoid doing so. And second, we found that some news websites would return errors if we made high-frequency requests to them. To get around this, our code traverses the URL redirection chain until encountering the final link.\n",
      "\n",
      "This code is designed to be executed from the command line in order to better handle parallelization and error recovery. See full versions of this code in /src/urls/scrapeUrls.py and /src/urls/socialUrlUtils.py."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Bitly Traffic Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, Bitly provides a [REST API](http://dev.bitly.com/) for retrieving link click statistics. We simply built a script to traverse every link and retrieve its aggregate click count.\n",
      "\n",
      "Like URL retrieval code, this script is designed to be executed from the command line in order to better handle parallelization and error recovery. See the full version in: /src/bitly/getBitlyTraffic.py."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}