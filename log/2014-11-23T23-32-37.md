Micah spent some time looking at sentiment analysis matters and inadvertently learned a few things about the NLP pipeline. This is meant to help him recall what he did and serve as a guidepost for others.

For starters, import NLTK and some corpuses:
	import nltk
	from nltk.corpus import wordnet as wn
	from nltk.corpus import sentiwordnet as swn

Consider a simple Tweet:
	tweet = "Fear-mongering about Ebola is threatening to reverse decades of progress for Africa's image http://t.co/aZdTwdEqJD via @nytopinion"

This must be tokenized. Here is a simple way to do so:
	tokens = nltk.word_tokenize(tweet)

Note that we may want to do some of the following:
- Remove URLs from social media message text; they do not tokenize well.
- Remove @ mentions.
- Remove hashtags or clean them (remove the "#").
We may even want to try another regular expression rather than relying on the default.

Next, we want to perform part-of-speech tagging. That is also pretty straightforward:
	posTags = nltk.tag.pos_tag(tokens)

This is where things get tricky. We need to determine some sort of meaning for each word that will map to a sentiments. One way to get this is with the Lesk algorithm. We *might* need to map the tags from pos_tags() to something that lesk() can use. I am not sure how to do that, but I think this tag mapping might be a first step:
	from nltk.tags.mapping import map_tag
	posTagsUniversal = [(word, map_tag('en-ptb', 'universal', tag)) for word, tag in posTags]
That doesn't produce the exact a/n/r/v mapping that lesk() expects. Just go with it for now.

**It turns out that Lesk will work without a POS tag. So the above might have been unnecessary.**

Next, pass each token into lesk() to determine a corresponding WordNet entry:
	meanings = [nltk.wsd.lesk(tokens,t) for t in tokens]
	meanings = [m for m in meanings if m is not None]

Finally, get scores:
	scores = []
	for m in meanings:
		swnEntry = swn.senti_synset(m.name())
		scores.append((t,m.name(),swnEntry.pos_score(), swnEntry.neg_score()))
`scores` will contain tuples of all positive/negative scores (and other info). They can be aggregated however we wish.

Here's an example of the resulting list:
	[
	 ('Fear-mongering', u'about.r.04', 0.0, 0.0),
	 ('about', u'ebola_hemorrhagic_fever.n.01', 0.0, 0.125),
	 ('Ebola', u'equal.v.01', 0.125, 0.125),
	 ('is', u'threaten.v.02', 0.0, 0.375),
	 ('threatening', u'invert.v.02', 0.0, 0.0),
	 ('to', u'ten.n.01', 0.0, 0.0),
	 ('reverse', u'progress.n.02', 0.125, 0.0),
	 ('decades', u'africa.n.01', 0.0, 0.0),
	 ('of', u'image.n.07', 0.0, 0.0),
	 ('progress', u'hypertext_transfer_protocol.n.01', 0.0, 0.0)
	]
One cool observation: Lesk can figure out that "Ebola" corresponds to "ebola_hemorrhagic_fever.n.01" with a neg_score() of 0.125 in SentiWordNet.

---

Some links:
- [SentiWordNet](http://sentiwordnet.isti.cnr.it)
- [Lesk algorithm implementation](http://www.nltk.org/howto/wsd.html)
- [NLTK categorization/tagging documentation](http://www.nltk.org/book/ch05.html)
- [NLTK tag mapping](http://www.nltk.org/_modules/nltk/tag/mapping.html)
- [NLTK Word Sense Disambiguation](http://www.nltk.org/howto/wsd.html)
